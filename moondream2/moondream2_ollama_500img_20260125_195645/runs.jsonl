{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 189.517, "latencies_ms": [189.517], "images_per_second": 5.277, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "urn of flowers sits on a table in a living room, with a woman standing nearby and a television set on top of a wooden stand.\n", "error": null, "sys_before": {"cpu_percent": 7.2, "ram_used_mb": 10095.5, "ram_available_mb": 112410.8, "ram_percent": 8.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10334.9, "ram_available_mb": 112171.4, "ram_percent": 8.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.39, 34.39], "power_watts_avg": 34.39, "power_watts_peak": 34.39, "energy_joules_est": 6.53, "sample_count": 2, "duration_seconds": 0.19}, "timestamp": "2026-01-25T19:56:50.321996"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 81.402, "latencies_ms": [81.402], "images_per_second": 12.285, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Tv - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10334.9, "ram_available_mb": 112171.4, "ram_percent": 8.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10335.1, "ram_available_mb": 112171.2, "ram_percent": 8.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.39], "power_watts_avg": 34.39, "power_watts_peak": 34.39, "energy_joules_est": 2.82, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T19:56:50.477610"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 129.239, "latencies_ms": [129.239], "images_per_second": 7.738, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. Woman standing in living room with a vase on table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10335.1, "ram_available_mb": 112171.2, "ram_percent": 8.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10334.9, "ram_available_mb": 112171.4, "ram_percent": 8.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.39, 29.98], "power_watts_avg": 32.19, "power_watts_peak": 34.39, "energy_joules_est": 4.17, "sample_count": 2, "duration_seconds": 0.13}, "timestamp": "2026-01-25T19:56:50.682074"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 353.513, "latencies_ms": [353.513], "images_per_second": 2.829, "prompt_tokens": 757, "response_tokens_est": 61, "n_tiles": 1, "output_text": "\nIn a cozy living room, there are two people present. One person stands near a window with yellow walls, while another person is standing in front of a kitchen area that includes a stove top oven and a refrigerator. The room features wooden floors and white furniture, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10334.9, "ram_available_mb": 112171.4, "ram_percent": 8.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10336.8, "ram_available_mb": 112169.5, "ram_percent": 8.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [29.98, 29.98, 29.98, 29.98], "power_watts_avg": 29.98, "power_watts_peak": 29.98, "energy_joules_est": 10.61, "sample_count": 4, "duration_seconds": 0.354}, "timestamp": "2026-01-25T19:56:51.088912"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 299.269, "latencies_ms": [299.269], "images_per_second": 3.341, "prompt_tokens": 756, "response_tokens_est": 53, "n_tiles": 1, "output_text": "\n A woman is standing in a living room with yellow walls and wood floors. She has her back to the camera. The television is on top of a wooden stand. There are two chairs near the dining table, which also has a vase on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10336.8, "ram_available_mb": 112169.5, "ram_percent": 8.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10336.1, "ram_available_mb": 112170.2, "ram_percent": 8.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [26.9, 26.9, 26.9], "power_watts_avg": 26.9, "power_watts_peak": 26.9, "energy_joules_est": 8.06, "sample_count": 3, "duration_seconds": 0.3}, "timestamp": "2026-01-25T19:56:51.394682"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 343.468, "latencies_ms": [343.468], "images_per_second": 2.911, "prompt_tokens": 744, "response_tokens_est": 35, "n_tiles": 1, "output_text": "urn of a brown bear with its mouth open, looking directly at the camera.\n\n2. The bear is sitting on grass in front of some bushes and trees.", "error": null, "sys_before": {"cpu_percent": 3.2, "ram_used_mb": 10339.6, "ram_available_mb": 112166.7, "ram_percent": 8.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10364.4, "ram_available_mb": 112141.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [26.9, 30.89, 30.89, 30.89], "power_watts_avg": 29.89, "power_watts_peak": 30.89, "energy_joules_est": 10.27, "sample_count": 4, "duration_seconds": 0.344}, "timestamp": "2026-01-25T19:56:51.812613"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.51, "latencies_ms": [78.51], "images_per_second": 12.737, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Grass", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10364.7, "ram_available_mb": 112141.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10366.9, "ram_available_mb": 112139.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [30.89], "power_watts_avg": 30.89, "power_watts_peak": 30.89, "energy_joules_est": 2.43, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T19:56:51.918415"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 120.21, "latencies_ms": [120.21], "images_per_second": 8.319, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nThe bear is sitting in a grassy area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10367.1, "ram_available_mb": 112139.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10366.6, "ram_available_mb": 112139.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [30.89, 33.15], "power_watts_avg": 32.02, "power_watts_peak": 33.15, "energy_joules_est": 3.87, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T19:56:52.124450"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 148.366, "latencies_ms": [148.366], "images_per_second": 6.74, "prompt_tokens": 757, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nA brown bear sits in a grassy field, facing the camera with its mouth open.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10366.9, "ram_available_mb": 112139.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10367.3, "ram_available_mb": 112139.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [33.15, 33.15], "power_watts_avg": 33.15, "power_watts_peak": 33.15, "energy_joules_est": 4.94, "sample_count": 2, "duration_seconds": 0.149}, "timestamp": "2026-01-25T19:56:52.330485"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 116.801, "latencies_ms": [116.801], "images_per_second": 8.562, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The bear is brown and has a black nose.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10367.3, "ram_available_mb": 112139.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10368.9, "ram_available_mb": 112137.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [33.15, 33.15], "power_watts_avg": 33.15, "power_watts_peak": 33.15, "energy_joules_est": 3.88, "sample_count": 2, "duration_seconds": 0.117}, "timestamp": "2026-01-25T19:56:52.535429"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 437.888, "latencies_ms": [437.888], "images_per_second": 2.284, "prompt_tokens": 744, "response_tokens_est": 58, "n_tiles": 1, "output_text": "\nA cozy bedroom features a large window with green trees outside, allowing natural light to fill the space. The room is adorned with various pieces of furniture and decorations, including a blue bedspread, wooden dresser, bookshelf filled with books, and a chair near the window.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10370.4, "ram_available_mb": 112135.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10378.0, "ram_available_mb": 112128.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [29.9, 29.9, 29.9, 29.9, 29.9], "power_watts_avg": 29.9, "power_watts_peak": 29.9, "energy_joules_est": 13.1, "sample_count": 5, "duration_seconds": 0.438}, "timestamp": "2026-01-25T19:56:53.051436"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 80.704, "latencies_ms": [80.704], "images_per_second": 12.391, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bed: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10378.0, "ram_available_mb": 112128.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10377.8, "ram_available_mb": 112128.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.07], "power_watts_avg": 32.07, "power_watts_peak": 32.07, "energy_joules_est": 2.61, "sample_count": 1, "duration_seconds": 0.081}, "timestamp": "2026-01-25T19:56:53.156560"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 128.444, "latencies_ms": [128.444], "images_per_second": 7.786, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n A large window with a green tree outside and a mirror on top of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10377.8, "ram_available_mb": 112128.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10381.4, "ram_available_mb": 112124.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.07, 32.07], "power_watts_avg": 32.07, "power_watts_peak": 32.07, "energy_joules_est": 4.13, "sample_count": 2, "duration_seconds": 0.129}, "timestamp": "2026-01-25T19:56:53.364515"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 201.778, "latencies_ms": [201.778], "images_per_second": 4.956, "prompt_tokens": 757, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA cozy bedroom with a blue bedspread, wooden dresser, bookshelf filled with books, and a window that lets in natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10381.4, "ram_available_mb": 112124.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10382.4, "ram_available_mb": 112123.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.07, 32.07, 31.42], "power_watts_avg": 31.86, "power_watts_peak": 32.07, "energy_joules_est": 6.45, "sample_count": 3, "duration_seconds": 0.203}, "timestamp": "2026-01-25T19:56:53.670563"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 126.533, "latencies_ms": [126.533], "images_per_second": 7.903, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n A room with a blue bedspread and wooden dresser.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10382.4, "ram_available_mb": 112123.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10386.3, "ram_available_mb": 112120.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.42, 31.42], "power_watts_avg": 31.42, "power_watts_peak": 31.42, "energy_joules_est": 3.99, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T19:56:53.875598"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 281.014, "latencies_ms": [281.014], "images_per_second": 3.559, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA red stop sign is mounted on a metal pole in front of a row of bushes and trees, with a white building visible behind it.", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 10386.3, "ram_available_mb": 112120.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10407.2, "ram_available_mb": 112099.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [31.42, 31.42, 25.93], "power_watts_avg": 29.59, "power_watts_peak": 31.42, "energy_joules_est": 8.33, "sample_count": 3, "duration_seconds": 0.282}, "timestamp": "2026-01-25T19:56:54.187282"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 71.936, "latencies_ms": [71.936], "images_per_second": 13.901, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Stop sign", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10407.2, "ram_available_mb": 112099.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10406.2, "ram_available_mb": 112100.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [25.93], "power_watts_avg": 25.93, "power_watts_peak": 25.93, "energy_joules_est": 1.87, "sample_count": 1, "duration_seconds": 0.072}, "timestamp": "2026-01-25T19:56:54.292490"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 141.417, "latencies_ms": [141.417], "images_per_second": 7.071, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n 1. A red stop sign on a metal pole in front of trees and bushes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10406.2, "ram_available_mb": 112100.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10411.2, "ram_available_mb": 112095.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [25.93, 25.93], "power_watts_avg": 25.93, "power_watts_peak": 25.93, "energy_joules_est": 3.68, "sample_count": 2, "duration_seconds": 0.142}, "timestamp": "2026-01-25T19:56:54.501662"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 223.21, "latencies_ms": [223.21], "images_per_second": 4.48, "prompt_tokens": 757, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA red stop sign stands on a pole in front of some bushes, with trees nearby. The sky above appears to be clear blue, indicating good weather conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10411.2, "ram_available_mb": 112095.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10406.3, "ram_available_mb": 112100.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.65, 30.65, 30.65], "power_watts_avg": 30.65, "power_watts_peak": 30.65, "energy_joules_est": 6.86, "sample_count": 3, "duration_seconds": 0.224}, "timestamp": "2026-01-25T19:56:54.809521"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 106.07, "latencies_ms": [106.07], "images_per_second": 9.428, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n The stop sign is red and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10406.1, "ram_available_mb": 112100.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10407.3, "ram_available_mb": 112099.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [30.65, 30.65], "power_watts_avg": 30.65, "power_watts_peak": 30.65, "energy_joules_est": 3.27, "sample_count": 2, "duration_seconds": 0.107}, "timestamp": "2026-01-25T19:56:55.016216"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 258.46, "latencies_ms": [258.46], "images_per_second": 3.869, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "urn of bears, two brown and one light brown, are huddled together on a bed with a blue blanket.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10407.3, "ram_available_mb": 112099.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10409.7, "ram_available_mb": 112096.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [29.81, 29.81, 29.81], "power_watts_avg": 29.81, "power_watts_peak": 29.81, "energy_joules_est": 7.71, "sample_count": 3, "duration_seconds": 0.259}, "timestamp": "2026-01-25T19:56:55.331501"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 177.903, "latencies_ms": [177.903], "images_per_second": 5.621, "prompt_tokens": 759, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\n 1. Brown teddy bear (0.25, 0.13, 0.58, 0.4)", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10409.7, "ram_available_mb": 112096.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10411.4, "ram_available_mb": 112094.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [29.81, 29.81], "power_watts_avg": 29.81, "power_watts_peak": 29.81, "energy_joules_est": 5.32, "sample_count": 2, "duration_seconds": 0.179}, "timestamp": "2026-01-25T19:56:55.538148"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 138.112, "latencies_ms": [138.112], "images_per_second": 7.24, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n 1. Brown teddy bear on top of other brown teddy bears.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10411.4, "ram_available_mb": 112094.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10411.9, "ram_available_mb": 112094.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [29.62, 29.62], "power_watts_avg": 29.62, "power_watts_peak": 29.62, "energy_joules_est": 4.11, "sample_count": 2, "duration_seconds": 0.139}, "timestamp": "2026-01-25T19:56:55.745774"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 279.756, "latencies_ms": [279.756], "images_per_second": 3.575, "prompt_tokens": 757, "response_tokens_est": 45, "n_tiles": 1, "output_text": "\nIn a cozy room, three teddy bears are sitting together on a bed. The largest bear has its arms wrapped around two smaller bears, creating an endearing scene of camaraderie among the stuffed animals.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 10411.9, "ram_available_mb": 112094.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10409.7, "ram_available_mb": 112096.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [29.62, 29.62, 29.62], "power_watts_avg": 29.62, "power_watts_peak": 29.62, "energy_joules_est": 8.3, "sample_count": 3, "duration_seconds": 0.28}, "timestamp": "2026-01-25T19:56:56.052860"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 130.132, "latencies_ms": [130.132], "images_per_second": 7.684, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n The teddy bear is brown and has a blue blanket on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10409.7, "ram_available_mb": 112096.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10408.6, "ram_available_mb": 112097.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.25, 32.25], "power_watts_avg": 32.25, "power_watts_peak": 32.25, "energy_joules_est": 4.22, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T19:56:56.258572"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 301.828, "latencies_ms": [301.828], "images_per_second": 3.313, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "iced skis on a woman's feet as she stands in the snow, wearing a red jacket and black pants with ski poles in her hands.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10408.6, "ram_available_mb": 112097.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10415.4, "ram_available_mb": 112090.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.25, 32.25, 32.25, 31.98], "power_watts_avg": 32.18, "power_watts_peak": 32.25, "energy_joules_est": 9.74, "sample_count": 4, "duration_seconds": 0.302}, "timestamp": "2026-01-25T19:56:56.672134"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 136.4, "latencies_ms": [136.4], "images_per_second": 7.331, "prompt_tokens": 759, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n 1. Skier in red jacket and black pants skiing down a snowy slope", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10415.4, "ram_available_mb": 112090.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10411.5, "ram_available_mb": 112094.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.98, 31.98], "power_watts_avg": 31.98, "power_watts_peak": 31.98, "energy_joules_est": 4.38, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T19:56:56.879433"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 99.282, "latencies_ms": [99.282], "images_per_second": 10.072, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nThe skier is in front of a mountain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10411.5, "ram_available_mb": 112094.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10411.3, "ram_available_mb": 112095.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.98], "power_watts_avg": 31.98, "power_watts_peak": 31.98, "energy_joules_est": 3.2, "sample_count": 1, "duration_seconds": 0.1}, "timestamp": "2026-01-25T19:56:56.984433"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 157.382, "latencies_ms": [157.382], "images_per_second": 6.354, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA woman in a red jacket and black pants is skiing down a snowy mountain slope, holding ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10411.3, "ram_available_mb": 112095.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10408.2, "ram_available_mb": 112098.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 2.0}, "power_stats": {"power_watts_samples": [30.37, 30.37], "power_watts_avg": 30.37, "power_watts_peak": 30.37, "energy_joules_est": 4.79, "sample_count": 2, "duration_seconds": 0.158}, "timestamp": "2026-01-25T19:56:57.191223"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 109.543, "latencies_ms": [109.543], "images_per_second": 9.129, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The woman is wearing a red jacket and black pants.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 10408.2, "ram_available_mb": 112098.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10408.2, "ram_available_mb": 112098.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 2.0}, "power_stats": {"power_watts_samples": [30.37, 30.37], "power_watts_avg": 30.37, "power_watts_peak": 30.37, "energy_joules_est": 3.34, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T19:56:57.397327"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 276.021, "latencies_ms": [276.021], "images_per_second": 3.623, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA white refrigerator-freezer combo stands in a kitchen with wooden cabinets and beige walls, accompanied by an oven and stovetop.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10408.2, "ram_available_mb": 112098.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10428.1, "ram_available_mb": 112078.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 2.0}, "power_stats": {"power_watts_samples": [30.37, 26.57, 26.57], "power_watts_avg": 27.84, "power_watts_peak": 30.37, "energy_joules_est": 7.69, "sample_count": 3, "duration_seconds": 0.276}, "timestamp": "2026-01-25T19:56:57.709196"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 505.05, "latencies_ms": [505.05], "images_per_second": 1.98, "prompt_tokens": 759, "response_tokens_est": 90, "n_tiles": 1, "output_text": "\n 1. Oven  2. Drawer 3. Refrigerator 4. Drawer 5. Drawer 6. Drawer 7. Drawer 8. Drawer 9. Drawer 10. Drawer 11. Drawer 12. Drawer 13. Drawer 14. Drawer 15. Drawer 16. Drawer 17. Drawer 18. Drawer 19. Drawer 20. Drawer 21. Drawer 22.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10428.1, "ram_available_mb": 112078.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10425.1, "ram_available_mb": 112081.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [26.57, 26.57, 26.57, 33.57, 33.57, 33.57], "power_watts_avg": 30.07, "power_watts_peak": 33.57, "energy_joules_est": 15.21, "sample_count": 6, "duration_seconds": 0.506}, "timestamp": "2026-01-25T19:56:58.316888"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 123.291, "latencies_ms": [123.291], "images_per_second": 8.111, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n 1. White refrigerator with water dispenser and freezer on top of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10425.1, "ram_available_mb": 112081.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10427.3, "ram_available_mb": 112079.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.57, 33.57], "power_watts_avg": 33.57, "power_watts_peak": 33.57, "energy_joules_est": 4.15, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T19:56:58.523591"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 210.658, "latencies_ms": [210.658], "images_per_second": 4.747, "prompt_tokens": 757, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nThe image shows a small kitchen with white appliances, including a refrigerator freezer on the right side. The kitchen also features wooden cabinets above the stove top oven.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10427.3, "ram_available_mb": 112079.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10432.7, "ram_available_mb": 112073.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.45, 36.45, 36.45], "power_watts_avg": 36.45, "power_watts_peak": 36.45, "energy_joules_est": 7.69, "sample_count": 3, "duration_seconds": 0.211}, "timestamp": "2026-01-25T19:56:58.829918"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 115.551, "latencies_ms": [115.551], "images_per_second": 8.654, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The kitchen is white and brown with a tiled floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10432.7, "ram_available_mb": 112073.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10435.3, "ram_available_mb": 112071.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 33.0}, "power_stats": {"power_watts_samples": [36.45, 36.45], "power_watts_avg": 36.45, "power_watts_peak": 36.45, "energy_joules_est": 4.23, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T19:56:59.036278"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 293.469, "latencies_ms": [293.469], "images_per_second": 3.408, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nTwo baseball players, dressed in white and green uniforms, are engaged in a game on a field with trees in the background.", "error": null, "sys_before": {"cpu_percent": 3.2, "ram_used_mb": 10435.3, "ram_available_mb": 112071.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 10440.3, "ram_available_mb": 112066.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 33.0}, "power_stats": {"power_watts_samples": [29.0, 29.0, 29.0], "power_watts_avg": 29.0, "power_watts_peak": 29.0, "energy_joules_est": 8.53, "sample_count": 3, "duration_seconds": 0.294}, "timestamp": "2026-01-25T19:56:59.353526"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 71.925, "latencies_ms": [71.925], "images_per_second": 13.903, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Tree", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10440.3, "ram_available_mb": 112066.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10444.5, "ram_available_mb": 112061.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 33.0}, "power_stats": {"power_watts_samples": [29.0], "power_watts_avg": 29.0, "power_watts_peak": 29.0, "energy_joules_est": 2.09, "sample_count": 1, "duration_seconds": 0.072}, "timestamp": "2026-01-25T19:56:59.457958"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 152.327, "latencies_ms": [152.327], "images_per_second": 6.565, "prompt_tokens": 763, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nThe man in a white shirt is running towards the ball while another man stands ready to catch it.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 10444.5, "ram_available_mb": 112061.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10426.9, "ram_available_mb": 112079.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 33.0}, "power_stats": {"power_watts_samples": [29.0, 29.27], "power_watts_avg": 29.13, "power_watts_peak": 29.27, "energy_joules_est": 4.45, "sample_count": 2, "duration_seconds": 0.153}, "timestamp": "2026-01-25T19:56:59.666029"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 349.719, "latencies_ms": [349.719], "images_per_second": 2.859, "prompt_tokens": 757, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\nIn a park, two baseball players are engaged in an intense game. One player is running towards first base while holding his glove open, ready to catch the ball. The other player stands nearby, also wearing a glove but not actively participating in the play at that moment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10426.9, "ram_available_mb": 112079.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10423.4, "ram_available_mb": 112082.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [29.27, 29.27, 29.27, 29.27], "power_watts_avg": 29.27, "power_watts_peak": 29.27, "energy_joules_est": 10.26, "sample_count": 4, "duration_seconds": 0.35}, "timestamp": "2026-01-25T19:57:00.072973"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 252.716, "latencies_ms": [252.716], "images_per_second": 3.957, "prompt_tokens": 756, "response_tokens_est": 42, "n_tiles": 1, "output_text": "\n The image shows two baseball players running on a field. One player is wearing a white shirt and gray pants, while the other player has a green hat and is holding a brown glove in his hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10423.4, "ram_available_mb": 112082.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10427.6, "ram_available_mb": 112078.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.62, 32.62, 32.62], "power_watts_avg": 32.62, "power_watts_peak": 32.62, "energy_joules_est": 8.26, "sample_count": 3, "duration_seconds": 0.253}, "timestamp": "2026-01-25T19:57:00.379817"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 285.009, "latencies_ms": [285.009], "images_per_second": 3.509, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA tennis player in a white and black outfit is captured mid-swing, with an orange racket, on a blue court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10427.8, "ram_available_mb": 112078.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10431.7, "ram_available_mb": 112074.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.62, 32.62, 31.49], "power_watts_avg": 32.24, "power_watts_peak": 32.62, "energy_joules_est": 9.2, "sample_count": 3, "duration_seconds": 0.285}, "timestamp": "2026-01-25T19:57:00.688351"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 98.829, "latencies_ms": [98.829], "images_per_second": 10.118, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Jp morgan sign", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10431.7, "ram_available_mb": 112074.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10426.2, "ram_available_mb": 112080.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.49], "power_watts_avg": 31.49, "power_watts_peak": 31.49, "energy_joules_est": 3.14, "sample_count": 1, "duration_seconds": 0.1}, "timestamp": "2026-01-25T19:57:00.793426"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 131.731, "latencies_ms": [131.731], "images_per_second": 7.591, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe man is playing tennis and has his racket in hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10426.2, "ram_available_mb": 112080.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10426.3, "ram_available_mb": 112080.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [31.49, 31.49], "power_watts_avg": 31.49, "power_watts_peak": 31.49, "energy_joules_est": 4.16, "sample_count": 2, "duration_seconds": 0.132}, "timestamp": "2026-01-25T19:57:00.999213"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 364.575, "latencies_ms": [364.575], "images_per_second": 2.743, "prompt_tokens": 757, "response_tokens_est": 62, "n_tiles": 1, "output_text": "\nIn the image, a tennis player dressed in white and black is captured mid-swing on a blue court. The player's right hand grips a red racket as they prepare to strike an incoming ball. A crowd of spectators watches intently from behind a barrier, their attention focused on the ongoing match.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10426.3, "ram_available_mb": 112080.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10438.6, "ram_available_mb": 112067.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [32.79, 32.79, 32.79, 32.79], "power_watts_avg": 32.79, "power_watts_peak": 32.79, "energy_joules_est": 11.96, "sample_count": 4, "duration_seconds": 0.365}, "timestamp": "2026-01-25T19:57:01.405309"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 54.282, "latencies_ms": [54.282], "images_per_second": 18.422, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10438.6, "ram_available_mb": 112067.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 7.4, "ram_used_mb": 10443.3, "ram_available_mb": 112063.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [32.79], "power_watts_avg": 32.79, "power_watts_peak": 32.79, "energy_joules_est": 1.8, "sample_count": 1, "duration_seconds": 0.055}, "timestamp": "2026-01-25T19:57:01.510353"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 279.144, "latencies_ms": [279.144], "images_per_second": 3.582, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA group of young people, including children and adults, stand on a tennis court holding racquets in their hands.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10443.3, "ram_available_mb": 112063.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [33.29, 33.29, 33.29], "power_watts_avg": 33.29, "power_watts_peak": 33.29, "energy_joules_est": 9.33, "sample_count": 3, "duration_seconds": 0.28}, "timestamp": "2026-01-25T19:57:01.824676"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 115.985, "latencies_ms": [115.985], "images_per_second": 8.622, "prompt_tokens": 759, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Man in red shirt and black shorts holding trophy", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10460.4, "ram_available_mb": 112045.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [33.29, 33.29], "power_watts_avg": 33.29, "power_watts_peak": 33.29, "energy_joules_est": 3.87, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T19:57:02.030206"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 100.551, "latencies_ms": [100.551], "images_per_second": 9.945, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Person in front of green fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.4, "ram_available_mb": 112045.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10446.9, "ram_available_mb": 112059.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [32.73, 32.73], "power_watts_avg": 32.73, "power_watts_peak": 32.73, "energy_joules_est": 3.31, "sample_count": 2, "duration_seconds": 0.101}, "timestamp": "2026-01-25T19:57:02.235336"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 228.652, "latencies_ms": [228.652], "images_per_second": 4.373, "prompt_tokens": 757, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nA group of young people, including both boys and girls, are posing for a photo on a tennis court. They are all holding tennis rackets in their hands.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.9, "ram_available_mb": 112059.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10451.3, "ram_available_mb": 112055.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [32.73, 32.73, 32.73], "power_watts_avg": 32.73, "power_watts_peak": 32.73, "energy_joules_est": 7.5, "sample_count": 3, "duration_seconds": 0.229}, "timestamp": "2026-01-25T19:57:02.540859"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 353.935, "latencies_ms": [353.935], "images_per_second": 2.825, "prompt_tokens": 756, "response_tokens_est": 62, "n_tiles": 1, "output_text": "\n The image shows a group of people posing for a photo on a tennis court. There are at least 12 people in the picture, with some holding tennis rackets and others wearing hats or other protective gear. The scene is set against an outdoor setting, which suggests that it might be a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.3, "ram_available_mb": 112055.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10447.7, "ram_available_mb": 112058.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [30.17, 30.17, 30.17, 30.17], "power_watts_avg": 30.17, "power_watts_peak": 30.17, "energy_joules_est": 10.7, "sample_count": 4, "duration_seconds": 0.355}, "timestamp": "2026-01-25T19:57:02.946783"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 292.161, "latencies_ms": [292.161], "images_per_second": 3.423, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "urns of water, a bridge in the background, and people sitting on benches under the bridge create an urban scene with a serene element.\n", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10447.7, "ram_available_mb": 112058.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10448.8, "ram_available_mb": 112057.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [30.17, 27.98, 27.98], "power_watts_avg": 28.71, "power_watts_peak": 30.17, "energy_joules_est": 8.4, "sample_count": 3, "duration_seconds": 0.293}, "timestamp": "2026-01-25T19:57:03.258792"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 176.565, "latencies_ms": [176.565], "images_per_second": 5.664, "prompt_tokens": 759, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\n 1. Bridge  2. Water 3. Sky 4. People 5. Camera 6. Bag 7. Person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.8, "ram_available_mb": 112057.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10451.3, "ram_available_mb": 112055.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [27.98, 27.98], "power_watts_avg": 27.98, "power_watts_peak": 27.98, "energy_joules_est": 4.96, "sample_count": 2, "duration_seconds": 0.177}, "timestamp": "2026-01-25T19:57:03.463575"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 106.223, "latencies_ms": [106.223], "images_per_second": 9.414, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Woman taking a picture of bird under bridge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.3, "ram_available_mb": 112055.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10453.5, "ram_available_mb": 112052.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [27.98, 34.9], "power_watts_avg": 31.44, "power_watts_peak": 34.9, "energy_joules_est": 3.35, "sample_count": 2, "duration_seconds": 0.107}, "timestamp": "2026-01-25T19:57:03.668901"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 282.086, "latencies_ms": [282.086], "images_per_second": 3.545, "prompt_tokens": 757, "response_tokens_est": 45, "n_tiles": 1, "output_text": "\nA woman in a black tank top takes a picture of a bird on the ground, while two other people sit nearby. The group is gathered under an arch bridge over a river, with trees visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.5, "ram_available_mb": 112052.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10445.3, "ram_available_mb": 112061.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [34.9, 34.9, 34.9], "power_watts_avg": 34.9, "power_watts_peak": 34.9, "energy_joules_est": 9.86, "sample_count": 3, "duration_seconds": 0.282}, "timestamp": "2026-01-25T19:57:03.974531"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 127.493, "latencies_ms": [127.493], "images_per_second": 7.844, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A woman is taking a picture of a bird on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.3, "ram_available_mb": 112061.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10445.2, "ram_available_mb": 112061.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.9, 34.03], "power_watts_avg": 34.47, "power_watts_peak": 34.9, "energy_joules_est": 4.4, "sample_count": 2, "duration_seconds": 0.128}, "timestamp": "2026-01-25T19:57:04.180183"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 300.631, "latencies_ms": [300.631], "images_per_second": 3.326, "prompt_tokens": 744, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA young woman with black hair is holding a Hello Kitty phone case and looking at it closely, possibly taking a picture or reading an interesting message on her device.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.2, "ram_available_mb": 112061.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10448.6, "ram_available_mb": 112057.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.03, 34.03, 34.03], "power_watts_avg": 34.03, "power_watts_peak": 34.03, "energy_joules_est": 10.25, "sample_count": 3, "duration_seconds": 0.301}, "timestamp": "2026-01-25T19:57:04.486777"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 94.796, "latencies_ms": [94.796], "images_per_second": 10.549, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Hello kitty phone case", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.6, "ram_available_mb": 112057.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10448.8, "ram_available_mb": 112057.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.15], "power_watts_avg": 33.15, "power_watts_peak": 33.15, "energy_joules_est": 3.15, "sample_count": 1, "duration_seconds": 0.095}, "timestamp": "2026-01-25T19:57:04.590383"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 94.752, "latencies_ms": [94.752], "images_per_second": 10.554, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nThe woman is looking at her cell phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.8, "ram_available_mb": 112057.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.15], "power_watts_avg": 33.15, "power_watts_peak": 33.15, "energy_joules_est": 3.15, "sample_count": 1, "duration_seconds": 0.095}, "timestamp": "2026-01-25T19:57:04.695489"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 176.672, "latencies_ms": [176.672], "images_per_second": 5.66, "prompt_tokens": 757, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA woman in a white shirt stands next to a crowd of people, looking at her phone with a Hello Kitty case.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10462.4, "ram_available_mb": 112043.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.15, 33.15], "power_watts_avg": 33.15, "power_watts_peak": 33.15, "energy_joules_est": 5.86, "sample_count": 2, "duration_seconds": 0.177}, "timestamp": "2026-01-25T19:57:04.899562"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 279.716, "latencies_ms": [279.716], "images_per_second": 3.575, "prompt_tokens": 756, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\n The woman is wearing a white shirt with a black and pink Hello Kitty design. She has her hair pulled back into a ponytail and she's holding up a cell phone to take a picture of herself.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.4, "ram_available_mb": 112043.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10456.6, "ram_available_mb": 112049.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [33.15, 31.97, 31.97], "power_watts_avg": 32.37, "power_watts_peak": 33.15, "energy_joules_est": 9.07, "sample_count": 3, "duration_seconds": 0.28}, "timestamp": "2026-01-25T19:57:05.205073"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 232.034, "latencies_ms": [232.034], "images_per_second": 4.31, "prompt_tokens": 744, "response_tokens_est": 19, "n_tiles": 1, "output_text": "urns of water are on a track, with children riding in front and behind them.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10456.6, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10456.8, "ram_available_mb": 112049.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [31.97, 31.97, 31.97], "power_watts_avg": 31.97, "power_watts_peak": 31.97, "energy_joules_est": 7.44, "sample_count": 3, "duration_seconds": 0.233}, "timestamp": "2026-01-25T19:57:05.517200"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 84.765, "latencies_ms": [84.765], "images_per_second": 11.797, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Speaker", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.8, "ram_available_mb": 112049.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [34.42], "power_watts_avg": 34.42, "power_watts_peak": 34.42, "energy_joules_est": 2.93, "sample_count": 1, "duration_seconds": 0.085}, "timestamp": "2026-01-25T19:57:05.621948"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 178.636, "latencies_ms": [178.636], "images_per_second": 5.598, "prompt_tokens": 763, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\n 1. A group of children on a red train car in front of a stage with speakers behind them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10450.6, "ram_available_mb": 112055.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [34.42, 34.42], "power_watts_avg": 34.42, "power_watts_peak": 34.42, "energy_joules_est": 6.16, "sample_count": 2, "duration_seconds": 0.179}, "timestamp": "2026-01-25T19:57:05.830210"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 121.848, "latencies_ms": [121.848], "images_per_second": 8.207, "prompt_tokens": 757, "response_tokens_est": 15, "n_tiles": 1, "output_text": "urns of water on a track, with children riding in them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.6, "ram_available_mb": 112055.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10448.1, "ram_available_mb": 112058.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [34.42, 34.42], "power_watts_avg": 34.42, "power_watts_peak": 34.42, "energy_joules_est": 4.2, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T19:57:06.035479"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 94.136, "latencies_ms": [94.136], "images_per_second": 10.623, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urns of light and dark brown wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.1, "ram_available_mb": 112058.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10448.3, "ram_available_mb": 112058.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [33.06], "power_watts_avg": 33.06, "power_watts_peak": 33.06, "energy_joules_est": 3.13, "sample_count": 1, "duration_seconds": 0.095}, "timestamp": "2026-01-25T19:57:06.140082"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 277.77, "latencies_ms": [277.77], "images_per_second": 3.6, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "iced tea is served in a small white bowl on a plate, accompanied by a sandwich cut in half and placed on a wooden table.\n ", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10448.3, "ram_available_mb": 112058.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10449.1, "ram_available_mb": 112057.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [33.06, 33.06, 33.06], "power_watts_avg": 33.06, "power_watts_peak": 33.06, "energy_joules_est": 9.19, "sample_count": 3, "duration_seconds": 0.278}, "timestamp": "2026-01-25T19:57:06.450134"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.759, "latencies_ms": [76.759], "images_per_second": 13.028, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Sandwich", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.1, "ram_available_mb": 112057.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10450.8, "ram_available_mb": 112055.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [33.06], "power_watts_avg": 33.06, "power_watts_peak": 33.06, "energy_joules_est": 2.55, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T19:57:06.555906"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 73.432, "latencies_ms": [73.432], "images_per_second": 13.618, "prompt_tokens": 763, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Sandwich on plate", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.8, "ram_available_mb": 112055.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10449.9, "ram_available_mb": 112056.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [29.82], "power_watts_avg": 29.82, "power_watts_peak": 29.82, "energy_joules_est": 2.21, "sample_count": 1, "duration_seconds": 0.074}, "timestamp": "2026-01-25T19:57:06.662185"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 223.029, "latencies_ms": [223.029], "images_per_second": 4.484, "prompt_tokens": 757, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA sandwich with meat, lettuce, and tomato sits on a white plate. The plate is placed on a table next to a small bowl of sauce or dressing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.9, "ram_available_mb": 112056.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10450.5, "ram_available_mb": 112055.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [29.82, 29.82, 29.82], "power_watts_avg": 29.82, "power_watts_peak": 29.82, "energy_joules_est": 6.66, "sample_count": 3, "duration_seconds": 0.223}, "timestamp": "2026-01-25T19:57:06.967590"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 127.337, "latencies_ms": [127.337], "images_per_second": 7.853, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n The plate is white and the food on it appears to be black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.5, "ram_available_mb": 112055.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10450.6, "ram_available_mb": 112055.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_watts_samples": [29.82, 30.65], "power_watts_avg": 30.24, "power_watts_peak": 30.65, "energy_joules_est": 3.86, "sample_count": 2, "duration_seconds": 0.128}, "timestamp": "2026-01-25T19:57:07.172820"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 276.473, "latencies_ms": [276.473], "images_per_second": 3.617, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA person in a wetsuit stands on a white surfboard, paddling out into the ocean with an oar.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.6, "ram_available_mb": 112055.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10450.4, "ram_available_mb": 112055.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_watts_samples": [30.65, 30.65, 30.65], "power_watts_avg": 30.65, "power_watts_peak": 30.65, "energy_joules_est": 8.49, "sample_count": 3, "duration_seconds": 0.277}, "timestamp": "2026-01-25T19:57:07.480111"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 62.398, "latencies_ms": [62.398], "images_per_second": 16.026, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.4, "ram_available_mb": 112055.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10448.9, "ram_available_mb": 112057.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_watts_samples": [30.65], "power_watts_avg": 30.65, "power_watts_peak": 30.65, "energy_joules_est": 1.94, "sample_count": 1, "duration_seconds": 0.063}, "timestamp": "2026-01-25T19:57:07.585524"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 125.572, "latencies_ms": [125.572], "images_per_second": 7.964, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n 1. Surfer on a white surfboard in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.9, "ram_available_mb": 112057.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10456.5, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_watts_samples": [31.06, 31.06], "power_watts_avg": 31.06, "power_watts_peak": 31.06, "energy_joules_est": 3.91, "sample_count": 2, "duration_seconds": 0.126}, "timestamp": "2026-01-25T19:57:07.791927"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 163.784, "latencies_ms": [163.784], "images_per_second": 6.106, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA person in a wetsuit stands on a surfboard, paddling out into the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.5, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [31.06, 31.06], "power_watts_avg": 31.06, "power_watts_peak": 31.06, "energy_joules_est": 5.1, "sample_count": 2, "duration_seconds": 0.164}, "timestamp": "2026-01-25T19:57:07.997254"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 140.568, "latencies_ms": [140.568], "images_per_second": 7.114, "prompt_tokens": 756, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n The image is a black and white photo of a person paddle boarding in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10453.6, "ram_available_mb": 112052.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [30.09, 30.09], "power_watts_avg": 30.09, "power_watts_peak": 30.09, "energy_joules_est": 4.24, "sample_count": 2, "duration_seconds": 0.141}, "timestamp": "2026-01-25T19:57:08.202650"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 432.445, "latencies_ms": [432.445], "images_per_second": 2.312, "prompt_tokens": 744, "response_tokens_est": 58, "n_tiles": 1, "output_text": "\nA desk with a computer setup, including two monitors and a keyboard, is situated in front of a window. A laptop sits on the left side of the desk, while another monitor is placed to its right. The desk also holds a mouse and speakers, completing the computer setup.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.6, "ram_available_mb": 112052.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10464.7, "ram_available_mb": 112041.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [30.09, 30.09, 30.09, 30.58, 30.58], "power_watts_avg": 30.28, "power_watts_peak": 30.58, "energy_joules_est": 13.11, "sample_count": 5, "duration_seconds": 0.433}, "timestamp": "2026-01-25T19:57:08.709989"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 102.876, "latencies_ms": [102.876], "images_per_second": 9.72, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Laptop (white) - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.7, "ram_available_mb": 112041.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [30.58, 30.58], "power_watts_avg": 30.58, "power_watts_peak": 30.58, "energy_joules_est": 3.15, "sample_count": 2, "duration_seconds": 0.103}, "timestamp": "2026-01-25T19:57:08.914878"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 131.206, "latencies_ms": [131.206], "images_per_second": 7.622, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n 1. White monitor on desk with computer and keyboard in front of it.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10457.1, "ram_available_mb": 112049.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [30.58, 31.1], "power_watts_avg": 30.84, "power_watts_peak": 31.1, "energy_joules_est": 4.06, "sample_count": 2, "duration_seconds": 0.132}, "timestamp": "2026-01-25T19:57:09.120022"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 135.535, "latencies_ms": [135.535], "images_per_second": 7.378, "prompt_tokens": 757, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nA desk with a laptop, desktop computer monitor, keyboard, mouse, and speakers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.1, "ram_available_mb": 112049.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10463.5, "ram_available_mb": 112042.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [31.1, 31.1], "power_watts_avg": 31.1, "power_watts_peak": 31.1, "energy_joules_est": 4.22, "sample_count": 2, "duration_seconds": 0.136}, "timestamp": "2026-01-25T19:57:09.325483"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 79.14, "latencies_ms": [79.14], "images_per_second": 12.636, "prompt_tokens": 756, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urns on the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.5, "ram_available_mb": 112042.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [31.1], "power_watts_avg": 31.1, "power_watts_peak": 31.1, "energy_joules_est": 2.48, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T19:57:09.430748"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 295.072, "latencies_ms": [295.072], "images_per_second": 3.389, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA highway overpass with a green sign for Hollywood Blvd and North Ventura 101, along with other signs such as exit 20 and exit 101.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [31.1, 25.55, 25.55], "power_watts_avg": 27.4, "power_watts_peak": 31.1, "energy_joules_est": 8.1, "sample_count": 3, "duration_seconds": 0.296}, "timestamp": "2026-01-25T19:57:09.744292"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.709, "latencies_ms": [76.709], "images_per_second": 13.036, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Highway 101: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [25.55], "power_watts_avg": 25.55, "power_watts_peak": 25.55, "energy_joules_est": 1.96, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T19:57:09.848675"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 113.846, "latencies_ms": [113.846], "images_per_second": 8.784, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. A green highway sign pointing to Hollywood Blvd.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [25.55, 25.55], "power_watts_avg": 25.55, "power_watts_peak": 25.55, "energy_joules_est": 2.92, "sample_count": 2, "duration_seconds": 0.114}, "timestamp": "2026-01-25T19:57:10.056209"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 599.698, "latencies_ms": [599.698], "images_per_second": 1.668, "prompt_tokens": 757, "response_tokens_est": 110, "n_tiles": 1, "output_text": "\nThe image shows a busy highway with multiple lanes filled with cars, including a black SUV. The highway has green directional signs pointing to various destinations such as \"North Ventura 101\", \"Hollywood Blvd\", and \"Sunset Blvd\". There are also two traffic lights visible in the scene, one near the center of the image and another further back on the right side.\n\nIn addition to the vehicles, there is a person standing at the edge of the highway, possibly waiting for their turn to cross or just observing the traffic flow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [31.73, 31.73, 31.73, 31.73, 31.73, 36.44], "power_watts_avg": 32.51, "power_watts_peak": 36.44, "energy_joules_est": 19.51, "sample_count": 6, "duration_seconds": 0.6}, "timestamp": "2026-01-25T19:57:10.663520"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 87.698, "latencies_ms": [87.698], "images_per_second": 11.403, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The sky is blue and clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10461.8, "ram_available_mb": 112044.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [36.44], "power_watts_avg": 36.44, "power_watts_peak": 36.44, "energy_joules_est": 3.2, "sample_count": 1, "duration_seconds": 0.088}, "timestamp": "2026-01-25T19:57:10.769034"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 282.156, "latencies_ms": [282.156], "images_per_second": 3.544, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA red double-decker bus is driving down a street, with an advertisement on its side for \"Alldwych\".", "error": null, "sys_before": {"cpu_percent": 3.7, "ram_used_mb": 10461.8, "ram_available_mb": 112044.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 75.0}, "power_stats": {"power_watts_samples": [36.44, 36.44, 36.44], "power_watts_avg": 36.44, "power_watts_peak": 36.44, "energy_joules_est": 10.29, "sample_count": 3, "duration_seconds": 0.282}, "timestamp": "2026-01-25T19:57:11.084703"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 80.404, "latencies_ms": [80.404], "images_per_second": 12.437, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bus: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 75.0}, "power_stats": {"power_watts_samples": [36.86], "power_watts_avg": 36.86, "power_watts_peak": 36.86, "energy_joules_est": 2.98, "sample_count": 1, "duration_seconds": 0.081}, "timestamp": "2026-01-25T19:57:11.189655"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 161.751, "latencies_ms": [161.751], "images_per_second": 6.182, "prompt_tokens": 763, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\n 1. A red double decker bus driving down a street with other vehicles and pedestrians in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10462.1, "ram_available_mb": 112044.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 75.0}, "power_stats": {"power_watts_samples": [36.86, 36.86], "power_watts_avg": 36.86, "power_watts_peak": 36.86, "energy_joules_est": 5.97, "sample_count": 2, "duration_seconds": 0.162}, "timestamp": "2026-01-25T19:57:11.398681"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 274.315, "latencies_ms": [274.315], "images_per_second": 3.645, "prompt_tokens": 757, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nA red double-decker bus drives down a city street, with tall buildings in the background. The bus has an advertisement on its side, and there are several people walking along the sidewalk near the bus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.1, "ram_available_mb": 112044.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 75.0}, "power_stats": {"power_watts_samples": [36.86, 34.1, 34.1], "power_watts_avg": 35.02, "power_watts_peak": 36.86, "energy_joules_est": 9.63, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T19:57:11.704085"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 111.137, "latencies_ms": [111.137], "images_per_second": 8.998, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The red double decker bus is driving down the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 75.0}, "power_stats": {"power_watts_samples": [34.1, 34.1], "power_watts_avg": 34.1, "power_watts_peak": 34.1, "energy_joules_est": 3.8, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T19:57:11.909267"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 264.898, "latencies_ms": [264.898], "images_per_second": 3.775, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA black and white cat is laying on a laptop computer, which has a silver keyboard in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10464.7, "ram_available_mb": 112041.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [34.1, 28.76, 28.76], "power_watts_avg": 30.54, "power_watts_peak": 34.1, "energy_joules_est": 8.11, "sample_count": 3, "duration_seconds": 0.266}, "timestamp": "2026-01-25T19:57:12.220911"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 64.164, "latencies_ms": [64.164], "images_per_second": 15.585, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Cat", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10464.7, "ram_available_mb": 112041.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [28.76], "power_watts_avg": 28.76, "power_watts_peak": 28.76, "energy_joules_est": 1.86, "sample_count": 1, "duration_seconds": 0.064}, "timestamp": "2026-01-25T19:57:12.326553"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 172.352, "latencies_ms": [172.352], "images_per_second": 5.802, "prompt_tokens": 763, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\n 1. Cat on laptop keyboard - this is a cat laying down in front of an open laptop computer.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10466.7, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [28.76, 28.76], "power_watts_avg": 28.76, "power_watts_peak": 28.76, "energy_joules_est": 4.97, "sample_count": 2, "duration_seconds": 0.173}, "timestamp": "2026-01-25T19:57:12.535744"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 38.941, "latencies_ms": [38.941], "images_per_second": 25.68, "prompt_tokens": 757, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.7, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 10472.8, "ram_available_mb": 112033.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [32.67], "power_watts_avg": 32.67, "power_watts_peak": 32.67, "energy_joules_est": 1.28, "sample_count": 1, "duration_seconds": 0.039}, "timestamp": "2026-01-25T19:57:12.640936"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 114.649, "latencies_ms": [114.649], "images_per_second": 8.722, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n 1. The cat is black and white with a gray face.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.8, "ram_available_mb": 112033.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10471.8, "ram_available_mb": 112034.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [32.67, 32.67], "power_watts_avg": 32.67, "power_watts_peak": 32.67, "energy_joules_est": 3.75, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T19:57:12.848991"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 259.045, "latencies_ms": [259.045], "images_per_second": 3.86, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nTwo airplanes are flying over a bridge, with the larger airplane on top and the smaller one below it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.8, "ram_available_mb": 112034.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [32.67, 32.67, 31.7], "power_watts_avg": 32.35, "power_watts_peak": 32.67, "energy_joules_est": 8.4, "sample_count": 3, "duration_seconds": 0.26}, "timestamp": "2026-01-25T19:57:13.158394"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 73.052, "latencies_ms": [73.052], "images_per_second": 13.689, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Plane 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [31.7], "power_watts_avg": 31.7, "power_watts_peak": 31.7, "energy_joules_est": 2.33, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T19:57:13.264212"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 88.206, "latencies_ms": [88.206], "images_per_second": 11.337, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\nThe airplanes are flying over a bridge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [31.7], "power_watts_avg": 31.7, "power_watts_peak": 31.7, "energy_joules_est": 2.81, "sample_count": 1, "duration_seconds": 0.089}, "timestamp": "2026-01-25T19:57:13.370765"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 415.214, "latencies_ms": [415.214], "images_per_second": 2.408, "prompt_tokens": 757, "response_tokens_est": 71, "n_tiles": 1, "output_text": "\nThe image shows a large bridge spanning across a body of water, with two airplanes flying above it. The sky appears cloudy, creating an interesting contrast between the clear blue sky and the overcast weather conditions. In the distance, there are buildings visible on one side of the bridge, suggesting that this scene takes place in a city or urban area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10453.9, "ram_available_mb": 112052.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [31.7, 31.7, 28.8, 28.8, 28.8], "power_watts_avg": 29.96, "power_watts_peak": 31.7, "energy_joules_est": 12.45, "sample_count": 5, "duration_seconds": 0.415}, "timestamp": "2026-01-25T19:57:13.877698"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 123.296, "latencies_ms": [123.296], "images_per_second": 8.111, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The bridge is made of steel and has a black color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.9, "ram_available_mb": 112052.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10449.8, "ram_available_mb": 112056.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [28.8, 28.8], "power_watts_avg": 28.8, "power_watts_peak": 28.8, "energy_joules_est": 3.57, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T19:57:14.084892"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 288.913, "latencies_ms": [288.913], "images_per_second": 3.461, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA mother zebra and her baby are standing in a field, with the baby zebra nursing from its mother's udder.", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 10449.8, "ram_available_mb": 112056.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10461.3, "ram_available_mb": 112045.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [31.54, 31.54, 31.54], "power_watts_avg": 31.54, "power_watts_peak": 31.54, "energy_joules_est": 9.13, "sample_count": 3, "duration_seconds": 0.29}, "timestamp": "2026-01-25T19:57:14.399338"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 68.021, "latencies_ms": [68.021], "images_per_second": 14.701, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Ear", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.3, "ram_available_mb": 112045.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10458.1, "ram_available_mb": 112048.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [31.54], "power_watts_avg": 31.54, "power_watts_peak": 31.54, "energy_joules_est": 2.15, "sample_count": 1, "duration_seconds": 0.068}, "timestamp": "2026-01-25T19:57:14.504674"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 109.453, "latencies_ms": [109.453], "images_per_second": 9.136, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe baby zebra is standing next to its mother.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.1, "ram_available_mb": 112048.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10463.4, "ram_available_mb": 112042.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [34.28, 34.28], "power_watts_avg": 34.28, "power_watts_peak": 34.28, "energy_joules_est": 3.76, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T19:57:14.713355"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 135.566, "latencies_ms": [135.566], "images_per_second": 7.376, "prompt_tokens": 757, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A mother zebra nursing her baby in a field of tall grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10463.9, "ram_available_mb": 112042.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [34.28, 34.28], "power_watts_avg": 34.28, "power_watts_peak": 34.28, "energy_joules_est": 4.66, "sample_count": 2, "duration_seconds": 0.136}, "timestamp": "2026-01-25T19:57:14.920336"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 217.631, "latencies_ms": [217.631], "images_per_second": 4.595, "prompt_tokens": 756, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\n The image is a black and white photograph of two zebras. One zebra has its head on the other's back while they are standing in tall grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.9, "ram_available_mb": 112042.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [34.28, 28.81, 28.81], "power_watts_avg": 30.63, "power_watts_peak": 34.28, "energy_joules_est": 6.69, "sample_count": 3, "duration_seconds": 0.218}, "timestamp": "2026-01-25T19:57:15.226974"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 264.53, "latencies_ms": [264.53], "images_per_second": 3.78, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urns on a table, purple walls, and a bed with a colorful comforter are present in a room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [28.81, 28.81, 28.81], "power_watts_avg": 28.81, "power_watts_peak": 28.81, "energy_joules_est": 7.63, "sample_count": 3, "duration_seconds": 0.265}, "timestamp": "2026-01-25T19:57:15.535206"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 62.876, "latencies_ms": [62.876], "images_per_second": 15.904, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Bed", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [30.3], "power_watts_avg": 30.3, "power_watts_peak": 30.3, "energy_joules_est": 1.92, "sample_count": 1, "duration_seconds": 0.063}, "timestamp": "2026-01-25T19:57:15.640692"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 63.95, "latencies_ms": [63.95], "images_per_second": 15.637, "prompt_tokens": 763, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Bed", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [30.3], "power_watts_avg": 30.3, "power_watts_peak": 30.3, "energy_joules_est": 1.94, "sample_count": 1, "duration_seconds": 0.064}, "timestamp": "2026-01-25T19:57:15.750155"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 396.421, "latencies_ms": [396.421], "images_per_second": 2.523, "prompt_tokens": 757, "response_tokens_est": 69, "n_tiles": 1, "output_text": "\nThe image shows a bedroom with purple walls, featuring a bed covered in a colorful comforter. The room has white doors leading to an outside balcony, providing a view of the cityscape. A dining table and chair are present in the room, suggesting that this space may also serve as a living area or multi-purpose room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.3, 30.3, 30.3, 33.05], "power_watts_avg": 30.99, "power_watts_peak": 33.05, "energy_joules_est": 12.29, "sample_count": 4, "duration_seconds": 0.397}, "timestamp": "2026-01-25T19:57:16.154095"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 290.663, "latencies_ms": [290.663], "images_per_second": 3.44, "prompt_tokens": 756, "response_tokens_est": 49, "n_tiles": 1, "output_text": "\n The room is painted purple and has a gray stone wall. There are two windows with white curtains that let in natural light. A black metal bed frame sits on the floor next to a table and chair set up for dining or relaxation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10461.9, "ram_available_mb": 112044.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.05, 33.05, 33.05], "power_watts_avg": 33.05, "power_watts_peak": 33.05, "energy_joules_est": 9.61, "sample_count": 3, "duration_seconds": 0.291}, "timestamp": "2026-01-25T19:57:16.458399"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 309.466, "latencies_ms": [309.466], "images_per_second": 3.231, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA purple bus with a sign on it that says \"Metropolitan Transport\" is driving down a street, and there are two people walking nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.9, "ram_available_mb": 112044.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10464.4, "ram_available_mb": 112041.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.05, 32.47, 32.47, 32.47], "power_watts_avg": 32.62, "power_watts_peak": 33.05, "energy_joules_est": 10.11, "sample_count": 4, "duration_seconds": 0.31}, "timestamp": "2026-01-25T19:57:16.868247"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 87.433, "latencies_ms": [87.433], "images_per_second": 11.437, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bus: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.4, "ram_available_mb": 112041.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.47], "power_watts_avg": 32.47, "power_watts_peak": 32.47, "energy_joules_est": 2.84, "sample_count": 1, "duration_seconds": 0.088}, "timestamp": "2026-01-25T19:57:16.972013"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 107.276, "latencies_ms": [107.276], "images_per_second": 9.322, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nA purple bus is driving down a street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [32.47, 36.09], "power_watts_avg": 34.28, "power_watts_peak": 36.09, "energy_joules_est": 3.7, "sample_count": 2, "duration_seconds": 0.108}, "timestamp": "2026-01-25T19:57:17.176857"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 182.466, "latencies_ms": [182.466], "images_per_second": 5.48, "prompt_tokens": 757, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA purple bus with a sign on top that says \"metrocentre via Ben\" driving down a street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10465.4, "ram_available_mb": 112040.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [36.09, 36.09], "power_watts_avg": 36.09, "power_watts_peak": 36.09, "energy_joules_est": 6.6, "sample_count": 2, "duration_seconds": 0.183}, "timestamp": "2026-01-25T19:57:17.381515"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 104.386, "latencies_ms": [104.386], "images_per_second": 9.58, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The bus is purple and has a white front.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.4, "ram_available_mb": 112040.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10470.0, "ram_available_mb": 112036.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [36.09, 36.09], "power_watts_avg": 36.09, "power_watts_peak": 36.09, "energy_joules_est": 3.77, "sample_count": 2, "duration_seconds": 0.105}, "timestamp": "2026-01-25T19:57:17.586615"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 219.437, "latencies_ms": [219.437], "images_per_second": 4.557, "prompt_tokens": 744, "response_tokens_est": 18, "n_tiles": 1, "output_text": "urn of green apples in a white bowl, with two on top and four below.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.0, "ram_available_mb": 112036.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.28, 30.28, 30.28], "power_watts_avg": 30.28, "power_watts_peak": 30.28, "energy_joules_est": 6.65, "sample_count": 3, "duration_seconds": 0.22}, "timestamp": "2026-01-25T19:57:17.895869"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 72.714, "latencies_ms": [72.714], "images_per_second": 13.752, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Apple", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.28], "power_watts_avg": 30.28, "power_watts_peak": 30.28, "energy_joules_est": 2.23, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T19:57:18.001069"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 113.648, "latencies_ms": [113.648], "images_per_second": 8.799, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nThe green apples are in a bowl on top of a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [28.5, 28.5], "power_watts_avg": 28.5, "power_watts_peak": 28.5, "energy_joules_est": 3.25, "sample_count": 2, "duration_seconds": 0.114}, "timestamp": "2026-01-25T19:57:18.206215"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 203.317, "latencies_ms": [203.317], "images_per_second": 4.918, "prompt_tokens": 757, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nIn a room, there is a white bowl filled with green apples. The bowl contains several green apples that are placed in various positions within the bowl.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [28.5, 28.5, 28.5], "power_watts_avg": 28.5, "power_watts_peak": 28.5, "energy_joules_est": 5.8, "sample_count": 3, "duration_seconds": 0.204}, "timestamp": "2026-01-25T19:57:18.511126"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 396.298, "latencies_ms": [396.298], "images_per_second": 2.523, "prompt_tokens": 756, "response_tokens_est": 67, "n_tiles": 1, "output_text": "\n The image shows a white bowl filled with green apples. The apples are shiny and appear to be freshly washed or polished. They are placed in the center of the frame, drawing attention as the main subject of the photo. The background is blurred, making it difficult to discern any other objects or details beyond the bowl of apples.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10467.1, "ram_available_mb": 112039.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [29.1, 29.1, 29.1, 29.1], "power_watts_avg": 29.1, "power_watts_peak": 29.1, "energy_joules_est": 11.55, "sample_count": 4, "duration_seconds": 0.397}, "timestamp": "2026-01-25T19:57:18.915205"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 324.8, "latencies_ms": [324.8], "images_per_second": 3.079, "prompt_tokens": 744, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA baseball game is in progress, with a batter swinging at an incoming pitch and a catcher crouched behind him. The umpire stands nearby, observing the play closely.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.1, "ram_available_mb": 112039.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [29.1, 29.71, 29.71, 29.71], "power_watts_avg": 29.56, "power_watts_peak": 29.71, "energy_joules_est": 9.62, "sample_count": 4, "duration_seconds": 0.325}, "timestamp": "2026-01-25T19:57:19.325386"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 99.966, "latencies_ms": [99.966], "images_per_second": 10.003, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1 pitcher 2 batter 3 catcher 4 umpire", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [29.71], "power_watts_avg": 29.71, "power_watts_peak": 29.71, "energy_joules_est": 2.99, "sample_count": 1, "duration_seconds": 0.101}, "timestamp": "2026-01-25T19:57:19.429739"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 77.12, "latencies_ms": [77.12], "images_per_second": 12.967, "prompt_tokens": 763, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Umpire", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10456.3, "ram_available_mb": 112050.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [29.71], "power_watts_avg": 29.71, "power_watts_peak": 29.71, "energy_joules_est": 2.3, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T19:57:19.533153"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 434.364, "latencies_ms": [434.364], "images_per_second": 2.302, "prompt_tokens": 757, "response_tokens_est": 76, "n_tiles": 1, "output_text": "\nIn the image, a baseball game is taking place on a field. A batter stands at home plate holding a bat, ready to swing. Behind him, a catcher in a red shirt crouches with his glove outstretched, prepared for the pitch. An umpire stands nearby, observing the play closely. The scene captures the intensity and excitement of a professional baseball game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.3, "ram_available_mb": 112050.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10451.1, "ram_available_mb": 112055.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.82, 34.82, 34.82, 34.82, 34.82], "power_watts_avg": 34.82, "power_watts_peak": 34.82, "energy_joules_est": 15.14, "sample_count": 5, "duration_seconds": 0.435}, "timestamp": "2026-01-25T19:57:20.039902"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 118.911, "latencies_ms": [118.911], "images_per_second": 8.41, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. The umpire is wearing a blue shirt and black pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.1, "ram_available_mb": 112055.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [37.08, 37.08], "power_watts_avg": 37.08, "power_watts_peak": 37.08, "energy_joules_est": 4.42, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T19:57:20.245591"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 293.906, "latencies_ms": [293.906], "images_per_second": 3.402, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA table is set with a white cake, plates of cheese and grapes, wine glasses, and utensils such as forks and knives.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10458.5, "ram_available_mb": 112047.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [37.08, 37.08, 37.08], "power_watts_avg": 37.08, "power_watts_peak": 37.08, "energy_joules_est": 10.91, "sample_count": 3, "duration_seconds": 0.294}, "timestamp": "2026-01-25T19:57:20.553848"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.256, "latencies_ms": [82.256], "images_per_second": 12.157, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Cake - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.5, "ram_available_mb": 112047.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.8], "power_watts_avg": 34.8, "power_watts_peak": 34.8, "energy_joules_est": 2.87, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T19:57:20.658769"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 143.214, "latencies_ms": [143.214], "images_per_second": 6.983, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n A white plate with a cake on it sits next to several glasses of wine.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.8, 34.8], "power_watts_avg": 34.8, "power_watts_peak": 34.8, "energy_joules_est": 5.0, "sample_count": 2, "duration_seconds": 0.144}, "timestamp": "2026-01-25T19:57:20.864650"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 419.196, "latencies_ms": [419.196], "images_per_second": 2.386, "prompt_tokens": 757, "response_tokens_est": 74, "n_tiles": 1, "output_text": "\nThe image shows a table covered with a red tablecloth, upon which sits an assortment of food items. There are several plates filled with cheese, grapes, and other snacks arranged on the table. A cake decorated with berries is also present on the table. The table is surrounded by wine glasses, suggesting that this gathering may be a celebration or a social event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10459.3, "ram_available_mb": 112047.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [34.8, 34.8, 30.37, 30.37, 30.37], "power_watts_avg": 32.14, "power_watts_peak": 34.8, "energy_joules_est": 13.48, "sample_count": 5, "duration_seconds": 0.419}, "timestamp": "2026-01-25T19:57:21.370983"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 197.437, "latencies_ms": [197.437], "images_per_second": 5.065, "prompt_tokens": 756, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\n The table is covered with a red cloth and has plates of food on it. There are wine glasses and cups placed around the table as well.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.3, "ram_available_mb": 112047.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10449.8, "ram_available_mb": 112056.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [30.37, 30.37], "power_watts_avg": 30.37, "power_watts_peak": 30.37, "energy_joules_est": 6.02, "sample_count": 2, "duration_seconds": 0.198}, "timestamp": "2026-01-25T19:57:21.576023"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 302.411, "latencies_ms": [302.411], "images_per_second": 3.307, "prompt_tokens": 744, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA person is riding a wave on a blue surfboard in the ocean, with their arm raised high above them as they navigate the greenish-blue water.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10449.8, "ram_available_mb": 112056.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10450.1, "ram_available_mb": 112056.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [31.37, 31.37, 31.37, 31.37], "power_watts_avg": 31.37, "power_watts_peak": 31.37, "energy_joules_est": 9.5, "sample_count": 4, "duration_seconds": 0.303}, "timestamp": "2026-01-25T19:57:21.989475"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 66.414, "latencies_ms": [66.414], "images_per_second": 15.057, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.1, "ram_available_mb": 112056.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10448.7, "ram_available_mb": 112057.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [36.87], "power_watts_avg": 36.87, "power_watts_peak": 36.87, "energy_joules_est": 2.46, "sample_count": 1, "duration_seconds": 0.067}, "timestamp": "2026-01-25T19:57:22.093921"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 121.453, "latencies_ms": [121.453], "images_per_second": 8.234, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe man is on a blue surfboard in the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.7, "ram_available_mb": 112057.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10440.7, "ram_available_mb": 112065.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [36.87, 36.87], "power_watts_avg": 36.87, "power_watts_peak": 36.87, "energy_joules_est": 4.49, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T19:57:22.299406"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 299.099, "latencies_ms": [299.099], "images_per_second": 3.343, "prompt_tokens": 757, "response_tokens_est": 49, "n_tiles": 1, "output_text": "\nIn the image, a person wearing a black shirt is riding a blue surfboard on top of a wave in the ocean. The surfer appears to be enjoying their time as they skillfully navigate through the greenish-blue water.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10440.7, "ram_available_mb": 112065.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10443.1, "ram_available_mb": 112063.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [36.87, 36.87, 32.51], "power_watts_avg": 35.41, "power_watts_peak": 36.87, "energy_joules_est": 10.61, "sample_count": 3, "duration_seconds": 0.3}, "timestamp": "2026-01-25T19:57:22.605137"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 372.373, "latencies_ms": [372.373], "images_per_second": 2.685, "prompt_tokens": 756, "response_tokens_est": 62, "n_tiles": 1, "output_text": "\n The image shows a man on a blue surfboard in the ocean. He is wearing a black shirt and appears to be enjoying his time riding the waves. The water surrounding him has a greenish hue, which could indicate that it's a cloudy day or there are certain elements present in the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10443.1, "ram_available_mb": 112063.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10445.0, "ram_available_mb": 112061.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.51, 32.51, 32.51, 32.51], "power_watts_avg": 32.51, "power_watts_peak": 32.51, "energy_joules_est": 12.11, "sample_count": 4, "duration_seconds": 0.373}, "timestamp": "2026-01-25T19:57:23.012693"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 181.228, "latencies_ms": [181.228], "images_per_second": 5.518, "prompt_tokens": 744, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urn of water on a table", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10445.0, "ram_available_mb": 112061.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10445.4, "ram_available_mb": 112060.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.66, 31.66], "power_watts_avg": 31.66, "power_watts_peak": 31.66, "energy_joules_est": 5.75, "sample_count": 2, "duration_seconds": 0.182}, "timestamp": "2026-01-25T19:57:23.222298"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 89.169, "latencies_ms": [89.169], "images_per_second": 11.215, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Person 2.3.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.4, "ram_available_mb": 112060.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 10445.6, "ram_available_mb": 112060.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.66], "power_watts_avg": 31.66, "power_watts_peak": 31.66, "energy_joules_est": 2.84, "sample_count": 1, "duration_seconds": 0.09}, "timestamp": "2026-01-25T19:57:23.326655"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 101.65, "latencies_ms": [101.65], "images_per_second": 9.838, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Boy in front row with tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.6, "ram_available_mb": 112060.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10445.4, "ram_available_mb": 112060.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.66, 31.66], "power_watts_avg": 31.66, "power_watts_peak": 31.66, "energy_joules_est": 3.22, "sample_count": 2, "duration_seconds": 0.102}, "timestamp": "2026-01-25T19:57:23.532227"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 46.053, "latencies_ms": [46.053], "images_per_second": 21.714, "prompt_tokens": 757, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.4, "ram_available_mb": 112060.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 10448.8, "ram_available_mb": 112057.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.33], "power_watts_avg": 35.33, "power_watts_peak": 35.33, "energy_joules_est": 1.65, "sample_count": 1, "duration_seconds": 0.047}, "timestamp": "2026-01-25T19:57:23.638119"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 43.829, "latencies_ms": [43.829], "images_per_second": 22.816, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10448.8, "ram_available_mb": 112057.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.0, "ram_used_mb": 10453.5, "ram_available_mb": 112052.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.33], "power_watts_avg": 35.33, "power_watts_peak": 35.33, "energy_joules_est": 1.56, "sample_count": 1, "duration_seconds": 0.044}, "timestamp": "2026-01-25T19:57:23.743158"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 307.191, "latencies_ms": [307.191], "images_per_second": 3.255, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\n A white plate holds a variety of breads, including baguettes and pita bread, along with a small bowl filled with sauce.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10453.5, "ram_available_mb": 112052.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10455.9, "ram_available_mb": 112050.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [35.33, 35.33, 35.33, 30.37], "power_watts_avg": 34.09, "power_watts_peak": 35.33, "energy_joules_est": 10.48, "sample_count": 4, "duration_seconds": 0.308}, "timestamp": "2026-01-25T19:57:24.156280"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 60.222, "latencies_ms": [60.222], "images_per_second": 16.605, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n Bread: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.9, "ram_available_mb": 112050.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10452.7, "ram_available_mb": 112053.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [30.37], "power_watts_avg": 30.37, "power_watts_peak": 30.37, "energy_joules_est": 1.84, "sample_count": 1, "duration_seconds": 0.061}, "timestamp": "2026-01-25T19:57:24.261548"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 135.053, "latencies_ms": [135.053], "images_per_second": 7.405, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "urn of sauce on a table next to bread sticks and wine glass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.7, "ram_available_mb": 112053.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10450.9, "ram_available_mb": 112055.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [30.37, 30.37], "power_watts_avg": 30.37, "power_watts_peak": 30.37, "energy_joules_est": 4.12, "sample_count": 2, "duration_seconds": 0.136}, "timestamp": "2026-01-25T19:57:24.467224"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 203.256, "latencies_ms": [203.256], "images_per_second": 4.92, "prompt_tokens": 757, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA white plate with a fork, breadsticks, and sauce sits on a wooden table. A glass of wine is also present on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.9, "ram_available_mb": 112055.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10451.2, "ram_available_mb": 112055.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [30.37, 25.53, 25.53], "power_watts_avg": 27.14, "power_watts_peak": 30.37, "energy_joules_est": 5.53, "sample_count": 3, "duration_seconds": 0.204}, "timestamp": "2026-01-25T19:57:24.774085"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 75.219, "latencies_ms": [75.219], "images_per_second": 13.294, "prompt_tokens": 756, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urn of water on table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.2, "ram_available_mb": 112055.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [25.53], "power_watts_avg": 25.53, "power_watts_peak": 25.53, "energy_joules_est": 1.93, "sample_count": 1, "duration_seconds": 0.075}, "timestamp": "2026-01-25T19:57:24.879217"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 252.916, "latencies_ms": [252.916], "images_per_second": 3.954, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "iced skiers in mid-air, with a person wearing a green and white jacket on the ground below.", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10449.9, "ram_available_mb": 112056.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 34.0}, "power_stats": {"power_watts_samples": [25.53, 25.53, 27.04], "power_watts_avg": 26.03, "power_watts_peak": 27.04, "energy_joules_est": 6.6, "sample_count": 3, "duration_seconds": 0.254}, "timestamp": "2026-01-25T19:57:25.191861"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 74.628, "latencies_ms": [74.628], "images_per_second": 13.4, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Skier in air", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.9, "ram_available_mb": 112056.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10452.3, "ram_available_mb": 112054.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 34.0}, "power_stats": {"power_watts_samples": [27.04], "power_watts_avg": 27.04, "power_watts_peak": 27.04, "energy_joules_est": 2.03, "sample_count": 1, "duration_seconds": 0.075}, "timestamp": "2026-01-25T19:57:25.298756"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 126.236, "latencies_ms": [126.236], "images_per_second": 7.922, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Person in front of skier with arms raised.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10452.3, "ram_available_mb": 112054.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10454.8, "ram_available_mb": 112051.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 34.0}, "power_stats": {"power_watts_samples": [27.04, 27.04], "power_watts_avg": 27.04, "power_watts_peak": 27.04, "energy_joules_est": 3.44, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T19:57:25.504753"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 267.125, "latencies_ms": [267.125], "images_per_second": 3.744, "prompt_tokens": 757, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\nIn a snowy mountain landscape, two skiers are captured mid-air performing jumps. One skier has their arms raised high in the air while the other skier stands nearby with ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.8, "ram_available_mb": 112051.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 34.0}, "power_stats": {"power_watts_samples": [30.48, 30.48, 30.48], "power_watts_avg": 30.48, "power_watts_peak": 30.48, "energy_joules_est": 8.15, "sample_count": 3, "duration_seconds": 0.268}, "timestamp": "2026-01-25T19:57:25.810662"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 104.602, "latencies_ms": [104.602], "images_per_second": 9.56, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The skier is wearing a colorful jacket and pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10452.3, "ram_available_mb": 112054.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [30.48, 30.48], "power_watts_avg": 30.48, "power_watts_peak": 30.48, "energy_joules_est": 3.2, "sample_count": 2, "duration_seconds": 0.105}, "timestamp": "2026-01-25T19:57:26.016926"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 252.561, "latencies_ms": [252.561], "images_per_second": 3.959, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA skier stands on a snowy mountain, looking out over the landscape with snow-covered trees in the background.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10452.3, "ram_available_mb": 112054.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10456.0, "ram_available_mb": 112050.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [29.32, 29.32, 29.32], "power_watts_avg": 29.32, "power_watts_peak": 29.32, "energy_joules_est": 7.42, "sample_count": 3, "duration_seconds": 0.253}, "timestamp": "2026-01-25T19:57:26.327540"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 80.997, "latencies_ms": [80.997], "images_per_second": 12.346, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Person skiing in the snow", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.0, "ram_available_mb": 112050.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10456.0, "ram_available_mb": 112050.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [29.32], "power_watts_avg": 29.32, "power_watts_peak": 29.32, "energy_joules_est": 2.38, "sample_count": 1, "duration_seconds": 0.081}, "timestamp": "2026-01-25T19:57:26.432619"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 114.179, "latencies_ms": [114.179], "images_per_second": 8.758, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n Person in green shirt and black pants skiing on a mountain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.0, "ram_available_mb": 112050.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [29.32, 31.2], "power_watts_avg": 30.26, "power_watts_peak": 31.2, "energy_joules_est": 3.46, "sample_count": 2, "duration_seconds": 0.114}, "timestamp": "2026-01-25T19:57:26.641288"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 144.041, "latencies_ms": [144.041], "images_per_second": 6.942, "prompt_tokens": 757, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA person in a green shirt stands on skis, looking out over a snowy mountain range.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [31.2, 31.2], "power_watts_avg": 31.2, "power_watts_peak": 31.2, "energy_joules_est": 4.5, "sample_count": 2, "duration_seconds": 0.144}, "timestamp": "2026-01-25T19:57:26.846292"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 113.768, "latencies_ms": [113.768], "images_per_second": 8.79, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The skier is wearing a green shirt and black pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 19.0}, "power_stats": {"power_watts_samples": [31.2, 31.2], "power_watts_avg": 31.2, "power_watts_peak": 31.2, "energy_joules_est": 3.57, "sample_count": 2, "duration_seconds": 0.114}, "timestamp": "2026-01-25T19:57:27.051650"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 289.342, "latencies_ms": [289.342], "images_per_second": 3.456, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA banana and a chocolate donut are placed together in a clear plastic bag, with the banana positioned on top of the donut.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 19.0}, "power_stats": {"power_watts_samples": [30.11, 30.11, 30.11], "power_watts_avg": 30.11, "power_watts_peak": 30.11, "energy_joules_est": 8.72, "sample_count": 3, "duration_seconds": 0.29}, "timestamp": "2026-01-25T19:57:27.359571"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 101.219, "latencies_ms": [101.219], "images_per_second": 9.88, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Banana: 0.5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10457.8, "ram_available_mb": 112048.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 19.0}, "power_stats": {"power_watts_samples": [30.11, 30.11], "power_watts_avg": 30.11, "power_watts_peak": 30.11, "energy_joules_est": 3.05, "sample_count": 2, "duration_seconds": 0.101}, "timestamp": "2026-01-25T19:57:27.563571"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 98.167, "latencies_ms": [98.167], "images_per_second": 10.187, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nThe banana is on top of a plastic bag.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.8, "ram_available_mb": 112048.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10456.6, "ram_available_mb": 112049.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 19.0}, "power_stats": {"power_watts_samples": [30.68], "power_watts_avg": 30.68, "power_watts_peak": 30.68, "energy_joules_est": 3.02, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T19:57:27.666886"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 127.368, "latencies_ms": [127.368], "images_per_second": 7.851, "prompt_tokens": 757, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nA banana and a chocolate donut are placed inside a clear plastic bag.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.6, "ram_available_mb": 112049.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10456.8, "ram_available_mb": 112049.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 19.0}, "power_stats": {"power_watts_samples": [30.68, 30.68], "power_watts_avg": 30.68, "power_watts_peak": 30.68, "energy_joules_est": 3.91, "sample_count": 2, "duration_seconds": 0.128}, "timestamp": "2026-01-25T19:57:27.874301"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 119.518, "latencies_ms": [119.518], "images_per_second": 8.367, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n The banana is yellow and the donut has chocolate frosting.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10456.8, "ram_available_mb": 112049.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.68, 30.68], "power_watts_avg": 30.68, "power_watts_peak": 30.68, "energy_joules_est": 3.68, "sample_count": 2, "duration_seconds": 0.12}, "timestamp": "2026-01-25T19:57:28.080588"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 292.471, "latencies_ms": [292.471], "images_per_second": 3.419, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nOn a light blue surface, there is a white mug with a skull and crossbones design on it, accompanied by a black knife.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [29.82, 29.82, 29.82], "power_watts_avg": 29.82, "power_watts_peak": 29.82, "energy_joules_est": 8.74, "sample_count": 3, "duration_seconds": 0.293}, "timestamp": "2026-01-25T19:57:28.388067"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 93.25, "latencies_ms": [93.25], "images_per_second": 10.724, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Mug: 0.44", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10457.7, "ram_available_mb": 112048.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [29.82], "power_watts_avg": 29.82, "power_watts_peak": 29.82, "energy_joules_est": 2.79, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T19:57:28.491520"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 134.211, "latencies_ms": [134.211], "images_per_second": 7.451, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nA mug with a skull and crossbones design on it sits next to a knife.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.7, "ram_available_mb": 112048.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10457.7, "ram_available_mb": 112048.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [29.99, 29.99], "power_watts_avg": 29.99, "power_watts_peak": 29.99, "energy_joules_est": 4.03, "sample_count": 2, "duration_seconds": 0.134}, "timestamp": "2026-01-25T19:57:28.698741"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 161.124, "latencies_ms": [161.124], "images_per_second": 6.206, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA white mug with a skull and crossbones design sits on a table, accompanied by a black knife.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.7, "ram_available_mb": 112048.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10457.7, "ram_available_mb": 112048.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [29.99, 29.99], "power_watts_avg": 29.99, "power_watts_peak": 29.99, "energy_joules_est": 4.84, "sample_count": 2, "duration_seconds": 0.161}, "timestamp": "2026-01-25T19:57:28.903800"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 114.051, "latencies_ms": [114.051], "images_per_second": 8.768, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe mug has a skull and crossbones design on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.7, "ram_available_mb": 112048.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10459.1, "ram_available_mb": 112047.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [29.99, 31.45], "power_watts_avg": 30.72, "power_watts_peak": 31.45, "energy_joules_est": 3.51, "sample_count": 2, "duration_seconds": 0.114}, "timestamp": "2026-01-25T19:57:29.109065"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 246.879, "latencies_ms": [246.879], "images_per_second": 4.051, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA group of people are gathered around a bar, with some standing and others sitting at it.", "error": null, "sys_before": {"cpu_percent": 3.2, "ram_used_mb": 10459.1, "ram_available_mb": 112047.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10459.3, "ram_available_mb": 112047.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [31.45, 31.45, 31.45], "power_watts_avg": 31.45, "power_watts_peak": 31.45, "energy_joules_est": 7.8, "sample_count": 3, "duration_seconds": 0.248}, "timestamp": "2026-01-25T19:57:29.426882"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 259.578, "latencies_ms": [259.578], "images_per_second": 3.852, "prompt_tokens": 759, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\n 1. Doorway  2. Wine glass  3. Wine glass  4. Wine glass  5. Wine glass  6. Wine glass  7. Wine glass  8. Wine glass", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.3, "ram_available_mb": 112047.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10454.9, "ram_available_mb": 112051.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [31.45, 29.95, 29.95], "power_watts_avg": 30.45, "power_watts_peak": 31.45, "energy_joules_est": 7.92, "sample_count": 3, "duration_seconds": 0.26}, "timestamp": "2026-01-25T19:57:29.733881"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 169.134, "latencies_ms": [169.134], "images_per_second": 5.912, "prompt_tokens": 763, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\n 1. Man in white shirt and black pants standing next to a woman in black top and plaid shorts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.9, "ram_available_mb": 112051.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10455.6, "ram_available_mb": 112050.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [29.95, 29.95], "power_watts_avg": 29.95, "power_watts_peak": 29.95, "energy_joules_est": 5.07, "sample_count": 2, "duration_seconds": 0.169}, "timestamp": "2026-01-25T19:57:29.939809"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 266.111, "latencies_ms": [266.111], "images_per_second": 3.758, "prompt_tokens": 757, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\nA group of people are gathered around a bar, with some standing in front of it. The bar has several bottles on its shelves, indicating that they might be enjoying drinks or celebrating an occasion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.6, "ram_available_mb": 112050.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10451.7, "ram_available_mb": 112054.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [29.95, 31.71, 31.71], "power_watts_avg": 31.12, "power_watts_peak": 31.71, "energy_joules_est": 8.29, "sample_count": 3, "duration_seconds": 0.266}, "timestamp": "2026-01-25T19:57:30.245348"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 392.139, "latencies_ms": [392.139], "images_per_second": 2.55, "prompt_tokens": 756, "response_tokens_est": 67, "n_tiles": 1, "output_text": "\n The image shows a group of people standing around a bar. There are at least six people in the scene, with some wearing ties and others not. A man is leaning on the counter while another person stands next to him. The bar has several bottles displayed behind it, indicating that they might be enjoying drinks or wine together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.7, "ram_available_mb": 112054.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10444.2, "ram_available_mb": 112062.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [31.71, 31.71, 31.71, 33.67], "power_watts_avg": 32.2, "power_watts_peak": 33.67, "energy_joules_est": 12.64, "sample_count": 4, "duration_seconds": 0.392}, "timestamp": "2026-01-25T19:57:30.650334"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 245.35, "latencies_ms": [245.35], "images_per_second": 4.076, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "urns of water are visible in the background, with a large ship docked nearby and two birds flying overhead.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10444.0, "ram_available_mb": 112062.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10449.8, "ram_available_mb": 112056.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [33.67, 33.67, 33.67], "power_watts_avg": 33.67, "power_watts_peak": 33.67, "energy_joules_est": 8.27, "sample_count": 3, "duration_seconds": 0.246}, "timestamp": "2026-01-25T19:57:30.959125"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 91.498, "latencies_ms": [91.498], "images_per_second": 10.929, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Boat: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.8, "ram_available_mb": 112056.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10446.8, "ram_available_mb": 112059.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [33.67], "power_watts_avg": 33.67, "power_watts_peak": 33.67, "energy_joules_est": 3.09, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T19:57:31.063723"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 99.801, "latencies_ms": [99.801], "images_per_second": 10.02, "prompt_tokens": 763, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. White bird in sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.8, "ram_available_mb": 112059.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 10446.8, "ram_available_mb": 112059.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [37.22], "power_watts_avg": 37.22, "power_watts_peak": 37.22, "energy_joules_est": 3.72, "sample_count": 1, "duration_seconds": 0.1}, "timestamp": "2026-01-25T19:57:31.170061"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 169.404, "latencies_ms": [169.404], "images_per_second": 5.903, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nTwo white birds are flying over a field of tall grass, with several boats moored in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.8, "ram_available_mb": 112059.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10450.3, "ram_available_mb": 112056.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [37.22, 37.22], "power_watts_avg": 37.22, "power_watts_peak": 37.22, "energy_joules_est": 6.31, "sample_count": 2, "duration_seconds": 0.17}, "timestamp": "2026-01-25T19:57:31.374884"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 128.332, "latencies_ms": [128.332], "images_per_second": 7.792, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A large white boat with blue accents is docked at a pier.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.3, "ram_available_mb": 112056.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10452.9, "ram_available_mb": 112053.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [37.22, 37.22], "power_watts_avg": 37.22, "power_watts_peak": 37.22, "energy_joules_est": 4.8, "sample_count": 2, "duration_seconds": 0.129}, "timestamp": "2026-01-25T19:57:31.581256"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 250.81, "latencies_ms": [250.81], "images_per_second": 3.987, "prompt_tokens": 744, "response_tokens_est": 20, "n_tiles": 1, "output_text": "urn of water on a toilet seat, with a man in black clothing and gloves cleaning it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.9, "ram_available_mb": 112053.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [34.07, 34.07, 34.07], "power_watts_avg": 34.07, "power_watts_peak": 34.07, "energy_joules_est": 8.55, "sample_count": 3, "duration_seconds": 0.251}, "timestamp": "2026-01-25T19:57:31.893521"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 272.312, "latencies_ms": [272.312], "images_per_second": 3.672, "prompt_tokens": 759, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\n 1. Toilet bowl cleaner  2. Sink  3. Clock  4. Bathroom wall tile  5. Wall tile  6. Wall tile  7. Wall tile  8. Wall tile", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10449.5, "ram_available_mb": 112056.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [34.07, 30.85, 30.85], "power_watts_avg": 31.92, "power_watts_peak": 34.07, "energy_joules_est": 8.7, "sample_count": 3, "duration_seconds": 0.273}, "timestamp": "2026-01-25T19:57:32.198937"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 129.854, "latencies_ms": [129.854], "images_per_second": 7.701, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n 1. Man is kneeling down in front of a toilet bowl.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.5, "ram_available_mb": 112056.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10450.4, "ram_available_mb": 112055.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [30.85, 30.85], "power_watts_avg": 30.85, "power_watts_peak": 30.85, "energy_joules_est": 4.02, "sample_count": 2, "duration_seconds": 0.13}, "timestamp": "2026-01-25T19:57:32.403862"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 184.079, "latencies_ms": [184.079], "images_per_second": 5.432, "prompt_tokens": 757, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA man in a black shirt and jeans is kneeling down next to a white toilet, cleaning it with gloves on.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.4, "ram_available_mb": 112055.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10453.3, "ram_available_mb": 112053.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [30.85, 31.19], "power_watts_avg": 31.02, "power_watts_peak": 31.19, "energy_joules_est": 5.73, "sample_count": 2, "duration_seconds": 0.185}, "timestamp": "2026-01-25T19:57:32.609384"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 174.944, "latencies_ms": [174.944], "images_per_second": 5.716, "prompt_tokens": 756, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\n The image is a black and white photo of a man cleaning the toilet. He is wearing gloves while working on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.3, "ram_available_mb": 112053.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [31.19, 31.19], "power_watts_avg": 31.19, "power_watts_peak": 31.19, "energy_joules_est": 5.46, "sample_count": 2, "duration_seconds": 0.175}, "timestamp": "2026-01-25T19:57:32.814369"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 280.997, "latencies_ms": [280.997], "images_per_second": 3.559, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA group of people are standing on a snowy mountain, with snow-covered rocks in the background and a clear blue sky above them.", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10458.9, "ram_available_mb": 112047.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 45.0}, "power_stats": {"power_watts_samples": [31.19, 31.19, 34.2], "power_watts_avg": 32.19, "power_watts_peak": 34.2, "energy_joules_est": 9.06, "sample_count": 3, "duration_seconds": 0.282}, "timestamp": "2026-01-25T19:57:33.124342"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 152.715, "latencies_ms": [152.715], "images_per_second": 6.548, "prompt_tokens": 759, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n 1: 0.12, 0.13, 0.2, 0.22", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.9, "ram_available_mb": 112047.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10456.8, "ram_available_mb": 112049.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 45.0}, "power_stats": {"power_watts_samples": [34.2, 34.2], "power_watts_avg": 34.2, "power_watts_peak": 34.2, "energy_joules_est": 5.23, "sample_count": 2, "duration_seconds": 0.153}, "timestamp": "2026-01-25T19:57:33.329377"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 136.257, "latencies_ms": [136.257], "images_per_second": 7.339, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n 1. Person standing in front of mountain with snow on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.8, "ram_available_mb": 112049.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 45.0}, "power_stats": {"power_watts_samples": [34.2, 34.2], "power_watts_avg": 34.2, "power_watts_peak": 34.2, "energy_joules_est": 4.68, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T19:57:33.534239"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 154.764, "latencies_ms": [154.764], "images_per_second": 6.461, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA group of people are standing on a snowy mountain, with snow-covered mountains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10453.7, "ram_available_mb": 112052.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 45.0}, "power_stats": {"power_watts_samples": [34.46, 34.46], "power_watts_avg": 34.46, "power_watts_peak": 34.46, "energy_joules_est": 5.35, "sample_count": 2, "duration_seconds": 0.155}, "timestamp": "2026-01-25T19:57:33.739347"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 457.357, "latencies_ms": [457.357], "images_per_second": 2.186, "prompt_tokens": 756, "response_tokens_est": 79, "n_tiles": 1, "output_text": "\n The image shows a group of people standing on top of a snow-covered mountain. They are wearing black clothing and appear to be enjoying the beautiful scenery around them. The sky is clear blue with no clouds, providing an ideal backdrop for their adventure. The snow on the mountain slopes has a light brown color, indicating that it's freshly fallen or well-traveled by skiers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.7, "ram_available_mb": 112052.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10452.8, "ram_available_mb": 112053.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.46, 34.46, 34.46, 30.21, 30.21], "power_watts_avg": 32.76, "power_watts_peak": 34.46, "energy_joules_est": 15.0, "sample_count": 5, "duration_seconds": 0.458}, "timestamp": "2026-01-25T19:57:34.245340"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 283.662, "latencies_ms": [283.662], "images_per_second": 3.525, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA white bowl filled with rice and a colorful dish of food is placed on a wooden table, accompanied by two pieces of broccoli.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.8, "ram_available_mb": 112053.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.21, 30.21, 30.21], "power_watts_avg": 30.21, "power_watts_peak": 30.21, "energy_joules_est": 8.59, "sample_count": 3, "duration_seconds": 0.284}, "timestamp": "2026-01-25T19:57:34.554789"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 91.999, "latencies_ms": [91.999], "images_per_second": 10.87, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Broccoli and rice", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10451.8, "ram_available_mb": 112054.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.0], "power_watts_avg": 36.0, "power_watts_peak": 36.0, "energy_joules_est": 3.32, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T19:57:34.660208"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 108.61, "latencies_ms": [108.61], "images_per_second": 9.207, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nThe bowl of food is on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.8, "ram_available_mb": 112054.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10453.5, "ram_available_mb": 112052.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.0, 36.0], "power_watts_avg": 36.0, "power_watts_peak": 36.0, "energy_joules_est": 3.92, "sample_count": 2, "duration_seconds": 0.109}, "timestamp": "2026-01-25T19:57:34.867001"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 123.756, "latencies_ms": [123.756], "images_per_second": 8.08, "prompt_tokens": 757, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nA white bowl filled with food, including rice and broccoli.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.5, "ram_available_mb": 112052.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [36.0, 36.0], "power_watts_avg": 36.0, "power_watts_peak": 36.0, "energy_joules_est": 4.46, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T19:57:35.071343"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 187.181, "latencies_ms": [187.181], "images_per_second": 5.342, "prompt_tokens": 756, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\n The image shows a white bowl filled with food on top of a wooden table. In the bowl are pieces of broccoli and rice.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10451.4, "ram_available_mb": 112054.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10453.4, "ram_available_mb": 112052.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [33.49, 33.49], "power_watts_avg": 33.49, "power_watts_peak": 33.49, "energy_joules_est": 6.29, "sample_count": 2, "duration_seconds": 0.188}, "timestamp": "2026-01-25T19:57:35.276428"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 286.73, "latencies_ms": [286.73], "images_per_second": 3.488, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA person is performing a trick on their skateboard, with both feet firmly planted on it and the board tilted to the side.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10453.4, "ram_available_mb": 112052.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10450.2, "ram_available_mb": 112056.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [33.49, 33.49, 33.49], "power_watts_avg": 33.49, "power_watts_peak": 33.49, "energy_joules_est": 9.63, "sample_count": 3, "duration_seconds": 0.288}, "timestamp": "2026-01-25T19:57:35.585750"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 111.333, "latencies_ms": [111.333], "images_per_second": 8.982, "prompt_tokens": 759, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Skateboarder's left foot - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.2, "ram_available_mb": 112056.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.77, 30.77], "power_watts_avg": 30.77, "power_watts_peak": 30.77, "energy_joules_est": 3.44, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T19:57:35.790771"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 125.859, "latencies_ms": [125.859], "images_per_second": 7.945, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A person is skateboarding on a wooden ramp in an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10452.8, "ram_available_mb": 112053.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [30.77, 30.77], "power_watts_avg": 30.77, "power_watts_peak": 30.77, "energy_joules_est": 3.88, "sample_count": 2, "duration_seconds": 0.126}, "timestamp": "2026-01-25T19:57:35.996302"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 177.132, "latencies_ms": [177.132], "images_per_second": 5.646, "prompt_tokens": 757, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nIn a grassy field, a person wearing black pants and white shoes is performing a skateboard trick on a wooden ramp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [31.12, 31.12], "power_watts_avg": 31.12, "power_watts_peak": 31.12, "energy_joules_est": 5.53, "sample_count": 2, "duration_seconds": 0.178}, "timestamp": "2026-01-25T19:57:36.202080"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 109.908, "latencies_ms": [109.908], "images_per_second": 9.098, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The skateboarder is wearing black and white shoes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10453.1, "ram_available_mb": 112053.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [31.12, 31.12], "power_watts_avg": 31.12, "power_watts_peak": 31.12, "energy_joules_est": 3.43, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T19:57:36.407832"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 475.101, "latencies_ms": [475.101], "images_per_second": 2.105, "prompt_tokens": 744, "response_tokens_est": 65, "n_tiles": 1, "output_text": "\nA bunch of bananas is sitting on a wooden desk, with three visible in different stages of ripeness. The largest banana is at the bottom and slightly to the left, while two smaller ones are positioned above it. A keyboard can be seen in the background, suggesting that this scene takes place in an office setting.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10453.1, "ram_available_mb": 112053.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10453.1, "ram_available_mb": 112053.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [31.12, 27.2, 27.2, 27.2, 27.2], "power_watts_avg": 27.99, "power_watts_peak": 31.12, "energy_joules_est": 13.31, "sample_count": 5, "duration_seconds": 0.475}, "timestamp": "2026-01-25T19:57:36.921158"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 75.427, "latencies_ms": [75.427], "images_per_second": 13.258, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Banana", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.1, "ram_available_mb": 112053.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10450.1, "ram_available_mb": 112056.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_watts_samples": [27.2], "power_watts_avg": 27.2, "power_watts_peak": 27.2, "energy_joules_est": 2.06, "sample_count": 1, "duration_seconds": 0.076}, "timestamp": "2026-01-25T19:57:37.026317"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 109.691, "latencies_ms": [109.691], "images_per_second": 9.116, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. A bunch of bananas on a desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.1, "ram_available_mb": 112056.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10450.4, "ram_available_mb": 112055.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_watts_samples": [34.34, 34.34], "power_watts_avg": 34.34, "power_watts_peak": 34.34, "energy_joules_est": 3.78, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T19:57:37.233972"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 379.189, "latencies_ms": [379.189], "images_per_second": 2.637, "prompt_tokens": 757, "response_tokens_est": 65, "n_tiles": 1, "output_text": "\nIn a room, there are three bunches of bananas on a desk. The bananas appear to be ripe or overripe, with brown spots visible on their skins. A keyboard can also be seen in the background, suggesting that this scene takes place in an office setting where someone might be working at the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.4, "ram_available_mb": 112055.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_watts_samples": [34.34, 34.34, 34.34, 34.81], "power_watts_avg": 34.45, "power_watts_peak": 34.81, "energy_joules_est": 13.07, "sample_count": 4, "duration_seconds": 0.379}, "timestamp": "2026-01-25T19:57:37.641575"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 306.851, "latencies_ms": [306.851], "images_per_second": 3.259, "prompt_tokens": 756, "response_tokens_est": 50, "n_tiles": 1, "output_text": "\n The image shows a bunch of bananas sitting on top of a wooden desk. The bananas are yellow and appear to be ripe or overripe. There is also a cup placed near the bananas, possibly containing a beverage like coffee or tea.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10445.0, "ram_available_mb": 112061.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.81, 34.81, 34.81, 34.81], "power_watts_avg": 34.81, "power_watts_peak": 34.81, "energy_joules_est": 10.71, "sample_count": 4, "duration_seconds": 0.308}, "timestamp": "2026-01-25T19:57:38.048024"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 271.053, "latencies_ms": [271.053], "images_per_second": 3.689, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "iced tea is poured into a glass on a wooden table, accompanied by a plate of food with rice and vegetables.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10445.0, "ram_available_mb": 112061.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10445.5, "ram_available_mb": 112060.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.98, 31.98, 31.98], "power_watts_avg": 31.98, "power_watts_peak": 31.98, "energy_joules_est": 8.69, "sample_count": 3, "duration_seconds": 0.272}, "timestamp": "2026-01-25T19:57:38.358061"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 72.819, "latencies_ms": [72.819], "images_per_second": 13.733, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Rice and chicken", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.5, "ram_available_mb": 112060.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.98], "power_watts_avg": 31.98, "power_watts_peak": 31.98, "energy_joules_est": 2.34, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T19:57:38.462799"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 117.638, "latencies_ms": [117.638], "images_per_second": 8.501, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n A plate of food with rice and vegetables on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10448.5, "ram_available_mb": 112057.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.98, 34.13], "power_watts_avg": 33.06, "power_watts_peak": 34.13, "energy_joules_est": 3.9, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T19:57:38.671929"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 212.631, "latencies_ms": [212.631], "images_per_second": 4.703, "prompt_tokens": 757, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA plate of food with rice, chicken, broccoli, and carrots sits on a wooden table. A glass of water is also present on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.5, "ram_available_mb": 112057.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10448.8, "ram_available_mb": 112057.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.13, 34.13, 34.13], "power_watts_avg": 34.13, "power_watts_peak": 34.13, "energy_joules_est": 7.27, "sample_count": 3, "duration_seconds": 0.213}, "timestamp": "2026-01-25T19:57:38.979491"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 76.021, "latencies_ms": [76.021], "images_per_second": 13.154, "prompt_tokens": 756, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urn of water on table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.8, "ram_available_mb": 112057.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10439.8, "ram_available_mb": 112066.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [34.13], "power_watts_avg": 34.13, "power_watts_peak": 34.13, "energy_joules_est": 2.62, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T19:57:39.085189"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 276.769, "latencies_ms": [276.769], "images_per_second": 3.613, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "urns of flowers on a table, with a woman in a white dress standing behind it and another woman holding a bouquet.\n", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10439.8, "ram_available_mb": 112066.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10440.7, "ram_available_mb": 112065.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [29.23, 29.23, 29.23], "power_watts_avg": 29.23, "power_watts_peak": 29.23, "energy_joules_est": 8.1, "sample_count": 3, "duration_seconds": 0.277}, "timestamp": "2026-01-25T19:57:39.398745"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.72, "latencies_ms": [76.72], "images_per_second": 13.034, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Couch - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10440.7, "ram_available_mb": 112065.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10442.4, "ram_available_mb": 112063.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [29.23], "power_watts_avg": 29.23, "power_watts_peak": 29.23, "energy_joules_est": 2.25, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T19:57:39.504530"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 117.863, "latencies_ms": [117.863], "images_per_second": 8.484, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A girl is playing a video game in front of a couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10442.4, "ram_available_mb": 112063.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10444.6, "ram_available_mb": 112061.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [31.25, 31.25], "power_watts_avg": 31.25, "power_watts_peak": 31.25, "energy_joules_est": 3.7, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T19:57:39.713438"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 221.319, "latencies_ms": [221.319], "images_per_second": 4.518, "prompt_tokens": 757, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA young girl in a white dress stands on a rug, playing a video game with two other people. The room has a couch and a dining table nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10444.6, "ram_available_mb": 112061.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10442.4, "ram_available_mb": 112063.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.25, 31.25, 31.25], "power_watts_avg": 31.25, "power_watts_peak": 31.25, "energy_joules_est": 6.93, "sample_count": 3, "duration_seconds": 0.222}, "timestamp": "2026-01-25T19:57:40.019822"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 540.999, "latencies_ms": [540.999], "images_per_second": 1.848, "prompt_tokens": 756, "response_tokens_est": 98, "n_tiles": 1, "output_text": "\n The image shows a group of people gathered in a living room. A girl is playing a video game on the Nintendo Wii while others watch and enjoy her performance. There are two couches in the room - one near the center and another towards the right side. The room has wooden floors, which complement the overall decor. In addition to the main subjects, there are several other people present in the scene, some of whom might be watching or waiting for their turn to play.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10442.4, "ram_available_mb": 112063.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10444.6, "ram_available_mb": 112061.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.48, 31.48, 31.48, 31.48, 31.48, 32.42], "power_watts_avg": 31.64, "power_watts_peak": 32.42, "energy_joules_est": 17.13, "sample_count": 6, "duration_seconds": 0.541}, "timestamp": "2026-01-25T19:57:40.626950"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 260.385, "latencies_ms": [260.385], "images_per_second": 3.84, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "Two men in suits stand facing each other, shaking hands over a table with plates of food and glasses on it.", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 10444.6, "ram_available_mb": 112061.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10444.9, "ram_available_mb": 112061.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.42, 32.42, 32.42], "power_watts_avg": 32.42, "power_watts_peak": 32.42, "energy_joules_est": 8.45, "sample_count": 3, "duration_seconds": 0.261}, "timestamp": "2026-01-25T19:57:40.937403"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 337.967, "latencies_ms": [337.967], "images_per_second": 2.959, "prompt_tokens": 759, "response_tokens_est": 59, "n_tiles": 1, "output_text": "\n 1. Man in suit and tie 2. Man in gray shirt 3. Man in black shirt 4. Man in white shirt 5. Man in blue shirt 6. Man in black shirt 7. Man in gray shirt 8. Man in black shirt 9. Man in gray shirt 10.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10444.9, "ram_available_mb": 112061.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [32.42, 36.78, 36.78, 36.78], "power_watts_avg": 35.69, "power_watts_peak": 36.78, "energy_joules_est": 12.08, "sample_count": 4, "duration_seconds": 0.338}, "timestamp": "2026-01-25T19:57:41.343551"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 93.887, "latencies_ms": [93.887], "images_per_second": 10.651, "prompt_tokens": 763, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urns on tables in background", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10446.6, "ram_available_mb": 112059.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [36.78], "power_watts_avg": 36.78, "power_watts_peak": 36.78, "energy_joules_est": 3.46, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T19:57:41.448262"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 158.395, "latencies_ms": [158.395], "images_per_second": 6.313, "prompt_tokens": 757, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nTwo men in suits are shaking hands, surrounded by other people at tables with plates of food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.6, "ram_available_mb": 112059.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10446.6, "ram_available_mb": 112059.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [36.78, 33.99], "power_watts_avg": 35.39, "power_watts_peak": 36.78, "energy_joules_est": 5.62, "sample_count": 2, "duration_seconds": 0.159}, "timestamp": "2026-01-25T19:57:41.656323"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 95.466, "latencies_ms": [95.466], "images_per_second": 10.475, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urns of light yellow and dark brown.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.6, "ram_available_mb": 112059.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10446.1, "ram_available_mb": 112060.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [33.99], "power_watts_avg": 33.99, "power_watts_peak": 33.99, "energy_joules_est": 3.25, "sample_count": 1, "duration_seconds": 0.096}, "timestamp": "2026-01-25T19:57:41.760227"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 247.812, "latencies_ms": [247.812], "images_per_second": 4.035, "prompt_tokens": 744, "response_tokens_est": 19, "n_tiles": 1, "output_text": "urn of water on a table, with a man standing behind it and looking at him.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 10446.1, "ram_available_mb": 112060.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10450.5, "ram_available_mb": 112055.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [33.99, 33.99, 33.99], "power_watts_avg": 33.99, "power_watts_peak": 33.99, "energy_joules_est": 8.43, "sample_count": 3, "duration_seconds": 0.248}, "timestamp": "2026-01-25T19:57:42.070530"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 209.01, "latencies_ms": [209.01], "images_per_second": 4.784, "prompt_tokens": 759, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\n 1. Shirt  2. Tie 3. Hand 4. Finger 5. Ring 6. Ear 7. Nose 8. Mouth 9. Neck 10. Head", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.5, "ram_available_mb": 112055.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10444.1, "ram_available_mb": 112062.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [34.62, 34.62, 34.62], "power_watts_avg": 34.62, "power_watts_peak": 34.62, "energy_joules_est": 7.26, "sample_count": 3, "duration_seconds": 0.21}, "timestamp": "2026-01-25T19:57:42.376249"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 115.937, "latencies_ms": [115.937], "images_per_second": 8.625, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe man is holding a cigarette in his right hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10444.3, "ram_available_mb": 112062.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10443.6, "ram_available_mb": 112062.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [34.62, 34.62], "power_watts_avg": 34.62, "power_watts_peak": 34.62, "energy_joules_est": 4.03, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T19:57:42.582549"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 96.486, "latencies_ms": [96.486], "images_per_second": 10.364, "prompt_tokens": 757, "response_tokens_est": 9, "n_tiles": 1, "output_text": "urn of water on a table.\n", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10443.6, "ram_available_mb": 112062.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10444.6, "ram_available_mb": 112061.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [32.6], "power_watts_avg": 32.6, "power_watts_peak": 32.6, "energy_joules_est": 3.16, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T19:57:42.686813"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 116.99, "latencies_ms": [116.99], "images_per_second": 8.548, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The man is wearing a white shirt and a black tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10444.6, "ram_available_mb": 112061.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10445.8, "ram_available_mb": 112060.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [32.6, 32.6], "power_watts_avg": 32.6, "power_watts_peak": 32.6, "energy_joules_est": 3.82, "sample_count": 2, "duration_seconds": 0.117}, "timestamp": "2026-01-25T19:57:42.892613"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 278.738, "latencies_ms": [278.738], "images_per_second": 3.588, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "urns of a living room with a television on top and two couches, one blue plaid couch and one pink chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.8, "ram_available_mb": 112060.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10446.3, "ram_available_mb": 112060.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [32.6, 26.43, 26.43], "power_watts_avg": 28.49, "power_watts_peak": 32.6, "energy_joules_est": 7.97, "sample_count": 3, "duration_seconds": 0.28}, "timestamp": "2026-01-25T19:57:43.200942"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 83.648, "latencies_ms": [83.648], "images_per_second": 11.955, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Chair 1: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.3, "ram_available_mb": 112060.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10446.3, "ram_available_mb": 112060.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [26.43], "power_watts_avg": 26.43, "power_watts_peak": 26.43, "energy_joules_est": 2.23, "sample_count": 1, "duration_seconds": 0.084}, "timestamp": "2026-01-25T19:57:43.306119"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 149.907, "latencies_ms": [149.907], "images_per_second": 6.671, "prompt_tokens": 763, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n A blue and red plaid couch sits in a living room with a chair to its left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.3, "ram_available_mb": 112060.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10446.3, "ram_available_mb": 112060.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [26.43, 26.43], "power_watts_avg": 26.43, "power_watts_peak": 26.43, "energy_joules_est": 3.98, "sample_count": 2, "duration_seconds": 0.15}, "timestamp": "2026-01-25T19:57:43.515172"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 132.443, "latencies_ms": [132.443], "images_per_second": 7.55, "prompt_tokens": 757, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nA living room with a blue plaid couch, a chair, and a television.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.3, "ram_available_mb": 112060.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10449.4, "ram_available_mb": 112056.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.18, 30.18], "power_watts_avg": 30.18, "power_watts_peak": 30.18, "energy_joules_est": 4.01, "sample_count": 2, "duration_seconds": 0.133}, "timestamp": "2026-01-25T19:57:43.721072"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 300.854, "latencies_ms": [300.854], "images_per_second": 3.324, "prompt_tokens": 756, "response_tokens_est": 52, "n_tiles": 1, "output_text": "\n The room is painted yellow and has a whiteboard on the wall. There are two couches in the room - one blue plaid couch and another red chair with a plaid pattern. A television set is placed between these two pieces of furniture.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.4, "ram_available_mb": 112056.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10452.2, "ram_available_mb": 112054.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.18, 30.18, 30.18, 33.97], "power_watts_avg": 31.12, "power_watts_peak": 33.97, "energy_joules_est": 9.38, "sample_count": 4, "duration_seconds": 0.301}, "timestamp": "2026-01-25T19:57:44.126190"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 285.04, "latencies_ms": [285.04], "images_per_second": 3.508, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA surfer in a yellow shirt and black shorts rides a wave on a white surfboard, skillfully navigating the water's surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.2, "ram_available_mb": 112054.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10448.3, "ram_available_mb": 112058.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.97, 33.97, 33.97], "power_watts_avg": 33.97, "power_watts_peak": 33.97, "energy_joules_est": 9.7, "sample_count": 3, "duration_seconds": 0.286}, "timestamp": "2026-01-25T19:57:44.435165"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 67.347, "latencies_ms": [67.347], "images_per_second": 14.849, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Water", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10448.3, "ram_available_mb": 112058.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.97], "power_watts_avg": 33.97, "power_watts_peak": 33.97, "energy_joules_est": 2.3, "sample_count": 1, "duration_seconds": 0.068}, "timestamp": "2026-01-25T19:57:44.540598"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 157.381, "latencies_ms": [157.381], "images_per_second": 6.354, "prompt_tokens": 763, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n 1. Surfer in yellow shirt and black shorts riding a wave on top of a surfboard.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10449.3, "ram_available_mb": 112057.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.54, 33.54], "power_watts_avg": 33.54, "power_watts_peak": 33.54, "energy_joules_est": 5.29, "sample_count": 2, "duration_seconds": 0.158}, "timestamp": "2026-01-25T19:57:44.749730"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 50.271, "latencies_ms": [50.271], "images_per_second": 19.892, "prompt_tokens": 757, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.3, "ram_available_mb": 112057.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 10449.3, "ram_available_mb": 112057.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.54], "power_watts_avg": 33.54, "power_watts_peak": 33.54, "energy_joules_est": 1.71, "sample_count": 1, "duration_seconds": 0.051}, "timestamp": "2026-01-25T19:57:44.855437"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 142.777, "latencies_ms": [142.777], "images_per_second": 7.004, "prompt_tokens": 756, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n The image shows a surfer riding a wave on top of his surfboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.3, "ram_available_mb": 112057.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10448.1, "ram_available_mb": 112058.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [33.54, 33.54], "power_watts_avg": 33.54, "power_watts_peak": 33.54, "energy_joules_est": 4.81, "sample_count": 2, "duration_seconds": 0.143}, "timestamp": "2026-01-25T19:57:45.064238"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 276.773, "latencies_ms": [276.773], "images_per_second": 3.613, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA black cat is sitting in front of a computer monitor, with its head resting on the screen and ears perked up.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10448.1, "ram_available_mb": 112058.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10449.8, "ram_available_mb": 112056.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [30.18, 30.18, 30.18], "power_watts_avg": 30.18, "power_watts_peak": 30.18, "energy_joules_est": 8.38, "sample_count": 3, "duration_seconds": 0.278}, "timestamp": "2026-01-25T19:57:45.377883"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 119.362, "latencies_ms": [119.362], "images_per_second": 8.378, "prompt_tokens": 759, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n Mouse pad, keyboard, mouse, phone, monitor, cat", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.8, "ram_available_mb": 112056.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10452.0, "ram_available_mb": 112054.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [30.18, 30.18], "power_watts_avg": 30.18, "power_watts_peak": 30.18, "energy_joules_est": 3.63, "sample_count": 2, "duration_seconds": 0.12}, "timestamp": "2026-01-25T19:57:45.584067"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 122.172, "latencies_ms": [122.172], "images_per_second": 8.185, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. A cat looking at a computer screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.0, "ram_available_mb": 112054.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10446.6, "ram_available_mb": 112059.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [30.81, 30.81], "power_watts_avg": 30.81, "power_watts_peak": 30.81, "energy_joules_est": 3.77, "sample_count": 2, "duration_seconds": 0.123}, "timestamp": "2026-01-25T19:57:45.790509"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 292.689, "latencies_ms": [292.689], "images_per_second": 3.417, "prompt_tokens": 757, "response_tokens_est": 46, "n_tiles": 1, "output_text": "\nA black cat sits on a desk, looking at a computer screen. The cat's head is resting on the keyboard of an open laptop. A phone can be seen in the background to the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.6, "ram_available_mb": 112059.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10449.2, "ram_available_mb": 112057.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [30.81, 30.81, 30.64], "power_watts_avg": 30.75, "power_watts_peak": 30.81, "energy_joules_est": 9.02, "sample_count": 3, "duration_seconds": 0.293}, "timestamp": "2026-01-25T19:57:46.096241"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 136.705, "latencies_ms": [136.705], "images_per_second": 7.315, "prompt_tokens": 756, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n The cat is sitting on a desk with an orange computer monitor and keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.1, "ram_available_mb": 112057.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10450.3, "ram_available_mb": 112056.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [30.64, 30.64], "power_watts_avg": 30.64, "power_watts_peak": 30.64, "energy_joules_est": 4.19, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T19:57:46.300863"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 266.006, "latencies_ms": [266.006], "images_per_second": 3.759, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA group of people gather around a child holding a blue balloon, with two men cutting it open using scissors.", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 10450.3, "ram_available_mb": 112056.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10453.8, "ram_available_mb": 112052.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [30.64, 30.64, 30.88], "power_watts_avg": 30.72, "power_watts_peak": 30.88, "energy_joules_est": 8.18, "sample_count": 3, "duration_seconds": 0.266}, "timestamp": "2026-01-25T19:57:46.611281"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 206.172, "latencies_ms": [206.172], "images_per_second": 4.85, "prompt_tokens": 759, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\n 1. Balloon 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.8, "ram_available_mb": 112052.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10448.5, "ram_available_mb": 112057.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [30.88, 30.88, 30.88], "power_watts_avg": 30.88, "power_watts_peak": 30.88, "energy_joules_est": 6.37, "sample_count": 3, "duration_seconds": 0.206}, "timestamp": "2026-01-25T19:57:46.916568"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 118.926, "latencies_ms": [118.926], "images_per_second": 8.409, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n Child in pink helmet and adult holding blue balloon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.2, "ram_available_mb": 112057.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10450.7, "ram_available_mb": 112055.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [30.88, 32.01], "power_watts_avg": 31.44, "power_watts_peak": 32.01, "energy_joules_est": 3.75, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T19:57:47.121288"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 188.57, "latencies_ms": [188.57], "images_per_second": 5.303, "prompt_tokens": 757, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA group of people, including a man in a suit and a police officer, are cutting a ribbon with a child wearing a pink helmet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.7, "ram_available_mb": 112055.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [32.01, 32.01], "power_watts_avg": 32.01, "power_watts_peak": 32.01, "energy_joules_est": 6.04, "sample_count": 2, "duration_seconds": 0.189}, "timestamp": "2026-01-25T19:57:47.326024"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 47.008, "latencies_ms": [47.008], "images_per_second": 21.273, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 10453.2, "ram_available_mb": 112053.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [32.01], "power_watts_avg": 32.01, "power_watts_peak": 32.01, "energy_joules_est": 1.52, "sample_count": 1, "duration_seconds": 0.047}, "timestamp": "2026-01-25T19:57:47.431185"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 408.116, "latencies_ms": [408.116], "images_per_second": 2.45, "prompt_tokens": 744, "response_tokens_est": 50, "n_tiles": 1, "output_text": "\nA white and purple bus is parked at a bus stop, displaying \"First Group\" on its front. The bus has a sign that says \"Free Wi-Fi Board\", indicating it may be a Wi-Fi hotspot for passengers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.2, "ram_available_mb": 112053.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10454.7, "ram_available_mb": 112051.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [32.01, 26.28, 26.28, 26.28, 26.28], "power_watts_avg": 27.43, "power_watts_peak": 32.01, "energy_joules_est": 11.2, "sample_count": 5, "duration_seconds": 0.408}, "timestamp": "2026-01-25T19:57:47.945085"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 96.541, "latencies_ms": [96.541], "images_per_second": 10.358, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Bus: 67545", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.7, "ram_available_mb": 112051.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10454.8, "ram_available_mb": 112051.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [26.28], "power_watts_avg": 26.28, "power_watts_peak": 26.28, "energy_joules_est": 2.54, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T19:57:48.049205"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 93.344, "latencies_ms": [93.344], "images_per_second": 10.713, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Bus on right side of photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.8, "ram_available_mb": 112051.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10455.2, "ram_available_mb": 112051.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [31.75], "power_watts_avg": 31.75, "power_watts_peak": 31.75, "energy_joules_est": 2.97, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T19:57:48.154559"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 344.239, "latencies_ms": [344.239], "images_per_second": 2.905, "prompt_tokens": 757, "response_tokens_est": 58, "n_tiles": 1, "output_text": "\nA white bus with a purple stripe is driving down a street, displaying \"First Group\" on its front. The bus has a sign that says \"Free Wi-Fi Board.\" There are several people walking around in the background, some of them carrying backpacks or handbags.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10455.2, "ram_available_mb": 112051.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10459.6, "ram_available_mb": 112046.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [31.75, 31.75, 31.75, 31.75], "power_watts_avg": 31.75, "power_watts_peak": 31.75, "energy_joules_est": 10.93, "sample_count": 4, "duration_seconds": 0.344}, "timestamp": "2026-01-25T19:57:48.560765"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 96.427, "latencies_ms": [96.427], "images_per_second": 10.37, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The bus is white and purple.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.6, "ram_available_mb": 112046.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10459.8, "ram_available_mb": 112046.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [37.18], "power_watts_avg": 37.18, "power_watts_peak": 37.18, "energy_joules_est": 3.6, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T19:57:48.665827"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 285.512, "latencies_ms": [285.512], "images_per_second": 3.502, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "urns on a shelf in a living room, with a man sitting on the floor and looking at his reflection in a large mirror.\n", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 10459.8, "ram_available_mb": 112046.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [37.18, 37.18, 37.18], "power_watts_avg": 37.18, "power_watts_peak": 37.18, "energy_joules_est": 10.62, "sample_count": 3, "duration_seconds": 0.286}, "timestamp": "2026-01-25T19:57:48.974386"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 71.702, "latencies_ms": [71.702], "images_per_second": 13.947, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n Mirror: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10452.5, "ram_available_mb": 112053.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [37.18], "power_watts_avg": 37.18, "power_watts_peak": 37.18, "energy_joules_est": 2.68, "sample_count": 1, "duration_seconds": 0.072}, "timestamp": "2026-01-25T19:57:49.079831"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 136.921, "latencies_ms": [136.921], "images_per_second": 7.303, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. Man in green shirt sitting on floor and looking at mirror.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10452.5, "ram_available_mb": 112053.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.3, 35.3], "power_watts_avg": 35.3, "power_watts_peak": 35.3, "energy_joules_est": 4.85, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T19:57:49.284694"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 247.014, "latencies_ms": [247.014], "images_per_second": 4.048, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA man sits in front of a large mirror, holding his phone. The room has wooden walls and flooring, with a window on one side that lets in natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.3, 35.3, 35.3], "power_watts_avg": 35.3, "power_watts_peak": 35.3, "energy_joules_est": 8.73, "sample_count": 3, "duration_seconds": 0.247}, "timestamp": "2026-01-25T19:57:49.589765"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 124.442, "latencies_ms": [124.442], "images_per_second": 8.036, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n The man is sitting on the floor in front of a mirror.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10468.4, "ram_available_mb": 112037.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.79, 31.79], "power_watts_avg": 31.79, "power_watts_peak": 31.79, "energy_joules_est": 3.97, "sample_count": 2, "duration_seconds": 0.125}, "timestamp": "2026-01-25T19:57:49.795225"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 269.949, "latencies_ms": [269.949], "images_per_second": 3.704, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA group of young men are gathered in a room, each holding a surfboard and smiling for a photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.4, "ram_available_mb": 112037.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10471.2, "ram_available_mb": 112035.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [31.79, 31.79, 30.38], "power_watts_avg": 31.32, "power_watts_peak": 31.79, "energy_joules_est": 8.47, "sample_count": 3, "duration_seconds": 0.27}, "timestamp": "2026-01-25T19:57:50.107780"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.147, "latencies_ms": [82.147], "images_per_second": 12.173, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Surfboard: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10470.4, "ram_available_mb": 112035.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [30.38], "power_watts_avg": 30.38, "power_watts_peak": 30.38, "energy_joules_est": 2.5, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T19:57:50.213586"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 155.932, "latencies_ms": [155.932], "images_per_second": 6.413, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n 1. Boy holding yellow surfboard with a man in black shirt and hat behind him.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10470.4, "ram_available_mb": 112035.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10466.4, "ram_available_mb": 112039.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [30.38, 30.38], "power_watts_avg": 30.38, "power_watts_peak": 30.38, "energy_joules_est": 4.75, "sample_count": 2, "duration_seconds": 0.156}, "timestamp": "2026-01-25T19:57:50.417540"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 255.739, "latencies_ms": [255.739], "images_per_second": 3.91, "prompt_tokens": 757, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\nA group of young men are gathered in a room, holding surfboards. One man is taking a picture with his cell phone while another man holds up two surfboards for a photo opportunity.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10466.4, "ram_available_mb": 112039.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10463.8, "ram_available_mb": 112042.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [30.38, 30.23, 30.23], "power_watts_avg": 30.28, "power_watts_peak": 30.38, "energy_joules_est": 7.75, "sample_count": 3, "duration_seconds": 0.256}, "timestamp": "2026-01-25T19:57:50.722942"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 305.557, "latencies_ms": [305.557], "images_per_second": 3.273, "prompt_tokens": 756, "response_tokens_est": 48, "n_tiles": 1, "output_text": "\n The image shows a group of five young men posing with their surfboards. One boy is holding up his yellow surfboard while the others are also holding up their boards. They appear to be enjoying themselves and capturing this moment together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.8, "ram_available_mb": 112042.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10463.7, "ram_available_mb": 112042.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [30.23, 30.23, 30.23, 31.55], "power_watts_avg": 30.56, "power_watts_peak": 31.55, "energy_joules_est": 9.35, "sample_count": 4, "duration_seconds": 0.306}, "timestamp": "2026-01-25T19:57:51.129347"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 134.743, "latencies_ms": [134.743], "images_per_second": 7.422, "prompt_tokens": 744, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 10463.7, "ram_available_mb": 112042.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10463.8, "ram_available_mb": 112042.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.55, 31.55], "power_watts_avg": 31.55, "power_watts_peak": 31.55, "energy_joules_est": 4.26, "sample_count": 2, "duration_seconds": 0.135}, "timestamp": "2026-01-25T19:57:51.341243"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 81.759, "latencies_ms": [81.759], "images_per_second": 12.231, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Airplane", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10463.8, "ram_available_mb": 112042.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10460.8, "ram_available_mb": 112045.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.55], "power_watts_avg": 31.55, "power_watts_peak": 31.55, "energy_joules_est": 2.59, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T19:57:51.446588"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 105.038, "latencies_ms": [105.038], "images_per_second": 9.52, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\nThe airplane is parked on a runway.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.8, "ram_available_mb": 112045.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.55, 32.11], "power_watts_avg": 31.83, "power_watts_peak": 32.11, "energy_joules_est": 3.35, "sample_count": 2, "duration_seconds": 0.105}, "timestamp": "2026-01-25T19:57:51.650801"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 265.427, "latencies_ms": [265.427], "images_per_second": 3.768, "prompt_tokens": 757, "response_tokens_est": 42, "n_tiles": 1, "output_text": "\nA large gold airplane with blue accents is parked on a runway, facing towards the right side of the image. The sky above has some clouds scattered across it, creating an interesting backdrop for the aircraft.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.11, 32.11, 32.11], "power_watts_avg": 32.11, "power_watts_peak": 32.11, "energy_joules_est": 8.53, "sample_count": 3, "duration_seconds": 0.266}, "timestamp": "2026-01-25T19:57:51.956896"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 203.438, "latencies_ms": [203.438], "images_per_second": 4.916, "prompt_tokens": 756, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nThe airplane is painted in a gold color and has blue accents. The sky above the plane appears to be cloudy with some sunlight shining through.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.11, 28.62, 28.62], "power_watts_avg": 29.79, "power_watts_peak": 32.11, "energy_joules_est": 6.08, "sample_count": 3, "duration_seconds": 0.204}, "timestamp": "2026-01-25T19:57:52.264064"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 244.687, "latencies_ms": [244.687], "images_per_second": 4.087, "prompt_tokens": 744, "response_tokens_est": 20, "n_tiles": 1, "output_text": "urn with water inside a bathroom stall, with a person's feet visible in front of it.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10466.7, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [28.62, 28.62, 28.62], "power_watts_avg": 28.62, "power_watts_peak": 28.62, "energy_joules_est": 7.02, "sample_count": 3, "duration_seconds": 0.245}, "timestamp": "2026-01-25T19:57:52.574557"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 72.729, "latencies_ms": [72.729], "images_per_second": 13.75, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Toilet bowl cleaner", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.7, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.85], "power_watts_avg": 32.85, "power_watts_peak": 32.85, "energy_joules_est": 2.41, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T19:57:52.679691"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 137.608, "latencies_ms": [137.608], "images_per_second": 7.267, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n A person is standing in front of a toilet with their feet on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.85, 32.85], "power_watts_avg": 32.85, "power_watts_peak": 32.85, "energy_joules_est": 4.54, "sample_count": 2, "duration_seconds": 0.138}, "timestamp": "2026-01-25T19:57:52.885679"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 309.64, "latencies_ms": [309.64], "images_per_second": 3.23, "prompt_tokens": 757, "response_tokens_est": 52, "n_tiles": 1, "output_text": "\nIn this black and white photo, a person's feet are seen inside an open toilet bowl. The toilet appears to be dirty or in need of cleaning. A towel can also be spotted near the toilet, possibly used for drying hands after washing them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 8.0}, "power_stats": {"power_watts_samples": [32.85, 32.85, 31.33, 31.33], "power_watts_avg": 32.09, "power_watts_peak": 32.85, "energy_joules_est": 9.95, "sample_count": 4, "duration_seconds": 0.31}, "timestamp": "2026-01-25T19:57:53.291402"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 131.1, "latencies_ms": [131.1], "images_per_second": 7.628, "prompt_tokens": 756, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n The image is a black and white photo of a toilet in the bathroom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 8.0}, "power_stats": {"power_watts_samples": [31.33, 31.33], "power_watts_avg": 31.33, "power_watts_peak": 31.33, "energy_joules_est": 4.12, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T19:57:53.496716"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 263.136, "latencies_ms": [263.136], "images_per_second": 3.8, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "iced skier in a blue jacket and black pants, wearing a white helmet, skiing down a snowy mountain slope.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 8.0}, "power_stats": {"power_watts_samples": [27.89, 27.89, 27.89], "power_watts_avg": 27.89, "power_watts_peak": 27.89, "energy_joules_est": 7.36, "sample_count": 3, "duration_seconds": 0.264}, "timestamp": "2026-01-25T19:57:53.810881"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 117.512, "latencies_ms": [117.512], "images_per_second": 8.51, "prompt_tokens": 759, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Snow covered trees and evergreen forest in the background", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [27.89, 27.89], "power_watts_avg": 27.89, "power_watts_peak": 27.89, "energy_joules_est": 3.29, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T19:57:54.016237"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 143.269, "latencies_ms": [143.269], "images_per_second": 6.98, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n 1. Skier in blue jacket and white helmet skiing down a snowy slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [32.71, 32.71], "power_watts_avg": 32.71, "power_watts_peak": 32.71, "energy_joules_est": 4.7, "sample_count": 2, "duration_seconds": 0.144}, "timestamp": "2026-01-25T19:57:54.223194"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 182.509, "latencies_ms": [182.509], "images_per_second": 5.479, "prompt_tokens": 757, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA skier in a blue jacket and white helmet is skiing down a snowy mountain trail, surrounded by snow-covered trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [32.71, 32.71], "power_watts_avg": 32.71, "power_watts_peak": 32.71, "energy_joules_est": 5.98, "sample_count": 2, "duration_seconds": 0.183}, "timestamp": "2026-01-25T19:57:54.428585"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 121.627, "latencies_ms": [121.627], "images_per_second": 8.222, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The skier is wearing a blue jacket and black pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [32.71, 30.78], "power_watts_avg": 31.75, "power_watts_peak": 32.71, "energy_joules_est": 3.87, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T19:57:54.634343"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 288.882, "latencies_ms": [288.882], "images_per_second": 3.462, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA tennis player in a yellow shirt and black shorts is preparing to hit a ball on a blue court, with an umpire and spectators nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [30.78, 30.78, 30.78], "power_watts_avg": 30.78, "power_watts_peak": 30.78, "energy_joules_est": 8.9, "sample_count": 3, "duration_seconds": 0.289}, "timestamp": "2026-01-25T19:57:54.940253"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 106.949, "latencies_ms": [106.949], "images_per_second": 9.35, "prompt_tokens": 759, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Man in yellow shirt and black shorts playing tennis", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.78, 32.21], "power_watts_avg": 31.5, "power_watts_peak": 32.21, "energy_joules_est": 3.38, "sample_count": 2, "duration_seconds": 0.107}, "timestamp": "2026-01-25T19:57:55.145318"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 121.273, "latencies_ms": [121.273], "images_per_second": 8.246, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. A man in a yellow shirt and black shorts playing tennis.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.21, 32.21], "power_watts_avg": 32.21, "power_watts_peak": 32.21, "energy_joules_est": 3.93, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T19:57:55.351505"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 220.63, "latencies_ms": [220.63], "images_per_second": 4.532, "prompt_tokens": 757, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA tennis player in a yellow shirt stands on a blue court, holding a racket. A referee or umpire can be seen behind him, observing the game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.21, 32.21, 31.73], "power_watts_avg": 32.05, "power_watts_peak": 32.21, "energy_joules_est": 7.08, "sample_count": 3, "duration_seconds": 0.221}, "timestamp": "2026-01-25T19:57:55.655655"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 85.381, "latencies_ms": [85.381], "images_per_second": 11.712, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "urns of light blue and gray.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.73], "power_watts_avg": 31.73, "power_watts_peak": 31.73, "energy_joules_est": 2.73, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T19:57:55.759925"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 265.838, "latencies_ms": [265.838], "images_per_second": 3.762, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "urns of food on a table, with a bowl of orange and white food in front of another bowl containing brown sauce.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10468.2, "ram_available_mb": 112038.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [31.73, 31.73, 31.73], "power_watts_avg": 31.73, "power_watts_peak": 31.73, "energy_joules_est": 8.44, "sample_count": 3, "duration_seconds": 0.266}, "timestamp": "2026-01-25T19:57:56.068703"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 81.667, "latencies_ms": [81.667], "images_per_second": 12.245, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Onion rings", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10468.2, "ram_available_mb": 112038.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10467.8, "ram_available_mb": 112038.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [31.51], "power_watts_avg": 31.51, "power_watts_peak": 31.51, "energy_joules_est": 2.58, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T19:57:56.175083"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 129.569, "latencies_ms": [129.569], "images_per_second": 7.718, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A bowl of food next to a plate with a glass on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.8, "ram_available_mb": 112038.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10469.7, "ram_available_mb": 112036.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [31.51, 31.51], "power_watts_avg": 31.51, "power_watts_peak": 31.51, "energy_joules_est": 4.09, "sample_count": 2, "duration_seconds": 0.13}, "timestamp": "2026-01-25T19:57:56.380420"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 151.183, "latencies_ms": [151.183], "images_per_second": 6.615, "prompt_tokens": 757, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA table with a plate of food, including a bowl of soup and a bowl of carrots.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.7, "ram_available_mb": 112036.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10467.1, "ram_available_mb": 112039.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [31.51, 31.51], "power_watts_avg": 31.51, "power_watts_peak": 31.51, "energy_joules_est": 4.78, "sample_count": 2, "duration_seconds": 0.152}, "timestamp": "2026-01-25T19:57:56.586578"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 99.813, "latencies_ms": [99.813], "images_per_second": 10.019, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urn of soup and bowl of food.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.1, "ram_available_mb": 112039.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 7.5, "ram_used_mb": 10467.9, "ram_available_mb": 112038.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [31.52], "power_watts_avg": 31.52, "power_watts_peak": 31.52, "energy_joules_est": 3.15, "sample_count": 1, "duration_seconds": 0.1}, "timestamp": "2026-01-25T19:57:56.691447"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 254.375, "latencies_ms": [254.375], "images_per_second": 3.931, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nTwo sheep, one brown and one white, stand in a grassy field with their heads down.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10467.9, "ram_available_mb": 112038.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.52, 31.52, 31.52], "power_watts_avg": 31.52, "power_watts_peak": 31.52, "energy_joules_est": 8.04, "sample_count": 3, "duration_seconds": 0.255}, "timestamp": "2026-01-25T19:57:57.004591"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.865, "latencies_ms": [78.865], "images_per_second": 12.68, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Sheep: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10474.7, "ram_available_mb": 112031.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.05], "power_watts_avg": 31.05, "power_watts_peak": 31.05, "energy_joules_est": 2.46, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T19:57:57.111843"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 100.843, "latencies_ms": [100.843], "images_per_second": 9.916, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Sheep in front of brick wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.7, "ram_available_mb": 112031.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10474.7, "ram_available_mb": 112031.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.05, 31.05], "power_watts_avg": 31.05, "power_watts_peak": 31.05, "energy_joules_est": 3.14, "sample_count": 2, "duration_seconds": 0.101}, "timestamp": "2026-01-25T19:57:57.317529"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 318.283, "latencies_ms": [318.283], "images_per_second": 3.142, "prompt_tokens": 757, "response_tokens_est": 53, "n_tiles": 1, "output_text": "\n A group of sheep are standing in a grassy area, with one sheep looking at the camera. The sheep appear to be gathered together, possibly grazing or resting. In the background, there is a brick wall that adds an interesting contrast to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.7, "ram_available_mb": 112031.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.05, 31.05, 30.74, 30.74], "power_watts_avg": 30.9, "power_watts_peak": 31.05, "energy_joules_est": 9.85, "sample_count": 4, "duration_seconds": 0.319}, "timestamp": "2026-01-25T19:57:57.725601"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 477.007, "latencies_ms": [477.007], "images_per_second": 2.096, "prompt_tokens": 756, "response_tokens_est": 85, "n_tiles": 1, "output_text": "\n The image features a group of sheep standing in the grass. There are at least six sheep visible, with one being brown and white, another being black and white, and three others having different colors. They appear to be gathered together under a brick building or fence, possibly seeking shelter from the sun. The scene is set outdoors on a sunny day, which casts light onto the sheep and highlights their woolly textures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.74, 30.74, 30.74, 29.83, 29.83], "power_watts_avg": 30.37, "power_watts_peak": 30.74, "energy_joules_est": 14.5, "sample_count": 5, "duration_seconds": 0.477}, "timestamp": "2026-01-25T19:57:58.233697"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 268.168, "latencies_ms": [268.168], "images_per_second": 3.729, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "iced fruit and banana slices are arranged in a circular pattern, creating an abstract composition on a blue background with white flowers.", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [29.83, 29.83, 29.83], "power_watts_avg": 29.83, "power_watts_peak": 29.83, "energy_joules_est": 8.02, "sample_count": 3, "duration_seconds": 0.269}, "timestamp": "2026-01-25T19:57:58.549192"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.51, "latencies_ms": [79.51], "images_per_second": 12.577, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Banana - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [38.42], "power_watts_avg": 38.42, "power_watts_peak": 38.42, "energy_joules_est": 3.07, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T19:57:58.656338"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 120.32, "latencies_ms": [120.32], "images_per_second": 8.311, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe apple is in the center of a bunch of bananas.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10465.5, "ram_available_mb": 112040.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [38.42, 38.42], "power_watts_avg": 38.42, "power_watts_peak": 38.42, "energy_joules_est": 4.65, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T19:57:58.862654"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 266.769, "latencies_ms": [266.769], "images_per_second": 3.749, "prompt_tokens": 757, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\nThe image features a close-up of an apple, surrounded by bananas. The apples are arranged in such a way that they appear to be wrapped around the bananas, creating a unique and visually striking composition.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.5, "ram_available_mb": 112040.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 1.0}, "power_stats": {"power_watts_samples": [38.42, 38.42, 34.11], "power_watts_avg": 36.98, "power_watts_peak": 38.42, "energy_joules_est": 9.88, "sample_count": 3, "duration_seconds": 0.267}, "timestamp": "2026-01-25T19:57:59.170316"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 316.667, "latencies_ms": [316.667], "images_per_second": 3.158, "prompt_tokens": 756, "response_tokens_est": 53, "n_tiles": 1, "output_text": "\nThe image features a close-up of an apple and bananas. The apple is red with a yellow stem on top, while the bananas are green with brown stems. The background appears to be a blue floral patterned surface that has a slightly blurred effect.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 1.0}, "power_stats": {"power_watts_samples": [34.11, 34.11, 34.11, 34.11], "power_watts_avg": 34.11, "power_watts_peak": 34.11, "energy_joules_est": 10.81, "sample_count": 4, "duration_seconds": 0.317}, "timestamp": "2026-01-25T19:57:59.576301"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 347.807, "latencies_ms": [347.807], "images_per_second": 2.875, "prompt_tokens": 744, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\nA blue and white train is traveling down a track, with its front facing towards the right side of the image. The train appears to be in motion, as indicated by the blurred background.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 1.0}, "power_stats": {"power_watts_samples": [31.37, 31.37, 31.37, 31.37], "power_watts_avg": 31.37, "power_watts_peak": 31.37, "energy_joules_est": 10.93, "sample_count": 4, "duration_seconds": 0.348}, "timestamp": "2026-01-25T19:57:59.988594"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 251.062, "latencies_ms": [251.062], "images_per_second": 3.983, "prompt_tokens": 759, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\n 1. Train car 2. Train car 3. Train car 4. Train car 5. Train car 6. Train car 7. Train car 8. Train car 9. Train car 10.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.37, 36.4, 36.4], "power_watts_avg": 34.72, "power_watts_peak": 36.4, "energy_joules_est": 8.73, "sample_count": 3, "duration_seconds": 0.251}, "timestamp": "2026-01-25T19:58:00.294376"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 106.241, "latencies_ms": [106.241], "images_per_second": 9.413, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Train on left side of photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.4, 36.4], "power_watts_avg": 36.4, "power_watts_peak": 36.4, "energy_joules_est": 3.88, "sample_count": 2, "duration_seconds": 0.107}, "timestamp": "2026-01-25T19:58:00.500286"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 281.977, "latencies_ms": [281.977], "images_per_second": 3.546, "prompt_tokens": 757, "response_tokens_est": 46, "n_tiles": 1, "output_text": "\nA blue and white train is traveling down a track, with trees in the background. The train appears to be stopped at a station or platform, as there are people visible on the platform waiting for the train to depart.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10457.4, "ram_available_mb": 112048.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.92, 31.92, 31.92], "power_watts_avg": 31.92, "power_watts_peak": 31.92, "energy_joules_est": 9.02, "sample_count": 3, "duration_seconds": 0.282}, "timestamp": "2026-01-25T19:58:00.806506"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 315.77, "latencies_ms": [315.77], "images_per_second": 3.167, "prompt_tokens": 756, "response_tokens_est": 53, "n_tiles": 1, "output_text": "\n The image shows a blue and white train traveling down the tracks. The sky is clear and blue with no clouds visible. There are two people on the train, one sitting in the middle of the car and another person standing near the front of the car.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.7, "ram_available_mb": 112048.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [31.92, 31.92, 31.23, 31.23], "power_watts_avg": 31.58, "power_watts_peak": 31.92, "energy_joules_est": 9.98, "sample_count": 4, "duration_seconds": 0.316}, "timestamp": "2026-01-25T19:58:01.213827"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 267.996, "latencies_ms": [267.996], "images_per_second": 3.731, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urns of water are in a white bathtub, with a red rug on the floor and a shower curtain nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [31.23, 31.23, 31.23], "power_watts_avg": 31.23, "power_watts_peak": 31.23, "energy_joules_est": 8.39, "sample_count": 3, "duration_seconds": 0.269}, "timestamp": "2026-01-25T19:58:01.524106"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 101.12, "latencies_ms": [101.12], "images_per_second": 9.889, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Shower curtain - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [36.83, 36.83], "power_watts_avg": 36.83, "power_watts_peak": 36.83, "energy_joules_est": 3.74, "sample_count": 2, "duration_seconds": 0.102}, "timestamp": "2026-01-25T19:58:01.729650"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 121.586, "latencies_ms": [121.586], "images_per_second": 8.225, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Shower curtain on left side of bathtub.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10456.3, "ram_available_mb": 112050.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [36.83, 36.83], "power_watts_avg": 36.83, "power_watts_peak": 36.83, "energy_joules_est": 4.49, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T19:58:01.935232"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 408.98, "latencies_ms": [408.98], "images_per_second": 2.445, "prompt_tokens": 757, "response_tokens_est": 71, "n_tiles": 1, "output_text": "\nThe image shows a large, well-lit bathroom with white walls. The room features two sinks under a mirror, one on each side of the bathtub. A red shower curtain hangs over the tub, adding a pop of color to the space. There are also two lights above the mirrors, providing ample illumination for grooming and personal care tasks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.3, "ram_available_mb": 112050.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10459.8, "ram_available_mb": 112046.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [36.83, 29.95, 29.95, 29.95, 29.95], "power_watts_avg": 31.33, "power_watts_peak": 36.83, "energy_joules_est": 12.83, "sample_count": 5, "duration_seconds": 0.41}, "timestamp": "2026-01-25T19:58:02.441874"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 84.693, "latencies_ms": [84.693], "images_per_second": 11.807, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "urns of red and brown colors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.8, "ram_available_mb": 112046.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [29.95], "power_watts_avg": 29.95, "power_watts_peak": 29.95, "energy_joules_est": 2.55, "sample_count": 1, "duration_seconds": 0.085}, "timestamp": "2026-01-25T19:58:02.547755"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 299.009, "latencies_ms": [299.009], "images_per_second": 3.344, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA surfer in a wetsuit is skillfully riding a large wave on their surfboard, with the ocean and sky as the backdrop.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10461.8, "ram_available_mb": 112044.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [27.16, 27.16, 27.16], "power_watts_avg": 27.16, "power_watts_peak": 27.16, "energy_joules_est": 8.13, "sample_count": 3, "duration_seconds": 0.299}, "timestamp": "2026-01-25T19:58:02.855849"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.941, "latencies_ms": [79.941], "images_per_second": 12.509, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.8, "ram_available_mb": 112044.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10462.5, "ram_available_mb": 112043.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [27.16], "power_watts_avg": 27.16, "power_watts_peak": 27.16, "energy_joules_est": 2.18, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T19:58:02.960341"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 145.464, "latencies_ms": [145.464], "images_per_second": 6.875, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n 1. Surfer in black wetsuit on surfboard riding a wave.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10462.5, "ram_available_mb": 112043.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10460.6, "ram_available_mb": 112045.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [27.16, 35.36], "power_watts_avg": 31.26, "power_watts_peak": 35.36, "energy_joules_est": 4.55, "sample_count": 2, "duration_seconds": 0.146}, "timestamp": "2026-01-25T19:58:03.165522"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 61.029, "latencies_ms": [61.029], "images_per_second": 16.386, "prompt_tokens": 757, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.6, "ram_available_mb": 112045.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10463.5, "ram_available_mb": 112042.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [35.36], "power_watts_avg": 35.36, "power_watts_peak": 35.36, "energy_joules_est": 2.17, "sample_count": 1, "duration_seconds": 0.061}, "timestamp": "2026-01-25T19:58:03.272336"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 414.689, "latencies_ms": [414.689], "images_per_second": 2.411, "prompt_tokens": 756, "response_tokens_est": 70, "n_tiles": 1, "output_text": "\n The image is a black and white photo of a surfer riding a wave. The surfboard is white with a black design on the bottom. The surfer is wearing a black wetsuit while holding onto his surfboard as he rides the wave. The background features an ocean scene, which adds to the overall atmosphere of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.5, "ram_available_mb": 112042.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 10455.3, "ram_available_mb": 112051.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [35.36, 35.36, 35.36, 32.46, 32.46], "power_watts_avg": 34.2, "power_watts_peak": 35.36, "energy_joules_est": 14.19, "sample_count": 5, "duration_seconds": 0.415}, "timestamp": "2026-01-25T19:58:03.779107"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 420.45, "latencies_ms": [420.45], "images_per_second": 2.378, "prompt_tokens": 744, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\nA baseball game is in progress, with a batter at home plate holding his bat and waiting for the pitch. Behind him, a catcher crouches behind home plate, ready to receive the ball if it comes their way. An umpire stands nearby, observing the play closely.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.3, "ram_available_mb": 112051.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10453.4, "ram_available_mb": 112052.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [32.46, 32.46, 32.46, 33.88, 33.88], "power_watts_avg": 33.03, "power_watts_peak": 33.88, "energy_joules_est": 13.9, "sample_count": 5, "duration_seconds": 0.421}, "timestamp": "2026-01-25T19:58:04.289444"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.701, "latencies_ms": [78.701], "images_per_second": 12.706, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Batter: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.4, "ram_available_mb": 112052.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10455.6, "ram_available_mb": 112050.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [33.88], "power_watts_avg": 33.88, "power_watts_peak": 33.88, "energy_joules_est": 2.68, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T19:58:04.394681"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 131.854, "latencies_ms": [131.854], "images_per_second": 7.584, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n 1. Batter holding a bat and waiting for the pitch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.6, "ram_available_mb": 112050.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10454.1, "ram_available_mb": 112052.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [33.88, 35.2], "power_watts_avg": 34.54, "power_watts_peak": 35.2, "energy_joules_est": 4.56, "sample_count": 2, "duration_seconds": 0.132}, "timestamp": "2026-01-25T19:58:04.600452"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 389.105, "latencies_ms": [389.105], "images_per_second": 2.57, "prompt_tokens": 757, "response_tokens_est": 68, "n_tiles": 1, "output_text": "\nA baseball game is in progress, with a batter at home plate holding his bat over his shoulder. Behind him, a catcher crouches behind home plate, ready to receive the pitch. An umpire stands nearby, observing the play closely. The scene takes place on a baseball field, and there are spectators watching from the sidelines.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.1, "ram_available_mb": 112052.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10453.5, "ram_available_mb": 112052.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.2, 35.2, 35.2, 35.2], "power_watts_avg": 35.2, "power_watts_peak": 35.2, "energy_joules_est": 13.72, "sample_count": 4, "duration_seconds": 0.39}, "timestamp": "2026-01-25T19:58:05.007566"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 456.534, "latencies_ms": [456.534], "images_per_second": 2.19, "prompt_tokens": 756, "response_tokens_est": 82, "n_tiles": 1, "output_text": "\n The image shows a baseball game in progress. A batter is standing at home plate holding his bat and waiting for the pitch. Behind him, there are several people watching the game, including players on the field and others seated in the dugout. Some of these spectators have water bottles with them to stay hydrated during the match. The scene captures a typical baseball game moment filled with anticipation and excitement.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.5, "ram_available_mb": 112052.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10457.6, "ram_available_mb": 112048.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.86, 32.86, 32.86, 32.86, 32.86], "power_watts_avg": 32.86, "power_watts_peak": 32.86, "energy_joules_est": 15.01, "sample_count": 5, "duration_seconds": 0.457}, "timestamp": "2026-01-25T19:58:05.514516"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 340.108, "latencies_ms": [340.108], "images_per_second": 2.94, "prompt_tokens": 744, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\nA black and white photograph features a variety of fruits and vegetables, including apples, grapes, and a large orange ball. The objects are arranged in an artistic manner on a table or surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.6, "ram_available_mb": 112048.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.42, 36.42, 36.42, 36.42], "power_watts_avg": 36.42, "power_watts_peak": 36.42, "energy_joules_est": 12.41, "sample_count": 4, "duration_seconds": 0.341}, "timestamp": "2026-01-25T19:58:05.925680"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 69.954, "latencies_ms": [69.954], "images_per_second": 14.295, "prompt_tokens": 759, "response_tokens_est": 4, "n_tiles": 1, "output_text": "\n Apple 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [36.42], "power_watts_avg": 36.42, "power_watts_peak": 36.42, "energy_joules_est": 2.56, "sample_count": 1, "duration_seconds": 0.07}, "timestamp": "2026-01-25T19:58:06.030941"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 533.217, "latencies_ms": [533.217], "images_per_second": 1.875, "prompt_tokens": 763, "response_tokens_est": 93, "n_tiles": 1, "output_text": "\nThe image features a close-up of various fruits and vegetables on display. There are two apples in the scene - one positioned towards the left side and another closer to the center. A bunch of grapes is also present, with some grapes located at the top right corner and others scattered around the middle area. Additionally, there's an orange placed near the bottom left corner. The arrangement of these items creates a visually appealing composition that highlights their colors and textures.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10456.7, "ram_available_mb": 112049.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [37.15, 37.15, 37.15, 37.15, 37.15, 36.73], "power_watts_avg": 37.08, "power_watts_peak": 37.15, "energy_joules_est": 19.78, "sample_count": 6, "duration_seconds": 0.534}, "timestamp": "2026-01-25T19:58:06.638209"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 174.815, "latencies_ms": [174.815], "images_per_second": 5.72, "prompt_tokens": 757, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA black and white photograph of a still life arrangement featuring an apple, grapes, a melon, and peanuts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.7, "ram_available_mb": 112049.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10448.4, "ram_available_mb": 112057.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [36.73, 36.73], "power_watts_avg": 36.73, "power_watts_peak": 36.73, "energy_joules_est": 6.43, "sample_count": 2, "duration_seconds": 0.175}, "timestamp": "2026-01-25T19:58:06.843176"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 310.347, "latencies_ms": [310.347], "images_per_second": 3.222, "prompt_tokens": 756, "response_tokens_est": 55, "n_tiles": 1, "output_text": "\nThe image is a black and white photograph of various fruits and vegetables. The fruits include an apple, grapes, and a melon, while the vegetables consist of beans and peanuts. The composition of these objects creates a visually appealing contrast between the different textures and shapes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.4, "ram_available_mb": 112057.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10448.8, "ram_available_mb": 112057.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [36.73, 36.73, 34.04, 34.04], "power_watts_avg": 35.39, "power_watts_peak": 36.73, "energy_joules_est": 11.0, "sample_count": 4, "duration_seconds": 0.311}, "timestamp": "2026-01-25T19:58:07.251023"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 366.15, "latencies_ms": [366.15], "images_per_second": 2.731, "prompt_tokens": 744, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\nA street with a sidewalk runs through it, lined by buildings of various sizes and colors. Cars are parked along the side of the road, and trees can be seen in front of some of the buildings.", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 10448.8, "ram_available_mb": 112057.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10447.1, "ram_available_mb": 112059.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [34.04, 34.04, 34.04, 34.94], "power_watts_avg": 34.27, "power_watts_peak": 34.94, "energy_joules_est": 12.56, "sample_count": 4, "duration_seconds": 0.366}, "timestamp": "2026-01-25T19:58:07.664026"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 222.419, "latencies_ms": [222.419], "images_per_second": 4.496, "prompt_tokens": 759, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\n 1. Building  2. Building  3. Building  4. Building  5. Building  6. Building  7. Building  8. Building", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.1, "ram_available_mb": 112059.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10447.1, "ram_available_mb": 112059.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [34.94, 34.94, 34.94], "power_watts_avg": 34.94, "power_watts_peak": 34.94, "energy_joules_est": 7.78, "sample_count": 3, "duration_seconds": 0.223}, "timestamp": "2026-01-25T19:58:07.970539"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 123.12, "latencies_ms": [123.12], "images_per_second": 8.122, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA street with cars parked on it and a building in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10447.1, "ram_available_mb": 112059.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_watts_samples": [34.94, 34.88], "power_watts_avg": 34.91, "power_watts_peak": 34.94, "energy_joules_est": 4.32, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T19:58:08.176223"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 353.646, "latencies_ms": [353.646], "images_per_second": 2.828, "prompt_tokens": 757, "response_tokens_est": 58, "n_tiles": 1, "output_text": "\nThe image shows a street with tall buildings on both sides, including apartment buildings. The street has a sidewalk running alongside it, and there are cars parked along the side of the road. A few people can be seen walking or standing near the sidewalk, adding to the urban atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_watts_samples": [34.88, 34.88, 34.88, 34.88], "power_watts_avg": 34.88, "power_watts_peak": 34.88, "energy_joules_est": 12.36, "sample_count": 4, "duration_seconds": 0.354}, "timestamp": "2026-01-25T19:58:08.583520"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 100.421, "latencies_ms": [100.421], "images_per_second": 9.958, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The sky is blue and cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10440.1, "ram_available_mb": 112066.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_watts_samples": [32.01, 32.01], "power_watts_avg": 32.01, "power_watts_peak": 32.01, "energy_joules_est": 3.23, "sample_count": 2, "duration_seconds": 0.101}, "timestamp": "2026-01-25T19:58:08.789603"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 242.682, "latencies_ms": [242.682], "images_per_second": 4.121, "prompt_tokens": 744, "response_tokens_est": 18, "n_tiles": 1, "output_text": "urns on a shelf behind two people posing for a picture in a bar setting.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10440.1, "ram_available_mb": 112066.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10440.9, "ram_available_mb": 112065.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [32.01, 32.01, 32.08], "power_watts_avg": 32.03, "power_watts_peak": 32.08, "energy_joules_est": 7.79, "sample_count": 3, "duration_seconds": 0.243}, "timestamp": "2026-01-25T19:58:09.100579"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 298.466, "latencies_ms": [298.466], "images_per_second": 3.35, "prompt_tokens": 759, "response_tokens_est": 47, "n_tiles": 1, "output_text": "\n 1. Tv screen 2. Man's arm 3. Woman's arm 4. Man's shoulder 5. Woman's shoulder 6. Man's elbow 7. Woman's elbow 8. Man's hand 9. Woman's hand", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10440.9, "ram_available_mb": 112065.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10440.8, "ram_available_mb": 112065.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [32.08, 32.08, 32.08], "power_watts_avg": 32.08, "power_watts_peak": 32.08, "energy_joules_est": 9.59, "sample_count": 3, "duration_seconds": 0.299}, "timestamp": "2026-01-25T19:58:09.405764"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 172.327, "latencies_ms": [172.327], "images_per_second": 5.803, "prompt_tokens": 763, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n 1. A man wearing a tie and glasses is hugging another woman who has her arm around him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10440.8, "ram_available_mb": 112065.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10440.8, "ram_available_mb": 112065.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [32.08, 31.5], "power_watts_avg": 31.79, "power_watts_peak": 32.08, "energy_joules_est": 5.49, "sample_count": 2, "duration_seconds": 0.173}, "timestamp": "2026-01-25T19:58:09.610984"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 258.003, "latencies_ms": [258.003], "images_per_second": 3.876, "prompt_tokens": 757, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\nA man in a purple shirt stands next to a woman wearing a white tank top, both smiling at the camera. They are standing behind a bar counter with a TV screen above them.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10440.8, "ram_available_mb": 112065.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10448.4, "ram_available_mb": 112057.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [31.5, 31.5, 31.5], "power_watts_avg": 31.5, "power_watts_peak": 31.5, "energy_joules_est": 8.15, "sample_count": 3, "duration_seconds": 0.259}, "timestamp": "2026-01-25T19:58:09.917001"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 115.891, "latencies_ms": [115.891], "images_per_second": 8.629, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "urns of light on the wall and a television screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.4, "ram_available_mb": 112057.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10449.1, "ram_available_mb": 112057.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [31.5, 31.5], "power_watts_avg": 31.5, "power_watts_peak": 31.5, "energy_joules_est": 3.66, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T19:58:10.124187"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 256.157, "latencies_ms": [256.157], "images_per_second": 3.904, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA woman in a costume stands out from the crowd, holding a cell phone and smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10449.1, "ram_available_mb": 112057.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [31.5, 31.5, 31.5], "power_watts_avg": 31.5, "power_watts_peak": 31.5, "energy_joules_est": 8.08, "sample_count": 3, "duration_seconds": 0.256}, "timestamp": "2026-01-25T19:58:10.437048"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 106.65, "latencies_ms": [106.65], "images_per_second": 9.376, "prompt_tokens": 759, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Woman in gold and black costume - 2", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [31.5, 32.62], "power_watts_avg": 32.06, "power_watts_peak": 32.62, "energy_joules_est": 3.43, "sample_count": 2, "duration_seconds": 0.107}, "timestamp": "2026-01-25T19:58:10.643455"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 113.562, "latencies_ms": [113.562], "images_per_second": 8.806, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nThe woman is talking on a cell phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [32.62, 32.62], "power_watts_avg": 32.62, "power_watts_peak": 32.62, "energy_joules_est": 3.72, "sample_count": 2, "duration_seconds": 0.114}, "timestamp": "2026-01-25T19:58:10.849901"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 255.466, "latencies_ms": [255.466], "images_per_second": 3.914, "prompt_tokens": 757, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\nA woman in a costume stands out among a crowd of people, holding a cell phone to her ear. The group appears to be gathered for an event or performance, with some individuals wearing costumes as well.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10463.6, "ram_available_mb": 112042.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [32.62, 32.62, 29.95], "power_watts_avg": 31.73, "power_watts_peak": 32.62, "energy_joules_est": 8.13, "sample_count": 3, "duration_seconds": 0.256}, "timestamp": "2026-01-25T19:58:11.156793"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 110.567, "latencies_ms": [110.567], "images_per_second": 9.044, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The woman is wearing a gold helmet and black dress.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.6, "ram_available_mb": 112042.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [29.95, 29.95], "power_watts_avg": 29.95, "power_watts_peak": 29.95, "energy_joules_est": 3.32, "sample_count": 2, "duration_seconds": 0.111}, "timestamp": "2026-01-25T19:58:11.362154"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 255.235, "latencies_ms": [255.235], "images_per_second": 3.918, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA white toilet is situated in a bathroom with a shower and two buckets, one red and one green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10459.3, "ram_available_mb": 112047.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [29.95, 29.95, 29.05], "power_watts_avg": 29.65, "power_watts_peak": 29.95, "energy_joules_est": 7.58, "sample_count": 3, "duration_seconds": 0.256}, "timestamp": "2026-01-25T19:58:11.673899"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 103.335, "latencies_ms": [103.335], "images_per_second": 9.677, "prompt_tokens": 759, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Shower head and hose 2.0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.3, "ram_available_mb": 112047.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [29.05, 29.05], "power_watts_avg": 29.05, "power_watts_peak": 29.05, "energy_joules_est": 3.01, "sample_count": 2, "duration_seconds": 0.104}, "timestamp": "2026-01-25T19:58:11.880032"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 123.408, "latencies_ms": [123.408], "images_per_second": 8.103, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "urn with water on floor and bucket next to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [29.05, 29.05], "power_watts_avg": 29.05, "power_watts_peak": 29.05, "energy_joules_est": 3.6, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T19:58:12.087277"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 239.14, "latencies_ms": [239.14], "images_per_second": 4.182, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA white toilet with a blue brush sits in a bathroom, accompanied by two buckets. The room features white tiles on the walls and floor, creating a clean and simple atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.41, 30.41, 30.41], "power_watts_avg": 30.41, "power_watts_peak": 30.41, "energy_joules_est": 7.28, "sample_count": 3, "duration_seconds": 0.239}, "timestamp": "2026-01-25T19:58:12.393894"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 105.089, "latencies_ms": [105.089], "images_per_second": 9.516, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The bathroom is white and has a green bucket.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10463.0, "ram_available_mb": 112043.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.41, 28.98], "power_watts_avg": 29.69, "power_watts_peak": 30.41, "energy_joules_est": 3.14, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T19:58:12.600412"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 245.324, "latencies_ms": [245.324], "images_per_second": 4.076, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA man with glasses and a beard stands next to an elephant, both smiling as they interact.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10463.0, "ram_available_mb": 112043.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10454.3, "ram_available_mb": 112052.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [28.98, 28.98, 28.98], "power_watts_avg": 28.98, "power_watts_peak": 28.98, "energy_joules_est": 7.12, "sample_count": 3, "duration_seconds": 0.246}, "timestamp": "2026-01-25T19:58:12.914273"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 73.207, "latencies_ms": [73.207], "images_per_second": 13.66, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Elephant trunk", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.3, "ram_available_mb": 112052.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10450.6, "ram_available_mb": 112055.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [28.98], "power_watts_avg": 28.98, "power_watts_peak": 28.98, "energy_joules_est": 2.14, "sample_count": 1, "duration_seconds": 0.074}, "timestamp": "2026-01-25T19:58:13.019848"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 92.034, "latencies_ms": [92.034], "images_per_second": 10.866, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nThe elephant is in front of a man.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.6, "ram_available_mb": 112055.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10452.7, "ram_available_mb": 112053.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [30.29], "power_watts_avg": 30.29, "power_watts_peak": 30.29, "energy_joules_est": 2.8, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T19:58:13.124920"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 59.777, "latencies_ms": [59.777], "images_per_second": 16.729, "prompt_tokens": 757, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.7, "ram_available_mb": 112053.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [30.29], "power_watts_avg": 30.29, "power_watts_peak": 30.29, "energy_joules_est": 1.82, "sample_count": 1, "duration_seconds": 0.06}, "timestamp": "2026-01-25T19:58:13.229800"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 96.884, "latencies_ms": [96.884], "images_per_second": 10.322, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The man is wearing glasses and a tan shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10456.3, "ram_available_mb": 112050.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [30.29], "power_watts_avg": 30.29, "power_watts_peak": 30.29, "energy_joules_est": 2.94, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T19:58:13.335745"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 257.062, "latencies_ms": [257.062], "images_per_second": 3.89, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nFive children sit on a grassy field, each holding a white frisbee with black and red designs.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 10456.3, "ram_available_mb": 112050.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [30.29, 30.29, 32.42], "power_watts_avg": 31.0, "power_watts_peak": 32.42, "energy_joules_est": 7.98, "sample_count": 3, "duration_seconds": 0.257}, "timestamp": "2026-01-25T19:58:13.650812"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 91.581, "latencies_ms": [91.581], "images_per_second": 10.919, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n Frisbee 1: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [32.42], "power_watts_avg": 32.42, "power_watts_peak": 32.42, "energy_joules_est": 2.99, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T19:58:13.755623"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 121.098, "latencies_ms": [121.098], "images_per_second": 8.258, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nTwo white frisbees in front of a group of children.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 10455.4, "ram_available_mb": 112050.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [32.42, 32.42], "power_watts_avg": 32.42, "power_watts_peak": 32.42, "energy_joules_est": 3.93, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T19:58:13.960204"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 160.827, "latencies_ms": [160.827], "images_per_second": 6.218, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nFive children are sitting on a grassy field, with two of them holding white frisbees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.4, "ram_available_mb": 112050.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10458.9, "ram_available_mb": 112047.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [32.42, 29.6], "power_watts_avg": 31.01, "power_watts_peak": 32.42, "energy_joules_est": 4.99, "sample_count": 2, "duration_seconds": 0.161}, "timestamp": "2026-01-25T19:58:14.165811"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 504.598, "latencies_ms": [504.598], "images_per_second": 1.982, "prompt_tokens": 756, "response_tokens_est": 93, "n_tiles": 1, "output_text": "\n The image shows a group of children sitting on the grass. There are five kids in total, with two boys and three girls. One boy is holding a white Frisbee, while another boy has a white Frisbee in his hand as well. A girl is also holding a white Frisbee. All the children appear to be smiling for the camera. The background of the image features trees, suggesting that they are outdoors on a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.9, "ram_available_mb": 112047.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [29.6, 29.6, 29.6, 29.6, 31.89, 31.89], "power_watts_avg": 30.36, "power_watts_peak": 31.89, "energy_joules_est": 15.33, "sample_count": 6, "duration_seconds": 0.505}, "timestamp": "2026-01-25T19:58:14.771469"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 265.591, "latencies_ms": [265.591], "images_per_second": 3.765, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urns of water on a sidewalk, with a little girl holding an umbrella and standing in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [31.89, 31.89, 31.89], "power_watts_avg": 31.89, "power_watts_peak": 31.89, "energy_joules_est": 8.5, "sample_count": 3, "duration_seconds": 0.266}, "timestamp": "2026-01-25T19:58:15.083117"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.484, "latencies_ms": [78.484], "images_per_second": 12.741, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10441.2, "ram_available_mb": 112065.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [37.43], "power_watts_avg": 37.43, "power_watts_peak": 37.43, "energy_joules_est": 2.96, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T19:58:15.188767"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 137.716, "latencies_ms": [137.716], "images_per_second": 7.261, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA young girl in a red coat stands under an umbrella on a rainy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10441.2, "ram_available_mb": 112065.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10441.9, "ram_available_mb": 112064.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [37.43, 37.43], "power_watts_avg": 37.43, "power_watts_peak": 37.43, "energy_joules_est": 5.17, "sample_count": 2, "duration_seconds": 0.138}, "timestamp": "2026-01-25T19:58:15.394255"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 179.336, "latencies_ms": [179.336], "images_per_second": 5.576, "prompt_tokens": 757, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA young girl in a red coat stands on a sidewalk holding an umbrella, with her reflection visible in the wet pavement.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 10441.9, "ram_available_mb": 112064.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10440.9, "ram_available_mb": 112065.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [37.43, 32.35], "power_watts_avg": 34.89, "power_watts_peak": 37.43, "energy_joules_est": 6.28, "sample_count": 2, "duration_seconds": 0.18}, "timestamp": "2026-01-25T19:58:15.601943"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 124.593, "latencies_ms": [124.593], "images_per_second": 8.026, "prompt_tokens": 756, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n A young girl in a red coat and black pants is holding an umbrella.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10440.9, "ram_available_mb": 112065.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10442.7, "ram_available_mb": 112063.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [32.35, 32.35], "power_watts_avg": 32.35, "power_watts_peak": 32.35, "energy_joules_est": 4.05, "sample_count": 2, "duration_seconds": 0.125}, "timestamp": "2026-01-25T19:58:15.808926"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 300.571, "latencies_ms": [300.571], "images_per_second": 3.327, "prompt_tokens": 744, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA group of elephants, including a baby elephant with large ears and a trunk, are walking through a dirt path in front of a body of water.", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 10442.7, "ram_available_mb": 112063.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10442.7, "ram_available_mb": 112063.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [32.35, 32.35, 31.0, 31.0], "power_watts_avg": 31.68, "power_watts_peak": 32.35, "energy_joules_est": 9.55, "sample_count": 4, "duration_seconds": 0.302}, "timestamp": "2026-01-25T19:58:16.225029"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 86.563, "latencies_ms": [86.563], "images_per_second": 11.552, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Elephant : 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10442.7, "ram_available_mb": 112063.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10441.4, "ram_available_mb": 112064.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [31.0], "power_watts_avg": 31.0, "power_watts_peak": 31.0, "energy_joules_est": 2.69, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T19:58:16.329522"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 113.355, "latencies_ms": [113.355], "images_per_second": 8.822, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nThe elephant in front is looking at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10441.4, "ram_available_mb": 112064.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10441.4, "ram_available_mb": 112064.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [31.0, 31.0], "power_watts_avg": 31.0, "power_watts_peak": 31.0, "energy_joules_est": 3.52, "sample_count": 2, "duration_seconds": 0.114}, "timestamp": "2026-01-25T19:58:16.534741"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 255.384, "latencies_ms": [255.384], "images_per_second": 3.916, "prompt_tokens": 757, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\n A group of elephants, including a baby elephant, are walking through a dirt field. The baby elephant has its trunk extended forward as if reaching out to something or someone in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10441.4, "ram_available_mb": 112064.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10444.1, "ram_available_mb": 112062.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [31.46, 31.46, 31.46], "power_watts_avg": 31.46, "power_watts_peak": 31.46, "energy_joules_est": 8.05, "sample_count": 3, "duration_seconds": 0.256}, "timestamp": "2026-01-25T19:58:16.841934"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 89.766, "latencies_ms": [89.766], "images_per_second": 11.14, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The elephant is brown and gray.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10444.1, "ram_available_mb": 112062.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10442.5, "ram_available_mb": 112063.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [31.46], "power_watts_avg": 31.46, "power_watts_peak": 31.46, "energy_joules_est": 2.83, "sample_count": 1, "duration_seconds": 0.09}, "timestamp": "2026-01-25T19:58:16.947787"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 278.101, "latencies_ms": [278.101], "images_per_second": 3.596, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA surfer in a red shirt is skillfully riding a wave on their surfboard, with arms outstretched for balance and control.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10442.5, "ram_available_mb": 112063.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10446.1, "ram_available_mb": 112060.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [31.46, 27.38, 27.38], "power_watts_avg": 28.74, "power_watts_peak": 31.46, "energy_joules_est": 8.0, "sample_count": 3, "duration_seconds": 0.278}, "timestamp": "2026-01-25T19:58:17.264027"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 102.653, "latencies_ms": [102.653], "images_per_second": 9.742, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Water: 0.56", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10446.1, "ram_available_mb": 112060.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10443.0, "ram_available_mb": 112063.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [27.38, 27.38], "power_watts_avg": 27.38, "power_watts_peak": 27.38, "energy_joules_est": 2.83, "sample_count": 2, "duration_seconds": 0.103}, "timestamp": "2026-01-25T19:58:17.469818"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 127.407, "latencies_ms": [127.407], "images_per_second": 7.849, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nThe surfer is riding a wave in front of a picture.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10443.0, "ram_available_mb": 112063.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10443.4, "ram_available_mb": 112062.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [27.38, 31.56], "power_watts_avg": 29.47, "power_watts_peak": 31.56, "energy_joules_est": 3.76, "sample_count": 2, "duration_seconds": 0.128}, "timestamp": "2026-01-25T19:58:17.675643"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 206.75, "latencies_ms": [206.75], "images_per_second": 4.837, "prompt_tokens": 757, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA surfer in a red shirt is riding a wave on his surfboard, skillfully navigating through the greenish-blue ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10443.4, "ram_available_mb": 112062.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10442.2, "ram_available_mb": 112064.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [31.56, 31.56, 31.56], "power_watts_avg": 31.56, "power_watts_peak": 31.56, "energy_joules_est": 6.53, "sample_count": 3, "duration_seconds": 0.207}, "timestamp": "2026-01-25T19:58:17.981089"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 553.083, "latencies_ms": [553.083], "images_per_second": 1.808, "prompt_tokens": 756, "response_tokens_est": 100, "n_tiles": 1, "output_text": "\n The image shows a surfer riding a wave on his surfboard. He is wearing a red shirt and green shorts while surfing the wave in the ocean. The colors of the scene are predominantly green and white from the water and the surfboard, with some brown tones from the surfer's clothing. The lighting appears to be natural, as it is daytime, which creates a beautiful contrast between the bright sunlight reflecting off the water and the darker shadows cast by the surfer on his board.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10442.2, "ram_available_mb": 112064.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10440.0, "ram_available_mb": 112066.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 68.0}, "power_stats": {"power_watts_samples": [31.56, 30.8, 30.8, 30.8, 30.8, 30.8], "power_watts_avg": 30.93, "power_watts_peak": 31.56, "energy_joules_est": 17.11, "sample_count": 6, "duration_seconds": 0.553}, "timestamp": "2026-01-25T19:58:18.589980"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 278.861, "latencies_ms": [278.861], "images_per_second": 3.586, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nTwo men on horseback are engaged in a playful fight, with one man holding his leg and the other man holding a stick.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10439.7, "ram_available_mb": 112066.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10441.0, "ram_available_mb": 112065.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 68.0}, "power_stats": {"power_watts_samples": [33.09, 33.09, 33.09], "power_watts_avg": 33.09, "power_watts_peak": 33.09, "energy_joules_est": 9.24, "sample_count": 3, "duration_seconds": 0.279}, "timestamp": "2026-01-25T19:58:18.902626"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 75.726, "latencies_ms": [75.726], "images_per_second": 13.206, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10441.0, "ram_available_mb": 112065.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10443.1, "ram_available_mb": 112063.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [33.09], "power_watts_avg": 33.09, "power_watts_peak": 33.09, "energy_joules_est": 2.51, "sample_count": 1, "duration_seconds": 0.076}, "timestamp": "2026-01-25T19:58:19.007630"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 148.303, "latencies_ms": [148.303], "images_per_second": 6.743, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nA man on a horse is in front of another man riding a horse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10443.1, "ram_available_mb": 112063.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10443.3, "ram_available_mb": 112063.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [37.92, 37.92], "power_watts_avg": 37.92, "power_watts_peak": 37.92, "energy_joules_est": 5.65, "sample_count": 2, "duration_seconds": 0.149}, "timestamp": "2026-01-25T19:58:19.213590"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 126.254, "latencies_ms": [126.254], "images_per_second": 7.921, "prompt_tokens": 757, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n Two men on horseback are racing each other along a beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10443.3, "ram_available_mb": 112063.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10445.8, "ram_available_mb": 112060.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [37.92, 37.92], "power_watts_avg": 37.92, "power_watts_peak": 37.92, "energy_joules_est": 4.81, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T19:58:19.419757"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 261.1, "latencies_ms": [261.1], "images_per_second": 3.83, "prompt_tokens": 756, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\n The image shows two men riding horses on a beach. One of the men is wearing a white shirt and hat while the other has a red hat. They are both holding sticks in their hands.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.8, "ram_available_mb": 112060.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10448.2, "ram_available_mb": 112058.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [37.92, 30.33, 30.33], "power_watts_avg": 32.86, "power_watts_peak": 37.92, "energy_joules_est": 8.6, "sample_count": 3, "duration_seconds": 0.262}, "timestamp": "2026-01-25T19:58:19.726261"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 255.142, "latencies_ms": [255.142], "images_per_second": 3.919, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA red and black dirt bike is parked in a garage, with a small dog sitting on its seat.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10448.2, "ram_available_mb": 112058.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10449.2, "ram_available_mb": 112057.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [30.33, 30.33, 30.33], "power_watts_avg": 30.33, "power_watts_peak": 30.33, "energy_joules_est": 7.75, "sample_count": 3, "duration_seconds": 0.255}, "timestamp": "2026-01-25T19:58:20.040584"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 227.317, "latencies_ms": [227.317], "images_per_second": 4.399, "prompt_tokens": 759, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\n 1. Truck in the background 2. Dog near red wheel 3. Red and black bike 4. White car behind truck 5. White car parked next to white car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.2, "ram_available_mb": 112057.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10450.9, "ram_available_mb": 112055.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [32.9, 32.9, 32.9], "power_watts_avg": 32.9, "power_watts_peak": 32.9, "energy_joules_est": 7.49, "sample_count": 3, "duration_seconds": 0.228}, "timestamp": "2026-01-25T19:58:20.347761"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 171.551, "latencies_ms": [171.551], "images_per_second": 5.829, "prompt_tokens": 763, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n 1. A red and black dirt bike is parked in front of a garage with a white door.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 10450.9, "ram_available_mb": 112055.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [32.9, 32.9], "power_watts_avg": 32.9, "power_watts_peak": 32.9, "energy_joules_est": 5.65, "sample_count": 2, "duration_seconds": 0.172}, "timestamp": "2026-01-25T19:58:20.554726"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 229.455, "latencies_ms": [229.455], "images_per_second": 4.358, "prompt_tokens": 757, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nIn a grassy area, there is an old motorcycle with a red frame parked next to a garage. The motorcycle has a dog sitting on its seat, looking at the camera.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10450.9, "ram_available_mb": 112055.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [35.27, 35.27, 35.27], "power_watts_avg": 35.27, "power_watts_peak": 35.27, "energy_joules_est": 8.11, "sample_count": 3, "duration_seconds": 0.23}, "timestamp": "2026-01-25T19:58:20.861462"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 269.397, "latencies_ms": [269.397], "images_per_second": 3.712, "prompt_tokens": 756, "response_tokens_est": 45, "n_tiles": 1, "output_text": "\n The image features a red and black dirt bike with a white front wheel. A small dog is sitting on the ground next to the bike. In the background, there are two cars parked in a garage or driveway.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.9, "ram_available_mb": 112055.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 2.0}, "power_stats": {"power_watts_samples": [35.27, 35.27, 29.61], "power_watts_avg": 33.38, "power_watts_peak": 35.27, "energy_joules_est": 9.01, "sample_count": 3, "duration_seconds": 0.27}, "timestamp": "2026-01-25T19:58:21.171943"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 263.577, "latencies_ms": [263.577], "images_per_second": 3.794, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA man stands on a sandy beach, holding onto a kite that is flying high in the sky.", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10448.0, "ram_available_mb": 112058.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 2.0}, "power_stats": {"power_watts_samples": [29.61, 29.61, 29.61], "power_watts_avg": 29.61, "power_watts_peak": 29.61, "energy_joules_est": 7.83, "sample_count": 3, "duration_seconds": 0.264}, "timestamp": "2026-01-25T19:58:21.487154"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 75.807, "latencies_ms": [75.807], "images_per_second": 13.191, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Kite - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.0, "ram_available_mb": 112058.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10451.7, "ram_available_mb": 112054.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 2.0}, "power_stats": {"power_watts_samples": [29.61], "power_watts_avg": 29.61, "power_watts_peak": 29.61, "energy_joules_est": 2.25, "sample_count": 1, "duration_seconds": 0.076}, "timestamp": "2026-01-25T19:58:21.593280"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 119.93, "latencies_ms": [119.93], "images_per_second": 8.338, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. A man flying a kite on the beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.7, "ram_available_mb": 112054.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10446.0, "ram_available_mb": 112060.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 2.0}, "power_stats": {"power_watts_samples": [35.28, 35.28], "power_watts_avg": 35.28, "power_watts_peak": 35.28, "energy_joules_est": 4.24, "sample_count": 2, "duration_seconds": 0.12}, "timestamp": "2026-01-25T19:58:21.799480"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 304.521, "latencies_ms": [304.521], "images_per_second": 3.284, "prompt_tokens": 757, "response_tokens_est": 48, "n_tiles": 1, "output_text": "\nA man in a black shirt stands on a sandy beach, holding onto a kite that is flying high above him. The sky is clear blue with fluffy white clouds, creating an ideal day for outdoor activities like kite flying.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.0, "ram_available_mb": 112060.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10448.4, "ram_available_mb": 112057.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [35.28, 35.28, 33.74, 33.74], "power_watts_avg": 34.51, "power_watts_peak": 35.28, "energy_joules_est": 10.53, "sample_count": 4, "duration_seconds": 0.305}, "timestamp": "2026-01-25T19:58:22.206958"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 93.608, "latencies_ms": [93.608], "images_per_second": 10.683, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The sky is blue and clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.4, "ram_available_mb": 112057.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10450.2, "ram_available_mb": 112056.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [33.74], "power_watts_avg": 33.74, "power_watts_peak": 33.74, "energy_joules_est": 3.18, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T19:58:22.312796"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 263.317, "latencies_ms": [263.317], "images_per_second": 3.798, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA kitchen counter is cluttered with various items, including a green bottle of dish soap and a red cloth.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10450.2, "ram_available_mb": 112056.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10441.3, "ram_available_mb": 112065.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [33.74, 33.74, 31.28], "power_watts_avg": 32.92, "power_watts_peak": 33.74, "energy_joules_est": 8.68, "sample_count": 3, "duration_seconds": 0.264}, "timestamp": "2026-01-25T19:58:22.626857"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 98.541, "latencies_ms": [98.541], "images_per_second": 10.148, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Green dish soap bottle (1)", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10441.3, "ram_available_mb": 112065.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10440.7, "ram_available_mb": 112065.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [31.28], "power_watts_avg": 31.28, "power_watts_peak": 31.28, "energy_joules_est": 3.09, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T19:58:22.732334"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 91.802, "latencies_ms": [91.802], "images_per_second": 10.893, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Green bottle on countertop.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 10440.7, "ram_available_mb": 112065.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10441.0, "ram_available_mb": 112065.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [31.28], "power_watts_avg": 31.28, "power_watts_peak": 31.28, "energy_joules_est": 2.88, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T19:58:22.836217"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 461.162, "latencies_ms": [461.162], "images_per_second": 2.168, "prompt_tokens": 757, "response_tokens_est": 81, "n_tiles": 1, "output_text": "\nThe image shows a kitchen with wooden cabinets, a black countertop, and various items on display. There are two sinks in the kitchen - one under the refrigerator and another next to the stove. A green bottle of dish soap sits on the countertop near the sink area. The kitchen also features a microwave above the stove, an oven below it, and several bottles placed around the countertops.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10441.0, "ram_available_mb": 112065.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10445.4, "ram_available_mb": 112060.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [31.28, 31.28, 33.35, 33.35, 33.35], "power_watts_avg": 32.53, "power_watts_peak": 33.35, "energy_joules_est": 15.01, "sample_count": 5, "duration_seconds": 0.461}, "timestamp": "2026-01-25T19:58:23.342152"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 110.394, "latencies_ms": [110.394], "images_per_second": 9.058, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The kitchen is dark and has a lot of clutter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.4, "ram_available_mb": 112060.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10446.2, "ram_available_mb": 112060.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [33.35, 33.35], "power_watts_avg": 33.35, "power_watts_peak": 33.35, "energy_joules_est": 3.7, "sample_count": 2, "duration_seconds": 0.111}, "timestamp": "2026-01-25T19:58:23.550088"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 303.754, "latencies_ms": [303.754], "images_per_second": 3.292, "prompt_tokens": 744, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA red and white kite with a black bird design soars high in the clear blue sky, held aloft by two strings attached to it.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 10446.2, "ram_available_mb": 112060.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10443.7, "ram_available_mb": 112062.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [33.39, 33.39, 33.39, 33.39], "power_watts_avg": 33.39, "power_watts_peak": 33.39, "energy_joules_est": 10.15, "sample_count": 4, "duration_seconds": 0.304}, "timestamp": "2026-01-25T19:58:23.960504"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 63.059, "latencies_ms": [63.059], "images_per_second": 15.858, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Sky", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10443.7, "ram_available_mb": 112062.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10446.2, "ram_available_mb": 112060.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 45.0}, "power_stats": {"power_watts_samples": [33.39], "power_watts_avg": 33.39, "power_watts_peak": 33.39, "energy_joules_est": 2.13, "sample_count": 1, "duration_seconds": 0.064}, "timestamp": "2026-01-25T19:58:24.068121"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 105.487, "latencies_ms": [105.487], "images_per_second": 9.48, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nThe kite is flying in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.2, "ram_available_mb": 112060.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10444.2, "ram_available_mb": 112062.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 45.0}, "power_stats": {"power_watts_samples": [34.4, 34.4], "power_watts_avg": 34.4, "power_watts_peak": 34.4, "energy_joules_est": 3.65, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T19:58:24.275560"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 277.247, "latencies_ms": [277.247], "images_per_second": 3.607, "prompt_tokens": 757, "response_tokens_est": 46, "n_tiles": 1, "output_text": "\nIn the image, a red and white kite with black designs soars high in the sky against a clear blue backdrop. The kite appears to be flying at an angle, creating a dynamic and visually striking scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10444.2, "ram_available_mb": 112062.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10448.1, "ram_available_mb": 112058.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 45.0}, "power_stats": {"power_watts_samples": [34.4, 34.4, 34.4], "power_watts_avg": 34.4, "power_watts_peak": 34.4, "energy_joules_est": 9.56, "sample_count": 3, "duration_seconds": 0.278}, "timestamp": "2026-01-25T19:58:24.582861"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 114.486, "latencies_ms": [114.486], "images_per_second": 8.735, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The kite is red and white with black designs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.1, "ram_available_mb": 112058.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 45.0}, "power_stats": {"power_watts_samples": [30.72, 30.72], "power_watts_avg": 30.72, "power_watts_peak": 30.72, "energy_joules_est": 3.53, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T19:58:24.789861"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 460.034, "latencies_ms": [460.034], "images_per_second": 2.174, "prompt_tokens": 744, "response_tokens_est": 62, "n_tiles": 1, "output_text": "\nThe image shows a hotel room with two beds, each covered in white linens and black pillows. The room is illuminated by natural light from an open window on the right side of the room. A nightstand sits between the beds, holding a lamp that casts a warm glow throughout the space.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10452.2, "ram_available_mb": 112054.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [30.72, 30.72, 31.28, 31.28, 31.28], "power_watts_avg": 31.06, "power_watts_peak": 31.28, "energy_joules_est": 14.3, "sample_count": 5, "duration_seconds": 0.461}, "timestamp": "2026-01-25T19:58:25.300566"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 89.403, "latencies_ms": [89.403], "images_per_second": 11.185, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Bed in room 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.2, "ram_available_mb": 112054.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10450.5, "ram_available_mb": 112055.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [31.28], "power_watts_avg": 31.28, "power_watts_peak": 31.28, "energy_joules_est": 2.81, "sample_count": 1, "duration_seconds": 0.09}, "timestamp": "2026-01-25T19:58:25.405433"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 118.865, "latencies_ms": [118.865], "images_per_second": 8.413, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Bed with black and white pillowcases on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.5, "ram_available_mb": 112055.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10450.5, "ram_available_mb": 112055.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [31.28, 34.26], "power_watts_avg": 32.77, "power_watts_peak": 34.26, "energy_joules_est": 3.9, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T19:58:25.610480"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 404.101, "latencies_ms": [404.101], "images_per_second": 2.475, "prompt_tokens": 757, "response_tokens_est": 69, "n_tiles": 1, "output_text": "\nThe image shows a hotel room with two beds, one of which has a black and white comforter. The room features a window that offers a view of trees outside, creating a serene atmosphere. A door can be seen on the right side of the room, suggesting an exit or entrance to another area within the hotel.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.5, "ram_available_mb": 112055.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10450.5, "ram_available_mb": 112055.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.26, 34.26, 34.26, 34.26, 33.83], "power_watts_avg": 34.17, "power_watts_peak": 34.26, "energy_joules_est": 13.84, "sample_count": 5, "duration_seconds": 0.405}, "timestamp": "2026-01-25T19:58:26.115978"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 114.413, "latencies_ms": [114.413], "images_per_second": 8.74, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The room is painted green and has a wooden floor.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10450.5, "ram_available_mb": 112055.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10451.0, "ram_available_mb": 112055.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.83, 33.83], "power_watts_avg": 33.83, "power_watts_peak": 33.83, "energy_joules_est": 3.88, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T19:58:26.320611"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 248.985, "latencies_ms": [248.985], "images_per_second": 4.016, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA person on a white and green motorcycle is racing down a road, with spectators watching from behind a fence.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10451.0, "ram_available_mb": 112055.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10450.5, "ram_available_mb": 112055.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.83, 33.83, 32.82], "power_watts_avg": 33.49, "power_watts_peak": 33.83, "energy_joules_est": 8.36, "sample_count": 3, "duration_seconds": 0.25}, "timestamp": "2026-01-25T19:58:26.629545"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 301.869, "latencies_ms": [301.869], "images_per_second": 3.313, "prompt_tokens": 759, "response_tokens_est": 49, "n_tiles": 1, "output_text": "\n 1. Person in background 2. Person in background 3. Person in background 4. Person in background 5. Person in background 6. Person in background 7. Person in background 8. Person in background 9. Person in background 10.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 10450.5, "ram_available_mb": 112055.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10456.5, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.82, 32.82, 32.82], "power_watts_avg": 32.82, "power_watts_peak": 32.82, "energy_joules_est": 9.91, "sample_count": 3, "duration_seconds": 0.302}, "timestamp": "2026-01-25T19:58:26.934696"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 112.827, "latencies_ms": [112.827], "images_per_second": 8.863, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Person on motorcycle in front of fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.4, "ram_available_mb": 112051.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10454.0, "ram_available_mb": 112052.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.82, 32.65], "power_watts_avg": 32.73, "power_watts_peak": 32.82, "energy_joules_est": 3.7, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T19:58:27.139996"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 159.798, "latencies_ms": [159.798], "images_per_second": 6.258, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA person on a white motorcycle is racing down a road, with spectators watching from behind a fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.0, "ram_available_mb": 112052.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10449.5, "ram_available_mb": 112056.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.65, 32.65], "power_watts_avg": 32.65, "power_watts_peak": 32.65, "energy_joules_est": 5.23, "sample_count": 2, "duration_seconds": 0.16}, "timestamp": "2026-01-25T19:58:27.346115"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 93.467, "latencies_ms": [93.467], "images_per_second": 10.699, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The motorcycle is white and green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.5, "ram_available_mb": 112056.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10451.3, "ram_available_mb": 112055.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.65], "power_watts_avg": 32.65, "power_watts_peak": 32.65, "energy_joules_est": 3.07, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T19:58:27.450690"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 243.503, "latencies_ms": [243.503], "images_per_second": 4.107, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urn of flowers sits on a table, accompanied by wine glasses and silverware for an elegant dinner setting.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 10451.3, "ram_available_mb": 112055.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10457.2, "ram_available_mb": 112049.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.65, 29.72, 29.72], "power_watts_avg": 30.69, "power_watts_peak": 32.65, "energy_joules_est": 7.48, "sample_count": 3, "duration_seconds": 0.244}, "timestamp": "2026-01-25T19:58:27.758892"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 557.123, "latencies_ms": [557.123], "images_per_second": 1.795, "prompt_tokens": 759, "response_tokens_est": 104, "n_tiles": 1, "output_text": "\n Glasses 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.2, "ram_available_mb": 112049.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10451.6, "ram_available_mb": 112054.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [29.72, 29.72, 29.72, 33.87, 33.87, 33.87], "power_watts_avg": 31.79, "power_watts_peak": 33.87, "energy_joules_est": 17.73, "sample_count": 6, "duration_seconds": 0.558}, "timestamp": "2026-01-25T19:58:28.366148"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 89.779, "latencies_ms": [89.779], "images_per_second": 11.138, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n A vase with flowers on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.6, "ram_available_mb": 112054.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.87], "power_watts_avg": 33.87, "power_watts_peak": 33.87, "energy_joules_est": 3.06, "sample_count": 1, "duration_seconds": 0.09}, "timestamp": "2026-01-25T19:58:28.470292"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 319.667, "latencies_ms": [319.667], "images_per_second": 3.128, "prompt_tokens": 757, "response_tokens_est": 54, "n_tiles": 1, "output_text": "\nThe image shows a table set for two with white flowers in a glass vase, silverware including forks, knives, and spoons, and wine glasses. The table is surrounded by chairs, suggesting that this is an intimate dining experience or a romantic setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10452.3, "ram_available_mb": 112054.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.87, 39.2, 39.2, 39.2], "power_watts_avg": 37.86, "power_watts_peak": 39.2, "energy_joules_est": 12.11, "sample_count": 4, "duration_seconds": 0.32}, "timestamp": "2026-01-25T19:58:28.876291"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 120.765, "latencies_ms": [120.765], "images_per_second": 8.281, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "urn of flowers on a table with wine glasses and silverware.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.3, "ram_available_mb": 112054.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10452.3, "ram_available_mb": 112054.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [39.2, 39.2], "power_watts_avg": 39.2, "power_watts_peak": 39.2, "energy_joules_est": 4.75, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T19:58:29.081713"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 269.38, "latencies_ms": [269.38], "images_per_second": 3.712, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urn of a clock with two faces, one showing the time as 12:15 and the other as 12:16.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10452.3, "ram_available_mb": 112054.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10448.2, "ram_available_mb": 112058.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [34.57, 34.57, 34.57], "power_watts_avg": 34.57, "power_watts_peak": 34.57, "energy_joules_est": 9.32, "sample_count": 3, "duration_seconds": 0.27}, "timestamp": "2026-01-25T19:58:29.390876"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 70.6, "latencies_ms": [70.6], "images_per_second": 14.164, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Clock", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.2, "ram_available_mb": 112058.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10448.4, "ram_available_mb": 112057.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [34.57], "power_watts_avg": 34.57, "power_watts_peak": 34.57, "energy_joules_est": 2.46, "sample_count": 1, "duration_seconds": 0.071}, "timestamp": "2026-01-25T19:58:29.497347"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 153.795, "latencies_ms": [153.795], "images_per_second": 6.502, "prompt_tokens": 763, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n 1. A clock on a pole in front of a field with tall grasses and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.4, "ram_available_mb": 112057.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10444.9, "ram_available_mb": 112061.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [32.85, 32.85], "power_watts_avg": 32.85, "power_watts_peak": 32.85, "energy_joules_est": 5.06, "sample_count": 2, "duration_seconds": 0.154}, "timestamp": "2026-01-25T19:58:29.702968"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 48.525, "latencies_ms": [48.525], "images_per_second": 20.608, "prompt_tokens": 757, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10444.9, "ram_available_mb": 112061.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 10440.1, "ram_available_mb": 112066.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [32.85], "power_watts_avg": 32.85, "power_watts_peak": 32.85, "energy_joules_est": 1.6, "sample_count": 1, "duration_seconds": 0.049}, "timestamp": "2026-01-25T19:58:29.807680"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 88.023, "latencies_ms": [88.023], "images_per_second": 11.361, "prompt_tokens": 756, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. The sky is white and cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10440.3, "ram_available_mb": 112066.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10444.0, "ram_available_mb": 112062.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [32.85], "power_watts_avg": 32.85, "power_watts_peak": 32.85, "energy_joules_est": 2.9, "sample_count": 1, "duration_seconds": 0.088}, "timestamp": "2026-01-25T19:58:29.913679"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 302.639, "latencies_ms": [302.639], "images_per_second": 3.304, "prompt_tokens": 744, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA young man in a black shirt and baseball cap is skateboarding on a concrete surface, with his arms outstretched for balance as he performs a trick.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 10444.0, "ram_available_mb": 112062.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10447.7, "ram_available_mb": 112058.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [32.85, 30.59, 30.59, 30.59], "power_watts_avg": 31.16, "power_watts_peak": 32.85, "energy_joules_est": 9.44, "sample_count": 4, "duration_seconds": 0.303}, "timestamp": "2026-01-25T19:58:30.324629"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.352, "latencies_ms": [76.352], "images_per_second": 13.097, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Trees", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.7, "ram_available_mb": 112058.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10451.3, "ram_available_mb": 112055.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [30.59], "power_watts_avg": 30.59, "power_watts_peak": 30.59, "energy_joules_est": 2.35, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T19:58:30.429921"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 151.933, "latencies_ms": [151.933], "images_per_second": 6.582, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n 1. Skateboarder in black shirt and hat on a skateboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.3, "ram_available_mb": 112055.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10451.3, "ram_available_mb": 112055.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [30.59, 29.6], "power_watts_avg": 30.09, "power_watts_peak": 30.59, "energy_joules_est": 4.58, "sample_count": 2, "duration_seconds": 0.152}, "timestamp": "2026-01-25T19:58:30.635209"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 179.064, "latencies_ms": [179.064], "images_per_second": 5.585, "prompt_tokens": 757, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA young man in a black shirt and baseball cap is skateboarding on a concrete surface, performing a trick.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.3, "ram_available_mb": 112055.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10452.4, "ram_available_mb": 112053.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [29.6, 29.6], "power_watts_avg": 29.6, "power_watts_peak": 29.6, "energy_joules_est": 5.32, "sample_count": 2, "duration_seconds": 0.18}, "timestamp": "2026-01-25T19:58:30.840492"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 115.229, "latencies_ms": [115.229], "images_per_second": 8.678, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The skateboarder is wearing a black shirt and hat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.4, "ram_available_mb": 112053.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10447.0, "ram_available_mb": 112059.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [29.6, 29.6], "power_watts_avg": 29.6, "power_watts_peak": 29.6, "energy_joules_est": 3.42, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T19:58:31.045321"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 278.801, "latencies_ms": [278.801], "images_per_second": 3.587, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA white plate holds a pile of orange carrots and green beans, with a blue peeler nearby on a kitchen countertop.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 10447.3, "ram_available_mb": 112059.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10444.6, "ram_available_mb": 112061.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [30.92, 30.92, 30.92], "power_watts_avg": 30.92, "power_watts_peak": 30.92, "energy_joules_est": 8.64, "sample_count": 3, "duration_seconds": 0.279}, "timestamp": "2026-01-25T19:58:31.354331"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 87.511, "latencies_ms": [87.511], "images_per_second": 11.427, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Carrots: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10444.6, "ram_available_mb": 112061.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10445.3, "ram_available_mb": 112061.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [30.92], "power_watts_avg": 30.92, "power_watts_peak": 30.92, "energy_joules_est": 2.72, "sample_count": 1, "duration_seconds": 0.088}, "timestamp": "2026-01-25T19:58:31.458399"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 126.468, "latencies_ms": [126.468], "images_per_second": 7.907, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A plate of carrots and a blue peeler are on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.3, "ram_available_mb": 112061.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10447.3, "ram_available_mb": 112059.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [30.92, 32.02], "power_watts_avg": 31.47, "power_watts_peak": 32.02, "energy_joules_est": 3.99, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T19:58:31.662327"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 268.426, "latencies_ms": [268.426], "images_per_second": 3.725, "prompt_tokens": 757, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\n A white plate with a bunch of carrots on top sits on a kitchen counter. Next to the plate, there are some purple radishes and green beans. A blue peeler can also be seen nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.6, "ram_available_mb": 112058.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10452.9, "ram_available_mb": 112053.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [32.02, 32.02, 32.02], "power_watts_avg": 32.02, "power_watts_peak": 32.02, "energy_joules_est": 8.6, "sample_count": 3, "duration_seconds": 0.269}, "timestamp": "2026-01-25T19:58:31.967796"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 119.768, "latencies_ms": [119.768], "images_per_second": 8.349, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n A plate of carrots and beets on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.9, "ram_available_mb": 112053.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10453.4, "ram_available_mb": 112052.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.02, 32.41], "power_watts_avg": 32.21, "power_watts_peak": 32.41, "energy_joules_est": 3.87, "sample_count": 2, "duration_seconds": 0.12}, "timestamp": "2026-01-25T19:58:32.173121"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 235.086, "latencies_ms": [235.086], "images_per_second": 4.254, "prompt_tokens": 744, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n A man is giving a presentation on stage, with two women watching from behind a screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.4, "ram_available_mb": 112052.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10456.6, "ram_available_mb": 112049.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.41, 32.41, 32.41], "power_watts_avg": 32.41, "power_watts_peak": 32.41, "energy_joules_est": 7.65, "sample_count": 3, "duration_seconds": 0.236}, "timestamp": "2026-01-25T19:58:32.483350"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 137.265, "latencies_ms": [137.265], "images_per_second": 7.285, "prompt_tokens": 759, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n Speaker 1: 0.42, 0.13, 0.63, 0.39", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.6, "ram_available_mb": 112049.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10456.0, "ram_available_mb": 112050.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.41, 32.44], "power_watts_avg": 32.42, "power_watts_peak": 32.44, "energy_joules_est": 4.46, "sample_count": 2, "duration_seconds": 0.138}, "timestamp": "2026-01-25T19:58:32.688965"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 54.577, "latencies_ms": [54.577], "images_per_second": 18.323, "prompt_tokens": 763, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.0, "ram_available_mb": 112050.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.44], "power_watts_avg": 32.44, "power_watts_peak": 32.44, "energy_joules_est": 1.79, "sample_count": 1, "duration_seconds": 0.055}, "timestamp": "2026-01-25T19:58:32.794626"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 154.039, "latencies_ms": [154.039], "images_per_second": 6.492, "prompt_tokens": 757, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nTwo people are watching a presentation on a large screen, with one person wearing a suit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10459.0, "ram_available_mb": 112047.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.44, 32.44], "power_watts_avg": 32.44, "power_watts_peak": 32.44, "energy_joules_est": 5.01, "sample_count": 2, "duration_seconds": 0.154}, "timestamp": "2026-01-25T19:58:33.000091"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 52.081, "latencies_ms": [52.081], "images_per_second": 19.201, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.0, "ram_available_mb": 112047.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [29.8], "power_watts_avg": 29.8, "power_watts_peak": 29.8, "energy_joules_est": 1.57, "sample_count": 1, "duration_seconds": 0.053}, "timestamp": "2026-01-25T19:58:33.106300"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 267.711, "latencies_ms": [267.711], "images_per_second": 3.735, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nTwo men sit on a sidewalk, with one holding a broom and dustpan while the other holds a camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [29.8, 29.8, 29.8], "power_watts_avg": 29.8, "power_watts_peak": 29.8, "energy_joules_est": 7.99, "sample_count": 3, "duration_seconds": 0.268}, "timestamp": "2026-01-25T19:58:33.417821"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 90.449, "latencies_ms": [90.449], "images_per_second": 11.056, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Motorcycle: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [29.8], "power_watts_avg": 29.8, "power_watts_peak": 29.8, "energy_joules_est": 2.71, "sample_count": 1, "duration_seconds": 0.091}, "timestamp": "2026-01-25T19:58:33.522901"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 129.916, "latencies_ms": [129.916], "images_per_second": 7.697, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nTwo people are sitting on a sidewalk next to parked motorcycles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10458.5, "ram_available_mb": 112047.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [29.31, 29.31], "power_watts_avg": 29.31, "power_watts_peak": 29.31, "energy_joules_est": 3.81, "sample_count": 2, "duration_seconds": 0.13}, "timestamp": "2026-01-25T19:58:33.728399"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 241.038, "latencies_ms": [241.038], "images_per_second": 4.149, "prompt_tokens": 757, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\nTwo men are sitting on a sidewalk, one of them holding a broom. Behind them, there are parked motorcycles and a building with a sign that says \"\u5927\u962a\u5e02\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.5, "ram_available_mb": 112047.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [29.31, 29.31, 29.31], "power_watts_avg": 29.31, "power_watts_peak": 29.31, "energy_joules_est": 7.08, "sample_count": 3, "duration_seconds": 0.241}, "timestamp": "2026-01-25T19:58:34.034297"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 346.267, "latencies_ms": [346.267], "images_per_second": 2.888, "prompt_tokens": 756, "response_tokens_est": 59, "n_tiles": 1, "output_text": "\n A man and a woman are sitting on the sidewalk next to parked motorcycles. The man is wearing a blue shirt with white stripes and has a cane in his hand. The woman is wearing a blue hat and holding a broom. They appear to be engaged in conversation or observing something nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10454.7, "ram_available_mb": 112051.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.45, 31.45, 31.45, 31.45], "power_watts_avg": 31.45, "power_watts_peak": 31.45, "energy_joules_est": 10.9, "sample_count": 4, "duration_seconds": 0.347}, "timestamp": "2026-01-25T19:58:34.440947"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 251.002, "latencies_ms": [251.002], "images_per_second": 3.984, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA white plate holds a serving of chicken and broccoli, with the broccoli pieces scattered around the plate.", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 10454.7, "ram_available_mb": 112051.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10447.9, "ram_available_mb": 112058.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.45, 30.8, 30.8], "power_watts_avg": 31.01, "power_watts_peak": 31.45, "energy_joules_est": 7.82, "sample_count": 3, "duration_seconds": 0.252}, "timestamp": "2026-01-25T19:58:34.757271"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 84.255, "latencies_ms": [84.255], "images_per_second": 11.869, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Plate - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.9, "ram_available_mb": 112058.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10448.3, "ram_available_mb": 112058.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.8], "power_watts_avg": 30.8, "power_watts_peak": 30.8, "energy_joules_est": 2.61, "sample_count": 1, "duration_seconds": 0.085}, "timestamp": "2026-01-25T19:58:34.863260"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 105.65, "latencies_ms": [105.65], "images_per_second": 9.465, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n A white plate with food on it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10448.3, "ram_available_mb": 112058.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10448.1, "ram_available_mb": 112058.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [30.8, 30.8], "power_watts_avg": 30.8, "power_watts_peak": 30.8, "energy_joules_est": 3.26, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T19:58:35.070219"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 110.2, "latencies_ms": [110.2], "images_per_second": 9.074, "prompt_tokens": 757, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nA white plate holds a serving of chicken and broccoli.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.1, "ram_available_mb": 112058.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10453.6, "ram_available_mb": 112052.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [33.01, 33.01], "power_watts_avg": 33.01, "power_watts_peak": 33.01, "energy_joules_est": 3.66, "sample_count": 2, "duration_seconds": 0.111}, "timestamp": "2026-01-25T19:58:35.276761"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 86.551, "latencies_ms": [86.551], "images_per_second": 11.554, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The plate is white and black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.6, "ram_available_mb": 112052.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10453.8, "ram_available_mb": 112052.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [33.01], "power_watts_avg": 33.01, "power_watts_peak": 33.01, "energy_joules_est": 2.87, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T19:58:35.381620"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 225.994, "latencies_ms": [225.994], "images_per_second": 4.425, "prompt_tokens": 744, "response_tokens_est": 18, "n_tiles": 1, "output_text": "urn of water is on a table, and there are two windows in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.8, "ram_available_mb": 112052.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10455.4, "ram_available_mb": 112050.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [33.01, 33.01, 31.33], "power_watts_avg": 32.45, "power_watts_peak": 33.01, "energy_joules_est": 7.34, "sample_count": 3, "duration_seconds": 0.226}, "timestamp": "2026-01-25T19:58:35.690856"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.281, "latencies_ms": [79.281], "images_per_second": 12.613, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Hat", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.4, "ram_available_mb": 112050.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10453.7, "ram_available_mb": 112052.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [31.33], "power_watts_avg": 31.33, "power_watts_peak": 31.33, "energy_joules_est": 2.49, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T19:58:35.796333"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 114.176, "latencies_ms": [114.176], "images_per_second": 8.758, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe man is wearing a blue shirt and red tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.7, "ram_available_mb": 112052.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [31.33, 31.33], "power_watts_avg": 31.33, "power_watts_peak": 31.33, "energy_joules_est": 3.59, "sample_count": 2, "duration_seconds": 0.114}, "timestamp": "2026-01-25T19:58:36.001647"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 182.247, "latencies_ms": [182.247], "images_per_second": 5.487, "prompt_tokens": 757, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA man wearing a blue shirt, red tie, and black hat stands in front of a building with large windows.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 10452.6, "ram_available_mb": 112053.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10446.2, "ram_available_mb": 112060.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [27.44, 27.44], "power_watts_avg": 27.44, "power_watts_peak": 27.44, "energy_joules_est": 5.02, "sample_count": 2, "duration_seconds": 0.183}, "timestamp": "2026-01-25T19:58:36.207289"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 101.26, "latencies_ms": [101.26], "images_per_second": 9.876, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The man is wearing a blue shirt and red tie.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10446.2, "ram_available_mb": 112060.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10454.8, "ram_available_mb": 112051.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [27.44, 27.44], "power_watts_avg": 27.44, "power_watts_peak": 27.44, "energy_joules_est": 2.78, "sample_count": 2, "duration_seconds": 0.101}, "timestamp": "2026-01-25T19:58:36.412088"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 273.196, "latencies_ms": [273.196], "images_per_second": 3.66, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA collage of six images, each showcasing a different slice of pizza on a white plate with a fork and knife nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.8, "ram_available_mb": 112051.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10450.6, "ram_available_mb": 112055.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [27.44, 27.96, 27.96], "power_watts_avg": 27.79, "power_watts_peak": 27.96, "energy_joules_est": 7.61, "sample_count": 3, "duration_seconds": 0.274}, "timestamp": "2026-01-25T19:58:36.724326"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 157.614, "latencies_ms": [157.614], "images_per_second": 6.345, "prompt_tokens": 759, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n [0.12, 0.13, 0.36, 0.32]", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.8, "ram_available_mb": 112055.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10450.5, "ram_available_mb": 112055.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [27.96, 27.96], "power_watts_avg": 27.96, "power_watts_peak": 27.96, "energy_joules_est": 4.42, "sample_count": 2, "duration_seconds": 0.158}, "timestamp": "2026-01-25T19:58:36.931279"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 166.984, "latencies_ms": [166.984], "images_per_second": 5.989, "prompt_tokens": 763, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n 1. Top left corner of image - A slice of pizza on a plate with sauce and cheese.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.5, "ram_available_mb": 112055.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10449.6, "ram_available_mb": 112056.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [27.96, 31.5], "power_watts_avg": 29.73, "power_watts_peak": 31.5, "energy_joules_est": 4.97, "sample_count": 2, "duration_seconds": 0.167}, "timestamp": "2026-01-25T19:58:37.135732"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 302.361, "latencies_ms": [302.361], "images_per_second": 3.307, "prompt_tokens": 757, "response_tokens_est": 48, "n_tiles": 1, "output_text": "\nA collage of six images featuring different types of pizza on plates, with a focus on cheese pizzas. The images show slices of various pizzas arranged in two rows of three, showcasing their appetizing toppings and textures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.6, "ram_available_mb": 112056.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [31.5, 31.5, 31.5, 31.5], "power_watts_avg": 31.5, "power_watts_peak": 31.5, "energy_joules_est": 9.54, "sample_count": 4, "duration_seconds": 0.303}, "timestamp": "2026-01-25T19:58:37.541407"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 120.288, "latencies_ms": [120.288], "images_per_second": 8.313, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. A white plate with a pizza on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10449.2, "ram_available_mb": 112057.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [35.01, 35.01], "power_watts_avg": 35.01, "power_watts_peak": 35.01, "energy_joules_est": 4.22, "sample_count": 2, "duration_seconds": 0.12}, "timestamp": "2026-01-25T19:58:37.747377"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 273.254, "latencies_ms": [273.254], "images_per_second": 3.66, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nTwo young girls in dresses interact with a black and white goat, petting its head while smiling at each other.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 10449.2, "ram_available_mb": 112057.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10445.6, "ram_available_mb": 112060.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.01, 35.01, 35.01], "power_watts_avg": 35.01, "power_watts_peak": 35.01, "energy_joules_est": 9.59, "sample_count": 3, "duration_seconds": 0.274}, "timestamp": "2026-01-25T19:58:38.057630"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 120.199, "latencies_ms": [120.199], "images_per_second": 8.32, "prompt_tokens": 759, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Girl in pink dress touching goat's head 2.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.6, "ram_available_mb": 112060.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10446.8, "ram_available_mb": 112059.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [33.29, 33.29], "power_watts_avg": 33.29, "power_watts_peak": 33.29, "energy_joules_est": 4.01, "sample_count": 2, "duration_seconds": 0.12}, "timestamp": "2026-01-25T19:58:38.261546"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 81.671, "latencies_ms": [81.671], "images_per_second": 12.244, "prompt_tokens": 763, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\nTwo girls petting a goat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.8, "ram_available_mb": 112059.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10446.8, "ram_available_mb": 112059.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [33.29], "power_watts_avg": 33.29, "power_watts_peak": 33.29, "energy_joules_est": 2.75, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T19:58:38.366557"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 189.048, "latencies_ms": [189.048], "images_per_second": 5.29, "prompt_tokens": 757, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\n Two young girls in dresses pet a black goat, while another girl stands nearby. The goats are standing on grass behind a fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.8, "ram_available_mb": 112059.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [33.29, 33.29], "power_watts_avg": 33.29, "power_watts_peak": 33.29, "energy_joules_est": 6.32, "sample_count": 2, "duration_seconds": 0.19}, "timestamp": "2026-01-25T19:58:38.572167"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 366.876, "latencies_ms": [366.876], "images_per_second": 2.726, "prompt_tokens": 756, "response_tokens_est": 62, "n_tiles": 1, "output_text": "\n The image shows two young girls petting a black and white goat. One girl is wearing a pink dress with a flowery pattern, while the other girl has on a blue dress. They are standing in front of a fence that separates them from another person who can be seen in the background.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10444.5, "ram_available_mb": 112061.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [30.02, 30.02, 30.02, 30.02], "power_watts_avg": 30.02, "power_watts_peak": 30.02, "energy_joules_est": 11.02, "sample_count": 4, "duration_seconds": 0.367}, "timestamp": "2026-01-25T19:58:38.976888"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 368.06, "latencies_ms": [368.06], "images_per_second": 2.717, "prompt_tokens": 744, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nA street lamp with a green sign on top stands in an empty intersection, casting a warm glow onto the road below. The sky is painted in shades of blue and orange as the sun sets behind distant mountains.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10444.5, "ram_available_mb": 112061.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10444.5, "ram_available_mb": 112061.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.02, 31.47, 31.47, 31.47], "power_watts_avg": 31.11, "power_watts_peak": 31.47, "energy_joules_est": 11.47, "sample_count": 4, "duration_seconds": 0.369}, "timestamp": "2026-01-25T19:58:39.386694"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 249.491, "latencies_ms": [249.491], "images_per_second": 4.008, "prompt_tokens": 759, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\n 1. Traffic light  2. Street sign  3. Street lamp  4. Street lamp  5. Street lamp  6. Street lamp  7. Street lamp  8. Street lamp", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10444.5, "ram_available_mb": 112061.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10442.3, "ram_available_mb": 112064.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.47, 31.47, 38.4], "power_watts_avg": 33.78, "power_watts_peak": 38.4, "energy_joules_est": 8.44, "sample_count": 3, "duration_seconds": 0.25}, "timestamp": "2026-01-25T19:58:39.692014"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 119.142, "latencies_ms": [119.142], "images_per_second": 8.393, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nA street light is on in front of a stop sign.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10442.3, "ram_available_mb": 112064.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10442.3, "ram_available_mb": 112064.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [38.4, 38.4], "power_watts_avg": 38.4, "power_watts_peak": 38.4, "energy_joules_est": 4.59, "sample_count": 2, "duration_seconds": 0.12}, "timestamp": "2026-01-25T19:58:39.898048"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 144.072, "latencies_ms": [144.072], "images_per_second": 6.941, "prompt_tokens": 757, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nA street light with a green signal on top of an intersection at night.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10442.3, "ram_available_mb": 112064.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10442.5, "ram_available_mb": 112063.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [38.4, 38.4], "power_watts_avg": 38.4, "power_watts_peak": 38.4, "energy_joules_est": 5.55, "sample_count": 2, "duration_seconds": 0.144}, "timestamp": "2026-01-25T19:58:40.103090"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 127.868, "latencies_ms": [127.868], "images_per_second": 7.821, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A street light with a green signal is lit up at night.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10442.5, "ram_available_mb": 112063.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10442.5, "ram_available_mb": 112063.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [34.56, 34.56], "power_watts_avg": 34.56, "power_watts_peak": 34.56, "energy_joules_est": 4.43, "sample_count": 2, "duration_seconds": 0.128}, "timestamp": "2026-01-25T19:58:40.308650"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 268.806, "latencies_ms": [268.806], "images_per_second": 3.72, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA woman stands in front of a table with several bunches of bananas, smiling and looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10442.5, "ram_available_mb": 112063.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10442.0, "ram_available_mb": 112064.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [34.56, 34.56, 30.82], "power_watts_avg": 33.31, "power_watts_peak": 34.56, "energy_joules_est": 8.99, "sample_count": 3, "duration_seconds": 0.27}, "timestamp": "2026-01-25T19:58:40.618626"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 70.902, "latencies_ms": [70.902], "images_per_second": 14.104, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Bananas", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10442.0, "ram_available_mb": 112064.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10446.1, "ram_available_mb": 112060.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [30.82], "power_watts_avg": 30.82, "power_watts_peak": 30.82, "energy_joules_est": 2.2, "sample_count": 1, "duration_seconds": 0.071}, "timestamp": "2026-01-25T19:58:40.723614"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 148.487, "latencies_ms": [148.487], "images_per_second": 6.735, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n 1. Woman with black hair and a floral patterned shirt smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.1, "ram_available_mb": 112060.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10446.0, "ram_available_mb": 112060.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [30.82, 30.82], "power_watts_avg": 30.82, "power_watts_peak": 30.82, "energy_joules_est": 4.58, "sample_count": 2, "duration_seconds": 0.149}, "timestamp": "2026-01-25T19:58:40.929376"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 170.702, "latencies_ms": [170.702], "images_per_second": 5.858, "prompt_tokens": 757, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA woman stands in front of a table with several bunches of bananas on display, smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.0, "ram_available_mb": 112060.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10447.7, "ram_available_mb": 112058.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [30.82, 30.54], "power_watts_avg": 30.68, "power_watts_peak": 30.82, "energy_joules_est": 5.26, "sample_count": 2, "duration_seconds": 0.171}, "timestamp": "2026-01-25T19:58:41.134947"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 147.062, "latencies_ms": [147.062], "images_per_second": 6.8, "prompt_tokens": 756, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n A woman with a black and white patterned shirt is smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.7, "ram_available_mb": 112058.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10447.7, "ram_available_mb": 112058.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [30.54, 30.54], "power_watts_avg": 30.54, "power_watts_peak": 30.54, "energy_joules_est": 4.52, "sample_count": 2, "duration_seconds": 0.148}, "timestamp": "2026-01-25T19:58:41.340689"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 291.414, "latencies_ms": [291.414], "images_per_second": 3.432, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nThe front of a building with graffiti on it, including the words \"Ridgeway\" and \"Grimm\".", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10447.7, "ram_available_mb": 112058.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10447.4, "ram_available_mb": 112058.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [30.54, 30.54, 31.11], "power_watts_avg": 30.73, "power_watts_peak": 31.11, "energy_joules_est": 8.98, "sample_count": 3, "duration_seconds": 0.292}, "timestamp": "2026-01-25T19:58:41.652563"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 85.997, "latencies_ms": [85.997], "images_per_second": 11.628, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Fire hydrant", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.4, "ram_available_mb": 112058.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10439.6, "ram_available_mb": 112066.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [31.11], "power_watts_avg": 31.11, "power_watts_peak": 31.11, "energy_joules_est": 2.68, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T19:58:41.757876"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 123.655, "latencies_ms": [123.655], "images_per_second": 8.087, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe building has a green door and a red roof.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10439.6, "ram_available_mb": 112066.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10439.2, "ram_available_mb": 112067.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [31.11, 31.11], "power_watts_avg": 31.11, "power_watts_peak": 31.11, "energy_joules_est": 3.85, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T19:58:41.964771"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 348.146, "latencies_ms": [348.146], "images_per_second": 2.872, "prompt_tokens": 757, "response_tokens_est": 59, "n_tiles": 1, "output_text": "\nThe image shows a city street with an old, run-down building featuring graffiti on its facade. The building has a green door and a fire hydrant in front of it. A tree can be seen to the right side of the building, adding some greenery to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10439.2, "ram_available_mb": 112067.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10440.0, "ram_available_mb": 112066.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 64.0}, "power_stats": {"power_watts_samples": [31.11, 31.65, 31.65, 31.65], "power_watts_avg": 31.52, "power_watts_peak": 31.65, "energy_joules_est": 10.99, "sample_count": 4, "duration_seconds": 0.349}, "timestamp": "2026-01-25T19:58:42.370735"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 115.296, "latencies_ms": [115.296], "images_per_second": 8.673, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The building has a green door and red shutters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10440.0, "ram_available_mb": 112066.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10440.2, "ram_available_mb": 112066.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 64.0}, "power_stats": {"power_watts_samples": [31.65, 31.65], "power_watts_avg": 31.65, "power_watts_peak": 31.65, "energy_joules_est": 3.66, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T19:58:42.576868"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 248.141, "latencies_ms": [248.141], "images_per_second": 4.03, "prompt_tokens": 744, "response_tokens_est": 20, "n_tiles": 1, "output_text": "urn of water on a table, with a man in black holding it up to his mouth.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10440.2, "ram_available_mb": 112066.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10441.2, "ram_available_mb": 112065.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 64.0}, "power_stats": {"power_watts_samples": [32.52, 32.52, 32.52], "power_watts_avg": 32.52, "power_watts_peak": 32.52, "energy_joules_est": 8.08, "sample_count": 3, "duration_seconds": 0.248}, "timestamp": "2026-01-25T19:58:42.889295"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 85.046, "latencies_ms": [85.046], "images_per_second": 11.758, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Man throwing frisbee", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10441.2, "ram_available_mb": 112065.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10443.6, "ram_available_mb": 112062.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 64.0}, "power_stats": {"power_watts_samples": [32.52], "power_watts_avg": 32.52, "power_watts_peak": 32.52, "energy_joules_est": 2.79, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T19:58:42.995890"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 106.185, "latencies_ms": [106.185], "images_per_second": 9.418, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Man with long hair and a blue hat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10443.6, "ram_available_mb": 112062.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10445.5, "ram_available_mb": 112060.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [32.52, 33.82], "power_watts_avg": 33.17, "power_watts_peak": 33.82, "energy_joules_est": 3.54, "sample_count": 2, "duration_seconds": 0.107}, "timestamp": "2026-01-25T19:58:43.203736"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 138.135, "latencies_ms": [138.135], "images_per_second": 7.239, "prompt_tokens": 757, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA man with long hair is playing frisbee in front of a crowd.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.5, "ram_available_mb": 112060.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10445.8, "ram_available_mb": 112060.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [33.82, 33.82], "power_watts_avg": 33.82, "power_watts_peak": 33.82, "energy_joules_est": 4.68, "sample_count": 2, "duration_seconds": 0.138}, "timestamp": "2026-01-25T19:58:43.410278"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 120.712, "latencies_ms": [120.712], "images_per_second": 8.284, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The man is wearing a black shirt and has long hair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.8, "ram_available_mb": 112060.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10445.1, "ram_available_mb": 112061.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [33.82, 28.59], "power_watts_avg": 31.2, "power_watts_peak": 33.82, "energy_joules_est": 3.78, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T19:58:43.616057"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 230.141, "latencies_ms": [230.141], "images_per_second": 4.345, "prompt_tokens": 744, "response_tokens_est": 19, "n_tiles": 1, "output_text": "urn of water on a table, with several people sitting around it and using their laptops.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.1, "ram_available_mb": 112061.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10448.3, "ram_available_mb": 112058.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [28.59, 28.59, 28.59], "power_watts_avg": 28.58, "power_watts_peak": 28.59, "energy_joules_est": 6.59, "sample_count": 3, "duration_seconds": 0.231}, "timestamp": "2026-01-25T19:58:43.930318"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 146.173, "latencies_ms": [146.173], "images_per_second": 6.841, "prompt_tokens": 759, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n 1. Laptop in front of woman with brown jacket and white shirt.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10448.3, "ram_available_mb": 112058.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10448.1, "ram_available_mb": 112058.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [28.59, 28.43], "power_watts_avg": 28.51, "power_watts_peak": 28.59, "energy_joules_est": 4.18, "sample_count": 2, "duration_seconds": 0.146}, "timestamp": "2026-01-25T19:58:44.136630"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 144.34, "latencies_ms": [144.34], "images_per_second": 6.928, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n 1. Laptop on table in front of woman with glasses and brown jacket.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10448.1, "ram_available_mb": 112058.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [28.43, 28.43], "power_watts_avg": 28.43, "power_watts_peak": 28.43, "energy_joules_est": 4.13, "sample_count": 2, "duration_seconds": 0.145}, "timestamp": "2026-01-25T19:58:44.341869"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 120.584, "latencies_ms": [120.584], "images_per_second": 8.293, "prompt_tokens": 757, "response_tokens_est": 12, "n_tiles": 1, "output_text": "urn of water on table in front of people using laptops", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [28.43, 28.43], "power_watts_avg": 28.43, "power_watts_peak": 28.43, "energy_joules_est": 3.44, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T19:58:44.547253"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 49.922, "latencies_ms": [49.922], "images_per_second": 20.031, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 10449.6, "ram_available_mb": 112056.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [30.02], "power_watts_avg": 30.02, "power_watts_peak": 30.02, "energy_joules_est": 1.51, "sample_count": 1, "duration_seconds": 0.05}, "timestamp": "2026-01-25T19:58:44.651675"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 243.465, "latencies_ms": [243.465], "images_per_second": 4.107, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA young girl stands under a blue umbrella, holding it with both hands and wearing a pink jacket.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10449.6, "ram_available_mb": 112056.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [30.02, 30.02, 30.02], "power_watts_avg": 30.02, "power_watts_peak": 30.02, "energy_joules_est": 7.34, "sample_count": 3, "duration_seconds": 0.244}, "timestamp": "2026-01-25T19:58:44.963941"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 253.917, "latencies_ms": [253.917], "images_per_second": 3.938, "prompt_tokens": 759, "response_tokens_est": 42, "n_tiles": 1, "output_text": "\n 1. Blue umbrella 2. Girl's arm 3. Girl's hand 4. Hand of the girl 5. Girl's head 6. Girl's hair 7. Girl's face 8. Girl's clothes", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10454.3, "ram_available_mb": 112052.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [30.02, 28.89, 28.89], "power_watts_avg": 29.27, "power_watts_peak": 30.02, "energy_joules_est": 7.44, "sample_count": 3, "duration_seconds": 0.254}, "timestamp": "2026-01-25T19:58:45.269827"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 116.315, "latencies_ms": [116.315], "images_per_second": 8.597, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe girl is holding a blue umbrella in front of her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.3, "ram_available_mb": 112052.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [28.89, 28.89], "power_watts_avg": 28.89, "power_watts_peak": 28.89, "energy_joules_est": 3.38, "sample_count": 2, "duration_seconds": 0.117}, "timestamp": "2026-01-25T19:58:45.476769"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 248.635, "latencies_ms": [248.635], "images_per_second": 4.022, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA young girl in a pink sweater stands under an open blue umbrella, holding it with both hands. She also has her arm around another person who is partially visible behind her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10451.1, "ram_available_mb": 112055.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [28.89, 30.19, 30.19], "power_watts_avg": 29.76, "power_watts_peak": 30.19, "energy_joules_est": 7.41, "sample_count": 3, "duration_seconds": 0.249}, "timestamp": "2026-01-25T19:58:45.783009"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 127.845, "latencies_ms": [127.845], "images_per_second": 7.822, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nThe girl is holding a blue umbrella and wearing a pink shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.1, "ram_available_mb": 112055.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10446.1, "ram_available_mb": 112060.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [30.19, 30.19], "power_watts_avg": 30.19, "power_watts_peak": 30.19, "energy_joules_est": 3.88, "sample_count": 2, "duration_seconds": 0.129}, "timestamp": "2026-01-25T19:58:45.988722"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 263.514, "latencies_ms": [263.514], "images_per_second": 3.795, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA man in a suit stands next to a computer monitor and keyboard, with two additional monitors on either side of him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.1, "ram_available_mb": 112060.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10439.1, "ram_available_mb": 112067.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 69.0}, "power_stats": {"power_watts_samples": [30.19, 29.36, 29.36], "power_watts_avg": 29.64, "power_watts_peak": 30.19, "energy_joules_est": 7.82, "sample_count": 3, "duration_seconds": 0.264}, "timestamp": "2026-01-25T19:58:46.301562"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 201.571, "latencies_ms": [201.571], "images_per_second": 4.961, "prompt_tokens": 759, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\n 1. Computer monitor  2. Keyboard 3. Mouse 4. Speaker 5. Monitor  6. Chair 7. Computer mouse 8. Computer mouse", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10439.1, "ram_available_mb": 112067.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10442.8, "ram_available_mb": 112063.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 69.0}, "power_stats": {"power_watts_samples": [29.36, 29.36, 34.51], "power_watts_avg": 31.08, "power_watts_peak": 34.51, "energy_joules_est": 6.28, "sample_count": 3, "duration_seconds": 0.202}, "timestamp": "2026-01-25T19:58:46.608686"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 123.683, "latencies_ms": [123.683], "images_per_second": 8.085, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. Man in suit standing next to a computer monitor and keyboard.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10442.8, "ram_available_mb": 112063.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10443.0, "ram_available_mb": 112063.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 69.0}, "power_stats": {"power_watts_samples": [34.51, 34.51], "power_watts_avg": 34.51, "power_watts_peak": 34.51, "energy_joules_est": 4.28, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T19:58:46.814569"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 136.93, "latencies_ms": [136.93], "images_per_second": 7.303, "prompt_tokens": 757, "response_tokens_est": 17, "n_tiles": 1, "output_text": "urn of water on a table, with a man standing next to it.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10443.0, "ram_available_mb": 112063.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10443.7, "ram_available_mb": 112062.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [34.51, 34.51], "power_watts_avg": 34.51, "power_watts_peak": 34.51, "energy_joules_est": 4.74, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T19:58:47.020231"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 96.872, "latencies_ms": [96.872], "images_per_second": 10.323, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "urns of light and dark colors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10443.6, "ram_available_mb": 112062.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10442.7, "ram_available_mb": 112063.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [32.17], "power_watts_avg": 32.17, "power_watts_peak": 32.17, "energy_joules_est": 3.14, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T19:58:47.125079"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 251.858, "latencies_ms": [251.858], "images_per_second": 3.97, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urns of water and tea on a table, with four people sitting around it in a cozy room.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10442.7, "ram_available_mb": 112063.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10446.3, "ram_available_mb": 112060.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [32.17, 32.17, 32.17], "power_watts_avg": 32.17, "power_watts_peak": 32.17, "energy_joules_est": 8.11, "sample_count": 3, "duration_seconds": 0.252}, "timestamp": "2026-01-25T19:58:47.438106"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 654.475, "latencies_ms": [654.475], "images_per_second": 1.528, "prompt_tokens": 759, "response_tokens_est": 120, "n_tiles": 1, "output_text": "\n 1. Coffee cup 2. Coffee cup 3. Coffee cup 4. Coffee cup 5. Coffee cup 6. Coffee cup 7. Coffee cup 8. Coffee cup 9. Coffee cup 10. Coffee cup 11. Coffee cup 12. Coffee cup 13. Coffee cup 14. Coffee cup 15. Coffee cup 16. Coffee cup 17. Coffee cup 18. Coffee cup 19. Coffee cup 20. Coffee cup 21. Coffee cup 22. Coffee cup 23. Coffee cup 24. Coffee cup 25. Coffee cup 26. Coffee cup 27. Coffee cup 28. Coffee cup 29. Coffee cup 30.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.3, "ram_available_mb": 112060.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10450.9, "ram_available_mb": 112055.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.17, 29.0, 29.0, 29.0, 29.0, 29.0, 38.18], "power_watts_avg": 30.77, "power_watts_peak": 38.18, "energy_joules_est": 20.16, "sample_count": 7, "duration_seconds": 0.655}, "timestamp": "2026-01-25T19:58:48.146569"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 139.356, "latencies_ms": [139.356], "images_per_second": 7.176, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n 1. A man in a blue shirt sitting at a table with other men.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.6, "ram_available_mb": 112055.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [38.18, 38.18], "power_watts_avg": 38.18, "power_watts_peak": 38.18, "energy_joules_est": 5.34, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T19:58:48.354938"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 126.812, "latencies_ms": [126.812], "images_per_second": 7.886, "prompt_tokens": 757, "response_tokens_est": 17, "n_tiles": 1, "output_text": "urns of tea on a table, with four people sitting around it.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [38.18, 38.18], "power_watts_avg": 38.18, "power_watts_peak": 38.18, "energy_joules_est": 4.87, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T19:58:48.561856"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 213.687, "latencies_ms": [213.687], "images_per_second": 4.68, "prompt_tokens": 756, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\n A group of people are sitting around a table with food and drinks. The room has wooden beams on the ceiling and large windows that let in natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10452.8, "ram_available_mb": 112053.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.03, 36.03, 36.03], "power_watts_avg": 36.03, "power_watts_peak": 36.03, "energy_joules_est": 7.72, "sample_count": 3, "duration_seconds": 0.214}, "timestamp": "2026-01-25T19:58:48.868699"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 288.383, "latencies_ms": [288.383], "images_per_second": 3.468, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA red pickup truck is driving down a snowy street, with snow being cleared from its tires by a plow attached to it.", "error": null, "sys_before": {"cpu_percent": 6.1, "ram_used_mb": 10452.8, "ram_available_mb": 112053.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10460.9, "ram_available_mb": 112045.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [36.03, 36.03, 29.85], "power_watts_avg": 33.97, "power_watts_peak": 36.03, "energy_joules_est": 9.81, "sample_count": 3, "duration_seconds": 0.289}, "timestamp": "2026-01-25T19:58:49.186341"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 61.692, "latencies_ms": [61.692], "images_per_second": 16.21, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Truck", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.9, "ram_available_mb": 112045.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [29.85], "power_watts_avg": 29.85, "power_watts_peak": 29.85, "energy_joules_est": 1.85, "sample_count": 1, "duration_seconds": 0.062}, "timestamp": "2026-01-25T19:58:49.291854"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 164.287, "latencies_ms": [164.287], "images_per_second": 6.087, "prompt_tokens": 763, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\n 1. Red truck driving down a snowy street with snow on its roof and plow attached to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10463.8, "ram_available_mb": 112042.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [29.85, 29.85], "power_watts_avg": 29.85, "power_watts_peak": 29.85, "energy_joules_est": 4.92, "sample_count": 2, "duration_seconds": 0.165}, "timestamp": "2026-01-25T19:58:49.498578"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 174.059, "latencies_ms": [174.059], "images_per_second": 5.745, "prompt_tokens": 757, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA red pickup truck with a snow plow attached is driving down a snowy street, clearing the road for other vehicles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.8, "ram_available_mb": 112042.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10463.8, "ram_available_mb": 112042.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [29.85, 32.07], "power_watts_avg": 30.96, "power_watts_peak": 32.07, "energy_joules_est": 5.41, "sample_count": 2, "duration_seconds": 0.175}, "timestamp": "2026-01-25T19:58:49.705271"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 105.881, "latencies_ms": [105.881], "images_per_second": 9.445, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The red truck is driving down a snowy street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.8, "ram_available_mb": 112042.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [32.07, 32.07], "power_watts_avg": 32.07, "power_watts_peak": 32.07, "energy_joules_est": 3.4, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T19:58:49.910302"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 265.026, "latencies_ms": [265.026], "images_per_second": 3.773, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA person stands in front of a large mirror, taking a selfie with their reflection visible on the wall above them.", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10452.3, "ram_available_mb": 112054.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 2.0}, "power_stats": {"power_watts_samples": [32.07, 31.49, 31.49], "power_watts_avg": 31.68, "power_watts_peak": 32.07, "energy_joules_est": 8.42, "sample_count": 3, "duration_seconds": 0.266}, "timestamp": "2026-01-25T19:58:50.228375"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 687.258, "latencies_ms": [687.258], "images_per_second": 1.455, "prompt_tokens": 759, "response_tokens_est": 128, "n_tiles": 1, "output_text": "\n 1. Mirror  2. Tv  3. Tv stand  4. Tv  5. Tv stand  6. Tv  7. Tv  8. Tv  9. Tv  10. Tv  11. Tv  12. Tv  13. Tv  14. Tv  15. Tv  16. Tv  17. Tv  18. Tv  19. Tv  20. Tv  21. Tv  22. Tv  23. Tv  24. Tv  25. Tv  26", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.3, "ram_available_mb": 112054.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10453.0, "ram_available_mb": 112053.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 2.0}, "power_stats": {"power_watts_samples": [31.49, 31.49, 31.49, 34.65, 34.65, 34.65, 34.65], "power_watts_avg": 33.29, "power_watts_peak": 34.65, "energy_joules_est": 22.9, "sample_count": 7, "duration_seconds": 0.688}, "timestamp": "2026-01-25T19:58:50.935172"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 157.292, "latencies_ms": [157.292], "images_per_second": 6.358, "prompt_tokens": 763, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n 1. A person taking a picture of themselves in a mirror with a television monitor above it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.0, "ram_available_mb": 112053.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10451.9, "ram_available_mb": 112054.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 93.0}, "power_stats": {"power_watts_samples": [34.65, 40.6], "power_watts_avg": 37.62, "power_watts_peak": 40.6, "energy_joules_est": 5.93, "sample_count": 2, "duration_seconds": 0.158}, "timestamp": "2026-01-25T19:58:51.140124"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 345.703, "latencies_ms": [345.703], "images_per_second": 2.893, "prompt_tokens": 757, "response_tokens_est": 58, "n_tiles": 1, "output_text": "\nIn a luxurious bathroom, a person stands in front of a large mirror with their back to the camera. The bathroom features two sinks under a glass-enclosed countertop, a television mounted on the wall above one sink, and a large mirror reflecting the room's opulence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.8, "ram_available_mb": 112054.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10450.7, "ram_available_mb": 112055.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 93.0}, "power_stats": {"power_watts_samples": [40.6, 40.6, 40.6, 40.6], "power_watts_avg": 40.6, "power_watts_peak": 40.6, "energy_joules_est": 14.06, "sample_count": 4, "duration_seconds": 0.346}, "timestamp": "2026-01-25T19:58:51.549184"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 116.047, "latencies_ms": [116.047], "images_per_second": 8.617, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n A person taking a picture of themselves in the mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.7, "ram_available_mb": 112055.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 93.0}, "power_stats": {"power_watts_samples": [37.81, 37.81], "power_watts_avg": 37.81, "power_watts_peak": 37.81, "energy_joules_est": 4.41, "sample_count": 2, "duration_seconds": 0.117}, "timestamp": "2026-01-25T19:58:51.757067"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 262.705, "latencies_ms": [262.705], "images_per_second": 3.807, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nTwo men are in an airport baggage claim area, with a white car parked nearby and a cart filled with luggage.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10452.5, "ram_available_mb": 112053.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [37.81, 37.81, 37.81], "power_watts_avg": 37.81, "power_watts_peak": 37.81, "energy_joules_est": 9.96, "sample_count": 3, "duration_seconds": 0.263}, "timestamp": "2026-01-25T19:58:52.070429"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 73.313, "latencies_ms": [73.313], "images_per_second": 13.64, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Car: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.5, "ram_available_mb": 112053.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10454.5, "ram_available_mb": 112051.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [34.69], "power_watts_avg": 34.69, "power_watts_peak": 34.69, "energy_joules_est": 2.56, "sample_count": 1, "duration_seconds": 0.074}, "timestamp": "2026-01-25T19:58:52.177627"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 97.599, "latencies_ms": [97.599], "images_per_second": 10.246, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Man with luggage in front of him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.5, "ram_available_mb": 112051.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [34.69], "power_watts_avg": 34.69, "power_watts_peak": 34.69, "energy_joules_est": 3.41, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T19:58:52.282730"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 301.906, "latencies_ms": [301.906], "images_per_second": 3.312, "prompt_tokens": 757, "response_tokens_est": 51, "n_tiles": 1, "output_text": "\nIn a parking garage, two men are loading luggage into their car. One man is pushing a cart with his luggage on wheels, while another man is carrying a suitcase in his hand. The car they are preparing to load has its trunk open.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10457.5, "ram_available_mb": 112048.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [34.69, 34.69, 34.69], "power_watts_avg": 34.69, "power_watts_peak": 34.69, "energy_joules_est": 10.48, "sample_count": 3, "duration_seconds": 0.302}, "timestamp": "2026-01-25T19:58:52.588994"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 86.099, "latencies_ms": [86.099], "images_per_second": 11.614, "prompt_tokens": 756, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n The car is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.5, "ram_available_mb": 112048.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10457.5, "ram_available_mb": 112048.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [32.21], "power_watts_avg": 32.21, "power_watts_peak": 32.21, "energy_joules_est": 2.78, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T19:58:52.693254"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 285.012, "latencies_ms": [285.012], "images_per_second": 3.509, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA white plate holds a sandwich with sesame seeds and a side of fries, accompanied by two small bowls filled with condiments.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10457.5, "ram_available_mb": 112048.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10454.5, "ram_available_mb": 112051.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.21, 32.21, 32.21], "power_watts_avg": 32.21, "power_watts_peak": 32.21, "energy_joules_est": 9.21, "sample_count": 3, "duration_seconds": 0.286}, "timestamp": "2026-01-25T19:58:53.007713"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 89.082, "latencies_ms": [89.082], "images_per_second": 11.226, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Sandwich: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.5, "ram_available_mb": 112051.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10456.6, "ram_available_mb": 112049.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.81], "power_watts_avg": 35.81, "power_watts_peak": 35.81, "energy_joules_est": 3.21, "sample_count": 1, "duration_seconds": 0.09}, "timestamp": "2026-01-25T19:58:53.113601"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 80.285, "latencies_ms": [80.285], "images_per_second": 12.456, "prompt_tokens": 763, "response_tokens_est": 7, "n_tiles": 1, "output_text": "ids of food items on plate", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.6, "ram_available_mb": 112049.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10456.6, "ram_available_mb": 112049.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.81], "power_watts_avg": 35.81, "power_watts_peak": 35.81, "energy_joules_est": 2.88, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T19:58:53.218561"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 210.02, "latencies_ms": [210.02], "images_per_second": 4.761, "prompt_tokens": 757, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA white plate holds a sandwich with sesame seeds on top, accompanied by French fries. A blue bowl of ketchup is also present on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.6, "ram_available_mb": 112049.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10458.4, "ram_available_mb": 112047.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.81, 35.81, 35.81], "power_watts_avg": 35.81, "power_watts_peak": 35.81, "energy_joules_est": 7.53, "sample_count": 3, "duration_seconds": 0.21}, "timestamp": "2026-01-25T19:58:53.525594"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 92.897, "latencies_ms": [92.897], "images_per_second": 10.765, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urn of ketchup and a blue bowl.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.4, "ram_available_mb": 112047.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.16], "power_watts_avg": 34.16, "power_watts_peak": 34.16, "energy_joules_est": 3.19, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T19:58:53.631703"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 285.431, "latencies_ms": [285.431], "images_per_second": 3.503, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "urns of flowers are placed on a table in front of a large window with curtains, and a bed is situated next to it.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10451.0, "ram_available_mb": 112055.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.16, 34.16, 34.16], "power_watts_avg": 34.16, "power_watts_peak": 34.16, "energy_joules_est": 9.76, "sample_count": 3, "duration_seconds": 0.286}, "timestamp": "2026-01-25T19:58:53.943376"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 71.52, "latencies_ms": [71.52], "images_per_second": 13.982, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Bed", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.0, "ram_available_mb": 112055.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10452.7, "ram_available_mb": 112053.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.16], "power_watts_avg": 34.16, "power_watts_peak": 34.16, "energy_joules_est": 2.45, "sample_count": 1, "duration_seconds": 0.072}, "timestamp": "2026-01-25T19:58:54.050261"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 109.286, "latencies_ms": [109.286], "images_per_second": 9.15, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urn on a table in front of bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.7, "ram_available_mb": 112053.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10452.7, "ram_available_mb": 112053.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.86, 31.86], "power_watts_avg": 31.86, "power_watts_peak": 31.86, "energy_joules_est": 3.49, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T19:58:54.256089"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 438.675, "latencies_ms": [438.675], "images_per_second": 2.28, "prompt_tokens": 757, "response_tokens_est": 76, "n_tiles": 1, "output_text": "\nThe image shows a large bed with a green mosquito netting in a room. The bed is positioned against a wall, with a window nearby that allows natural light to enter the space. A small table can be seen next to the bed, and there are two chairs placed around it. On one side of the room, there is a dining table accompanied by four chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.7, "ram_available_mb": 112053.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10452.7, "ram_available_mb": 112053.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.86, 31.86, 31.86, 32.25, 32.25], "power_watts_avg": 32.02, "power_watts_peak": 32.25, "energy_joules_est": 14.06, "sample_count": 5, "duration_seconds": 0.439}, "timestamp": "2026-01-25T19:58:54.762515"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 303.129, "latencies_ms": [303.129], "images_per_second": 3.299, "prompt_tokens": 756, "response_tokens_est": 47, "n_tiles": 1, "output_text": "\n The room is painted in a light brown color and has a green bedspread. There are two windows with curtains that let in natural light. A wooden table sits next to the bed, and there's a candle on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.7, "ram_available_mb": 112053.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [32.25, 32.25, 32.25, 31.33], "power_watts_avg": 32.02, "power_watts_peak": 32.25, "energy_joules_est": 9.72, "sample_count": 4, "duration_seconds": 0.303}, "timestamp": "2026-01-25T19:58:55.169572"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 275.832, "latencies_ms": [275.832], "images_per_second": 3.625, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA gray and white cat stands on top of a black car in an indoor garage setting, with various objects scattered around the space.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10455.1, "ram_available_mb": 112051.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [31.33, 31.33, 31.33], "power_watts_avg": 31.33, "power_watts_peak": 31.33, "energy_joules_est": 8.66, "sample_count": 3, "duration_seconds": 0.276}, "timestamp": "2026-01-25T19:58:55.482014"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 75.349, "latencies_ms": [75.349], "images_per_second": 13.272, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Lamp - 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.1, "ram_available_mb": 112051.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10455.5, "ram_available_mb": 112050.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [31.33], "power_watts_avg": 31.33, "power_watts_peak": 31.33, "energy_joules_est": 2.37, "sample_count": 1, "duration_seconds": 0.076}, "timestamp": "2026-01-25T19:58:55.587681"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 91.744, "latencies_ms": [91.744], "images_per_second": 10.9, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Cat on car hood - Front cat", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.5, "ram_available_mb": 112050.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10454.9, "ram_available_mb": 112051.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [36.09], "power_watts_avg": 36.09, "power_watts_peak": 36.09, "energy_joules_est": 3.32, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T19:58:55.693138"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 229.41, "latencies_ms": [229.41], "images_per_second": 4.359, "prompt_tokens": 757, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nIn a garage, there is a black car with a cat standing on its hood. The cat appears to be looking at something in the distance or possibly observing the surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.9, "ram_available_mb": 112051.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10454.9, "ram_available_mb": 112051.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [36.09, 36.09, 36.09], "power_watts_avg": 36.09, "power_watts_peak": 36.09, "energy_joules_est": 8.29, "sample_count": 3, "duration_seconds": 0.23}, "timestamp": "2026-01-25T19:58:56.000144"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 94.587, "latencies_ms": [94.587], "images_per_second": 10.572, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The cat is gray and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.9, "ram_available_mb": 112051.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10451.3, "ram_available_mb": 112055.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.09], "power_watts_avg": 36.09, "power_watts_peak": 36.09, "energy_joules_est": 3.44, "sample_count": 1, "duration_seconds": 0.095}, "timestamp": "2026-01-25T19:58:56.105858"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 308.227, "latencies_ms": [308.227], "images_per_second": 3.244, "prompt_tokens": 744, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA plate of food is placed on a metal table, featuring a sandwich with meat and vegetables. The sandwich appears to be covered in gravy or sauce.", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 10451.3, "ram_available_mb": 112055.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10449.9, "ram_available_mb": 112056.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.56, 33.56, 33.56, 33.56], "power_watts_avg": 33.56, "power_watts_peak": 33.56, "energy_joules_est": 10.35, "sample_count": 4, "duration_seconds": 0.308}, "timestamp": "2026-01-25T19:58:56.520863"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 83.123, "latencies_ms": [83.123], "images_per_second": 12.03, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Plate - 2", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10449.9, "ram_available_mb": 112056.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10449.5, "ram_available_mb": 112056.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.77], "power_watts_avg": 32.77, "power_watts_peak": 32.77, "energy_joules_est": 2.74, "sample_count": 1, "duration_seconds": 0.084}, "timestamp": "2026-01-25T19:58:56.626133"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 118.451, "latencies_ms": [118.451], "images_per_second": 8.442, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n A plate of food with sauce on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.5, "ram_available_mb": 112056.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.77, 32.77], "power_watts_avg": 32.77, "power_watts_peak": 32.77, "energy_joules_est": 3.89, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T19:58:56.832178"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 236.798, "latencies_ms": [236.798], "images_per_second": 4.223, "prompt_tokens": 757, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nA plate of food sits on a table, featuring a sandwich with meat, sauce, tomatoes, and onions. The dish appears to be a burger or a similar type of meal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10450.4, "ram_available_mb": 112055.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [32.77, 32.77, 30.94], "power_watts_avg": 32.16, "power_watts_peak": 32.77, "energy_joules_est": 7.64, "sample_count": 3, "duration_seconds": 0.237}, "timestamp": "2026-01-25T19:58:57.138222"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 82.107, "latencies_ms": [82.107], "images_per_second": 12.179, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The plate is white and black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.4, "ram_available_mb": 112055.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10453.1, "ram_available_mb": 112053.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [30.94], "power_watts_avg": 30.94, "power_watts_peak": 30.94, "energy_joules_est": 2.56, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T19:58:57.244225"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 287.2, "latencies_ms": [287.2], "images_per_second": 3.482, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\n A man in a white shirt stands over two men sitting on a couch, playing a video game with a Wii remote.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10453.1, "ram_available_mb": 112053.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10450.9, "ram_available_mb": 112055.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [30.94, 30.94, 30.94], "power_watts_avg": 30.94, "power_watts_peak": 30.94, "energy_joules_est": 8.9, "sample_count": 3, "duration_seconds": 0.288}, "timestamp": "2026-01-25T19:58:57.555545"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.593, "latencies_ms": [78.593], "images_per_second": 12.724, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Couch - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.9, "ram_available_mb": 112055.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [30.64], "power_watts_avg": 30.64, "power_watts_peak": 30.64, "energy_joules_est": 2.43, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T19:58:57.659945"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 158.068, "latencies_ms": [158.068], "images_per_second": 6.326, "prompt_tokens": 763, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n 1. A man in a white shirt playing wii with two other men sitting on a couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [30.64, 30.64], "power_watts_avg": 30.64, "power_watts_peak": 30.64, "energy_joules_est": 4.85, "sample_count": 2, "duration_seconds": 0.158}, "timestamp": "2026-01-25T19:58:57.865020"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 171.309, "latencies_ms": [171.309], "images_per_second": 5.837, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n A man in a white shirt stands over two men sitting on a couch, playing a video game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.8, "ram_available_mb": 112058.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10442.5, "ram_available_mb": 112063.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 8.0}, "power_stats": {"power_watts_samples": [30.64, 30.64], "power_watts_avg": 30.64, "power_watts_peak": 30.64, "energy_joules_est": 5.27, "sample_count": 2, "duration_seconds": 0.172}, "timestamp": "2026-01-25T19:58:58.071094"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 159.254, "latencies_ms": [159.254], "images_per_second": 6.279, "prompt_tokens": 756, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n A man in a white shirt is playing wii while two other men are sitting on the couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10442.5, "ram_available_mb": 112063.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 8.0}, "power_stats": {"power_watts_samples": [32.89, 32.89], "power_watts_avg": 32.89, "power_watts_peak": 32.89, "energy_joules_est": 5.26, "sample_count": 2, "duration_seconds": 0.16}, "timestamp": "2026-01-25T19:58:58.278769"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 317.269, "latencies_ms": [317.269], "images_per_second": 3.152, "prompt_tokens": 744, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA baseball catcher crouches on a field, wearing a black and white uniform with the number 15 on it. The catcher is holding a mitt in their right hand.", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10449.2, "ram_available_mb": 112057.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 8.0}, "power_stats": {"power_watts_samples": [32.89, 32.89, 32.89, 33.28], "power_watts_avg": 32.99, "power_watts_peak": 33.28, "energy_joules_est": 10.47, "sample_count": 4, "duration_seconds": 0.317}, "timestamp": "2026-01-25T19:58:58.694760"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 203.703, "latencies_ms": [203.703], "images_per_second": 4.909, "prompt_tokens": 759, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\n 1. Pitcher's mound 2. Home plate 3. Catcher's mitt 4. Glove 5. Helmet 6. Home plate", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.2, "ram_available_mb": 112057.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10450.9, "ram_available_mb": 112055.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 8.0}, "power_stats": {"power_watts_samples": [33.28, 33.28, 33.28], "power_watts_avg": 33.28, "power_watts_peak": 33.28, "energy_joules_est": 6.79, "sample_count": 3, "duration_seconds": 0.204}, "timestamp": "2026-01-25T19:58:59.002099"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 117.948, "latencies_ms": [117.948], "images_per_second": 8.478, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe catcher is crouched down in front of home plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.9, "ram_available_mb": 112055.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10445.3, "ram_available_mb": 112061.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [33.28, 33.45], "power_watts_avg": 33.37, "power_watts_peak": 33.45, "energy_joules_est": 3.95, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T19:58:59.208186"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 256.518, "latencies_ms": [256.518], "images_per_second": 3.898, "prompt_tokens": 757, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\nA baseball catcher crouches on a field, wearing his uniform with white pants and black shoes. He holds a mitt in front of him as he prepares to catch a ball during a game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.3, "ram_available_mb": 112061.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10440.5, "ram_available_mb": 112065.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [33.45, 33.45, 33.45], "power_watts_avg": 33.45, "power_watts_peak": 33.45, "energy_joules_est": 8.6, "sample_count": 3, "duration_seconds": 0.257}, "timestamp": "2026-01-25T19:58:59.516942"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 531.264, "latencies_ms": [531.264], "images_per_second": 1.882, "prompt_tokens": 756, "response_tokens_est": 96, "n_tiles": 1, "output_text": "\n The image shows a baseball catcher crouching on the field. He is wearing a black and white uniform with yellow stripes, which makes him easily identifiable as an infielder for his team. His helmet has a face guard to protect him from potential injuries during the game. In addition to that, he also wears gloves to catch the ball effectively. The scene takes place in front of a green field, providing a natural backdrop and setting for this athletic moment captured by the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10440.5, "ram_available_mb": 112065.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10440.3, "ram_available_mb": 112066.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [29.67, 29.67, 29.67, 29.67, 29.67, 34.02], "power_watts_avg": 30.39, "power_watts_peak": 34.02, "energy_joules_est": 16.16, "sample_count": 6, "duration_seconds": 0.532}, "timestamp": "2026-01-25T19:59:00.124213"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 236.945, "latencies_ms": [236.945], "images_per_second": 4.22, "prompt_tokens": 744, "response_tokens_est": 19, "n_tiles": 1, "output_text": "urns of soap and shampoo on a bathroom counter, with a pink wall behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10440.3, "ram_available_mb": 112066.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10443.5, "ram_available_mb": 112062.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.02, 34.02, 34.02], "power_watts_avg": 34.02, "power_watts_peak": 34.02, "energy_joules_est": 8.1, "sample_count": 3, "duration_seconds": 0.238}, "timestamp": "2026-01-25T19:59:00.433488"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 86.176, "latencies_ms": [86.176], "images_per_second": 11.604, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Shower curtain - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10443.5, "ram_available_mb": 112062.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10448.2, "ram_available_mb": 112058.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.02], "power_watts_avg": 34.02, "power_watts_peak": 34.02, "energy_joules_est": 2.95, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T19:59:00.538728"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 86.056, "latencies_ms": [86.056], "images_per_second": 11.62, "prompt_tokens": 763, "response_tokens_est": 6, "n_tiles": 1, "output_text": "urns on countertop", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10448.2, "ram_available_mb": 112058.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10448.2, "ram_available_mb": 112058.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.41], "power_watts_avg": 36.41, "power_watts_peak": 36.41, "energy_joules_est": 3.14, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T19:59:00.643377"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 228.058, "latencies_ms": [228.058], "images_per_second": 4.385, "prompt_tokens": 757, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nThe image shows a small bathroom with pink walls, white tile flooring, and a wooden vanity. The room features a toilet, sink, and shower area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.2, "ram_available_mb": 112058.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10447.3, "ram_available_mb": 112059.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.41, 36.41, 36.41], "power_watts_avg": 36.41, "power_watts_peak": 36.41, "energy_joules_est": 8.31, "sample_count": 3, "duration_seconds": 0.228}, "timestamp": "2026-01-25T19:59:00.950263"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 98.342, "latencies_ms": [98.342], "images_per_second": 10.169, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The bathroom has pink walls and a white shower curtain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.3, "ram_available_mb": 112059.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10452.0, "ram_available_mb": 112054.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [36.41], "power_watts_avg": 36.41, "power_watts_peak": 36.41, "energy_joules_est": 3.6, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T19:59:01.055239"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 230.108, "latencies_ms": [230.108], "images_per_second": 4.346, "prompt_tokens": 744, "response_tokens_est": 17, "n_tiles": 1, "output_text": "urn of gold and black color on a bed with white sheets in a bedroom.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 10452.0, "ram_available_mb": 112054.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10450.9, "ram_available_mb": 112055.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [31.24, 31.24, 31.24], "power_watts_avg": 31.24, "power_watts_peak": 31.24, "energy_joules_est": 7.2, "sample_count": 3, "duration_seconds": 0.23}, "timestamp": "2026-01-25T19:59:01.366023"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 72.141, "latencies_ms": [72.141], "images_per_second": 13.862, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Bed", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.1, "ram_available_mb": 112055.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10451.1, "ram_available_mb": 112055.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [31.24], "power_watts_avg": 31.24, "power_watts_peak": 31.24, "energy_joules_est": 2.27, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T19:59:01.471757"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 79.335, "latencies_ms": [79.335], "images_per_second": 12.605, "prompt_tokens": 763, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urn of flowers on a table", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.1, "ram_available_mb": 112055.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10452.8, "ram_available_mb": 112053.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [31.24], "power_watts_avg": 31.24, "power_watts_peak": 31.24, "energy_joules_est": 2.5, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T19:59:01.577394"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 340.797, "latencies_ms": [340.797], "images_per_second": 2.934, "prompt_tokens": 757, "response_tokens_est": 59, "n_tiles": 1, "output_text": "\nThe image shows a bedroom with a bed covered in a yellow and white plaid comforter. The room has a window that allows natural light to enter, creating a warm atmosphere. A nightstand next to the bed holds a lamp, providing additional lighting for reading or other activities.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.8, "ram_available_mb": 112053.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10453.4, "ram_available_mb": 112052.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [31.19, 31.19, 31.19, 31.19], "power_watts_avg": 31.19, "power_watts_peak": 31.19, "energy_joules_est": 10.65, "sample_count": 4, "duration_seconds": 0.341}, "timestamp": "2026-01-25T19:59:01.983661"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 123.256, "latencies_ms": [123.256], "images_per_second": 8.113, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A bed with a yellow and white plaid comforter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.4, "ram_available_mb": 112052.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10453.4, "ram_available_mb": 112052.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.19, 33.78], "power_watts_avg": 32.48, "power_watts_peak": 33.78, "energy_joules_est": 4.01, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T19:59:02.189730"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 311.684, "latencies_ms": [311.684], "images_per_second": 3.208, "prompt_tokens": 744, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA young man in a black suit and gold tie is helping a woman put on a flower, while she wears a black dress with a pink sash around her waist.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10453.4, "ram_available_mb": 112052.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10454.9, "ram_available_mb": 112051.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.78, 33.78, 33.78, 33.78], "power_watts_avg": 33.78, "power_watts_peak": 33.78, "energy_joules_est": 10.54, "sample_count": 4, "duration_seconds": 0.312}, "timestamp": "2026-01-25T19:59:02.599264"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 80.065, "latencies_ms": [80.065], "images_per_second": 12.49, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n Man: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.9, "ram_available_mb": 112051.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10454.4, "ram_available_mb": 112051.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.79], "power_watts_avg": 35.79, "power_watts_peak": 35.79, "energy_joules_est": 2.88, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T19:59:02.703402"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 97.74, "latencies_ms": [97.74], "images_per_second": 10.231, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nA woman is fixing a man's tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.4, "ram_available_mb": 112051.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10454.4, "ram_available_mb": 112051.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.79], "power_watts_avg": 35.79, "power_watts_peak": 35.79, "energy_joules_est": 3.51, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T19:59:02.807224"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 43.257, "latencies_ms": [43.257], "images_per_second": 23.117, "prompt_tokens": 757, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.4, "ram_available_mb": 112051.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.79], "power_watts_avg": 35.79, "power_watts_peak": 35.79, "energy_joules_est": 1.55, "sample_count": 1, "duration_seconds": 0.043}, "timestamp": "2026-01-25T19:59:02.911542"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 48.768, "latencies_ms": [48.768], "images_per_second": 20.505, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [35.79], "power_watts_avg": 35.79, "power_watts_peak": 35.79, "energy_joules_est": 1.77, "sample_count": 1, "duration_seconds": 0.049}, "timestamp": "2026-01-25T19:59:03.016793"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 270.943, "latencies_ms": [270.943], "images_per_second": 3.691, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA stop sign is attached to a chain link fence, with trash scattered around it and palm trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10456.3, "ram_available_mb": 112050.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [28.91, 28.91, 28.91], "power_watts_avg": 28.91, "power_watts_peak": 28.91, "energy_joules_est": 7.84, "sample_count": 3, "duration_seconds": 0.271}, "timestamp": "2026-01-25T19:59:03.328246"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.447, "latencies_ms": [79.447], "images_per_second": 12.587, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Stop sign", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.3, "ram_available_mb": 112050.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10450.8, "ram_available_mb": 112055.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [28.91], "power_watts_avg": 28.91, "power_watts_peak": 28.91, "energy_joules_est": 2.3, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T19:59:03.434166"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 104.699, "latencies_ms": [104.699], "images_per_second": 9.551, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nA stop sign is attached to a chain link fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.1, "ram_available_mb": 112055.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10451.3, "ram_available_mb": 112055.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [28.91, 27.4], "power_watts_avg": 28.16, "power_watts_peak": 28.91, "energy_joules_est": 2.97, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T19:59:03.640062"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 226.129, "latencies_ms": [226.129], "images_per_second": 4.422, "prompt_tokens": 757, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nA stop sign is attached to a chain-link fence, with trash scattered around the area. The fence is located in front of palm trees on a street corner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10443.9, "ram_available_mb": 112062.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [27.4, 27.4, 27.4], "power_watts_avg": 27.4, "power_watts_peak": 27.4, "energy_joules_est": 6.2, "sample_count": 3, "duration_seconds": 0.226}, "timestamp": "2026-01-25T19:59:03.947751"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 112.885, "latencies_ms": [112.885], "images_per_second": 8.859, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n The stop sign is red and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10443.9, "ram_available_mb": 112062.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10447.6, "ram_available_mb": 112058.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [27.4, 30.63], "power_watts_avg": 29.01, "power_watts_peak": 30.63, "energy_joules_est": 3.29, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T19:59:04.153371"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 261.974, "latencies_ms": [261.974], "images_per_second": 3.817, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA man stands next to a black bicycle with a basket, while another person rides a motorcycle behind him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.6, "ram_available_mb": 112058.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10453.4, "ram_available_mb": 112052.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [30.63, 30.63, 30.63], "power_watts_avg": 30.63, "power_watts_peak": 30.63, "energy_joules_est": 8.04, "sample_count": 3, "duration_seconds": 0.263}, "timestamp": "2026-01-25T19:59:04.463509"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 99.334, "latencies_ms": [99.334], "images_per_second": 10.067, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Motorcycle: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.6, "ram_available_mb": 112052.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [30.63], "power_watts_avg": 30.63, "power_watts_peak": 30.63, "energy_joules_est": 3.05, "sample_count": 1, "duration_seconds": 0.1}, "timestamp": "2026-01-25T19:59:04.568416"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 164.226, "latencies_ms": [164.226], "images_per_second": 6.089, "prompt_tokens": 763, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n 1. A black bicycle with a basket on it is parked next to another bike and a motorcycle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10458.0, "ram_available_mb": 112048.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [30.49, 30.49], "power_watts_avg": 30.49, "power_watts_peak": 30.49, "energy_joules_est": 5.01, "sample_count": 2, "duration_seconds": 0.164}, "timestamp": "2026-01-25T19:59:04.773840"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 240.385, "latencies_ms": [240.385], "images_per_second": 4.16, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\n A man stands next to a black bicycle with a basket, while another person rides a motorcycle behind him. The bicycles are parked on the side of the road near some bushes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.0, "ram_available_mb": 112048.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [30.49, 30.49, 30.49], "power_watts_avg": 30.49, "power_watts_peak": 30.49, "energy_joules_est": 7.35, "sample_count": 3, "duration_seconds": 0.241}, "timestamp": "2026-01-25T19:59:05.080815"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 92.425, "latencies_ms": [92.425], "images_per_second": 10.82, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The motorcycle is silver and black.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10462.6, "ram_available_mb": 112043.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [33.07], "power_watts_avg": 33.07, "power_watts_peak": 33.07, "energy_joules_est": 3.09, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T19:59:05.185461"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 325.276, "latencies_ms": [325.276], "images_per_second": 3.074, "prompt_tokens": 744, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA man in a blue shirt stands on a sidewalk, looking at a street sign with an arrow pointing to Proctor 2 and a \"No Parking\" sign above it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.6, "ram_available_mb": 112043.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10460.0, "ram_available_mb": 112046.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [33.07, 33.07, 33.07, 33.07], "power_watts_avg": 33.07, "power_watts_peak": 33.07, "energy_joules_est": 10.77, "sample_count": 4, "duration_seconds": 0.326}, "timestamp": "2026-01-25T19:59:05.595045"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 86.21, "latencies_ms": [86.21], "images_per_second": 11.6, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Pedestrian sign 2", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10460.0, "ram_available_mb": 112046.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [34.96], "power_watts_avg": 34.96, "power_watts_peak": 34.96, "energy_joules_est": 3.03, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T19:59:05.701338"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 140.193, "latencies_ms": [140.193], "images_per_second": 7.133, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA man in a blue shirt stands on the sidewalk next to a red car.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10460.6, "ram_available_mb": 112045.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [34.96, 34.96], "power_watts_avg": 34.96, "power_watts_peak": 34.96, "energy_joules_est": 4.92, "sample_count": 2, "duration_seconds": 0.141}, "timestamp": "2026-01-25T19:59:05.907285"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 293.593, "latencies_ms": [293.593], "images_per_second": 3.406, "prompt_tokens": 757, "response_tokens_est": 49, "n_tiles": 1, "output_text": "\nIn a city, there are two people standing on a sidewalk next to a street sign. A red car is parked nearby, and another person can be seen further down the road. The traffic light is green for cars but not bicycles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.6, "ram_available_mb": 112045.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [34.96, 31.66, 31.66], "power_watts_avg": 32.76, "power_watts_peak": 34.96, "energy_joules_est": 9.63, "sample_count": 3, "duration_seconds": 0.294}, "timestamp": "2026-01-25T19:59:06.212897"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 310.474, "latencies_ms": [310.474], "images_per_second": 3.221, "prompt_tokens": 756, "response_tokens_est": 52, "n_tiles": 1, "output_text": "\n The image shows a man standing on the sidewalk next to a street sign. There are other people in the scene as well, including one person walking and another riding a bicycle. A car is parked nearby, adding to the urban setting of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10462.0, "ram_available_mb": 112044.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [31.66, 31.66, 31.66, 32.28], "power_watts_avg": 31.82, "power_watts_peak": 32.28, "energy_joules_est": 9.89, "sample_count": 4, "duration_seconds": 0.311}, "timestamp": "2026-01-25T19:59:06.618586"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 260.87, "latencies_ms": [260.87], "images_per_second": 3.833, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "urns of gold and silver are displayed on a stone bench in an outdoor area, with two people standing nearby.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 10462.0, "ram_available_mb": 112044.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [32.28, 32.28, 32.28], "power_watts_avg": 32.28, "power_watts_peak": 32.28, "energy_joules_est": 8.43, "sample_count": 3, "duration_seconds": 0.261}, "timestamp": "2026-01-25T19:59:06.931768"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 123.8, "latencies_ms": [123.8], "images_per_second": 8.078, "prompt_tokens": 759, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Statue of two women sitting on a bench - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.8, "ram_available_mb": 112046.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10454.0, "ram_available_mb": 112052.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_watts_samples": [32.28, 36.62], "power_watts_avg": 34.45, "power_watts_peak": 36.62, "energy_joules_est": 4.28, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T19:59:07.138082"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 175.856, "latencies_ms": [175.856], "images_per_second": 5.686, "prompt_tokens": 763, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA bronze statue of a woman sitting on a bench with her arms crossed is in front of another statue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.0, "ram_available_mb": 112052.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10452.7, "ram_available_mb": 112053.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_watts_samples": [36.62, 36.62], "power_watts_avg": 36.62, "power_watts_peak": 36.62, "energy_joules_est": 6.45, "sample_count": 2, "duration_seconds": 0.176}, "timestamp": "2026-01-25T19:59:07.342974"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 153.041, "latencies_ms": [153.041], "images_per_second": 6.534, "prompt_tokens": 757, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nA bronze statue of two women sitting on a bench, with one woman holding a purse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.7, "ram_available_mb": 112053.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10453.7, "ram_available_mb": 112052.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_watts_samples": [36.62, 36.62], "power_watts_avg": 36.62, "power_watts_peak": 36.62, "energy_joules_est": 5.61, "sample_count": 2, "duration_seconds": 0.153}, "timestamp": "2026-01-25T19:59:07.549866"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 281.417, "latencies_ms": [281.417], "images_per_second": 3.553, "prompt_tokens": 756, "response_tokens_est": 45, "n_tiles": 1, "output_text": "\n The image features a bronze statue of two women sitting on a bench. One woman is wearing a hat and the other has her hands crossed over her chest. There are also bags placed next to them on the bench.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.7, "ram_available_mb": 112052.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10444.7, "ram_available_mb": 112061.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 72.0}, "power_stats": {"power_watts_samples": [31.98, 31.98, 31.98], "power_watts_avg": 31.98, "power_watts_peak": 31.98, "energy_joules_est": 9.01, "sample_count": 3, "duration_seconds": 0.282}, "timestamp": "2026-01-25T19:59:07.855910"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 437.275, "latencies_ms": [437.275], "images_per_second": 2.287, "prompt_tokens": 744, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\nA street sign is mounted on a pole, displaying directions to various locations in Germany and Austria. The signs include \"Severins-br\u00fccke\", \"Koeinmeise\", \"P\", \"Im Sionstal\", and \"No entry\".", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10444.7, "ram_available_mb": 112061.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10448.6, "ram_available_mb": 112057.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [31.98, 31.98, 32.28, 32.28, 32.28], "power_watts_avg": 32.16, "power_watts_peak": 32.28, "energy_joules_est": 14.07, "sample_count": 5, "duration_seconds": 0.438}, "timestamp": "2026-01-25T19:59:08.370721"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 223.815, "latencies_ms": [223.815], "images_per_second": 4.468, "prompt_tokens": 759, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\n 1. Severins-br\u00fccke 2. K\u00f6lmess 3. Koelnmesse 4. Im sionstal 5. P", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.6, "ram_available_mb": 112057.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10445.1, "ram_available_mb": 112061.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [32.28, 32.28, 36.05], "power_watts_avg": 33.54, "power_watts_peak": 36.05, "energy_joules_est": 7.52, "sample_count": 3, "duration_seconds": 0.224}, "timestamp": "2026-01-25T19:59:08.676510"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 148.631, "latencies_ms": [148.631], "images_per_second": 6.728, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n 1. A blue and white sign with a green arrow pointing to the right.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 10445.1, "ram_available_mb": 112061.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10436.4, "ram_available_mb": 112069.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [36.05, 36.05], "power_watts_avg": 36.05, "power_watts_peak": 36.05, "energy_joules_est": 5.37, "sample_count": 2, "duration_seconds": 0.149}, "timestamp": "2026-01-25T19:59:08.880683"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 482.05, "latencies_ms": [482.05], "images_per_second": 2.074, "prompt_tokens": 757, "response_tokens_est": 83, "n_tiles": 1, "output_text": "\nThe image shows a street sign with multiple signs attached to it, including one for a truck. The signs are in German and point towards different destinations such as \"Severins-br\u00fccke\", \"Koeinmeise\", \"P\", \"Im Sionstal\", and an arrow pointing left. In the background, there is a tree visible on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10436.4, "ram_available_mb": 112069.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10434.4, "ram_available_mb": 112071.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 14.0}, "power_stats": {"power_watts_samples": [36.05, 36.05, 32.56, 32.56, 32.56], "power_watts_avg": 33.96, "power_watts_peak": 36.05, "energy_joules_est": 16.38, "sample_count": 5, "duration_seconds": 0.482}, "timestamp": "2026-01-25T19:59:09.389854"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 617.514, "latencies_ms": [617.514], "images_per_second": 1.619, "prompt_tokens": 756, "response_tokens_est": 111, "n_tiles": 1, "output_text": "\nThe image shows a street sign with multiple signs attached to it. The signs include one that says \"Severins-br\u00fccke\" and another that says \"Koeinmeise\". There are also two other signs in the scene, one of which is red and white while the other has blue and yellow colors.\n\nThe street sign is located on a pole near some trees, with a cloudy sky above it. The presence of these signs indicates that this location might be in Germany or another country where German signage is common.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10434.4, "ram_available_mb": 112071.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10438.1, "ram_available_mb": 112068.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.56, 32.56, 33.14, 33.14, 33.14, 33.14, 33.14], "power_watts_avg": 32.97, "power_watts_peak": 33.14, "energy_joules_est": 20.37, "sample_count": 7, "duration_seconds": 0.618}, "timestamp": "2026-01-25T19:59:10.095477"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 256.425, "latencies_ms": [256.425], "images_per_second": 3.9, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nTwo young women stand in a train station, with one holding a suitcase and the other wearing a blue shirt.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 10438.1, "ram_available_mb": 112068.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10435.1, "ram_available_mb": 112071.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [40.29, 40.29, 40.29], "power_watts_avg": 40.29, "power_watts_peak": 40.29, "energy_joules_est": 10.36, "sample_count": 3, "duration_seconds": 0.257}, "timestamp": "2026-01-25T19:59:10.406068"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 496.605, "latencies_ms": [496.605], "images_per_second": 2.014, "prompt_tokens": 759, "response_tokens_est": 86, "n_tiles": 1, "output_text": "\n 1. Luggage bag 2. Luggage bag 3. Luggage bag 4. Luggage bag 5. Luggage bag 6. Luggage bag 7. Luggage bag 8. Luggage bag 9. Luggage bag 10. Luggage bag 11. Luggage bag 12. Luggage bag 13. Luggage bag 14. Luggage bag 15. Luggage bag 16. Luggage bag 17.0.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10435.1, "ram_available_mb": 112071.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10435.1, "ram_available_mb": 112071.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [40.29, 38.41, 38.41, 38.41, 38.41], "power_watts_avg": 38.78, "power_watts_peak": 40.29, "energy_joules_est": 19.27, "sample_count": 5, "duration_seconds": 0.497}, "timestamp": "2026-01-25T19:59:10.911283"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 172.632, "latencies_ms": [172.632], "images_per_second": 5.793, "prompt_tokens": 763, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA woman in a red shirt and another woman wearing a blue shirt are standing next to each other with their luggage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10435.1, "ram_available_mb": 112071.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10438.1, "ram_available_mb": 112068.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [38.41, 37.03], "power_watts_avg": 37.72, "power_watts_peak": 38.41, "energy_joules_est": 6.52, "sample_count": 2, "duration_seconds": 0.173}, "timestamp": "2026-01-25T19:59:11.115722"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 158.941, "latencies_ms": [158.941], "images_per_second": 6.292, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nTwo young women stand next to a black suitcase, one wearing a red shirt and holding a ticket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10438.1, "ram_available_mb": 112068.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10440.8, "ram_available_mb": 112065.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [37.03, 37.03], "power_watts_avg": 37.03, "power_watts_peak": 37.03, "energy_joules_est": 5.91, "sample_count": 2, "duration_seconds": 0.16}, "timestamp": "2026-01-25T19:59:11.321546"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 312.14, "latencies_ms": [312.14], "images_per_second": 3.204, "prompt_tokens": 756, "response_tokens_est": 53, "n_tiles": 1, "output_text": "\n The image shows two women standing next to a suitcase. One woman is wearing a red shirt and the other has on a blue shirt. They are both smiling at the camera. The suitcase they are holding appears to be black with a floral design on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10440.8, "ram_available_mb": 112065.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10443.2, "ram_available_mb": 112063.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [37.03, 37.03, 36.2, 36.2], "power_watts_avg": 36.61, "power_watts_peak": 37.03, "energy_joules_est": 11.44, "sample_count": 4, "duration_seconds": 0.312}, "timestamp": "2026-01-25T19:59:11.727542"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 277.212, "latencies_ms": [277.212], "images_per_second": 3.607, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nThree zebras with brown and white stripes are walking together in a line, moving towards the right side of the frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10443.2, "ram_available_mb": 112063.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10438.9, "ram_available_mb": 112067.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [36.2, 36.2, 36.2], "power_watts_avg": 36.2, "power_watts_peak": 36.2, "energy_joules_est": 10.06, "sample_count": 3, "duration_seconds": 0.278}, "timestamp": "2026-01-25T19:59:12.039298"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 261.303, "latencies_ms": [261.303], "images_per_second": 3.827, "prompt_tokens": 759, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\n 1. Tree trunk  2. Tree trunk  3. Tree trunk  4. Tree trunk  5. Tree trunk  6. Tree trunk  7. Tree trunk  8. Tree trunk", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10438.9, "ram_available_mb": 112067.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10434.1, "ram_available_mb": 112072.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [36.17, 36.17, 36.17], "power_watts_avg": 36.17, "power_watts_peak": 36.17, "energy_joules_est": 9.47, "sample_count": 3, "duration_seconds": 0.262}, "timestamp": "2026-01-25T19:59:12.344906"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 136.511, "latencies_ms": [136.511], "images_per_second": 7.325, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n 1. Zebra in front of other two zebras.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10434.1, "ram_available_mb": 112072.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10430.6, "ram_available_mb": 112075.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [36.17, 36.17], "power_watts_avg": 36.17, "power_watts_peak": 36.17, "energy_joules_est": 4.96, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T19:59:12.551020"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 159.253, "latencies_ms": [159.253], "images_per_second": 6.279, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n Three zebras are walking in a line through a field of trees, with purple flowers nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10430.8, "ram_available_mb": 112075.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10432.4, "ram_available_mb": 112073.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [35.95, 35.95], "power_watts_avg": 35.95, "power_watts_peak": 35.95, "energy_joules_est": 5.73, "sample_count": 2, "duration_seconds": 0.159}, "timestamp": "2026-01-25T19:59:12.757393"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 159.536, "latencies_ms": [159.536], "images_per_second": 6.268, "prompt_tokens": 756, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n The zebras are standing in a line and the background is filled with purple flowers.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10432.4, "ram_available_mb": 112073.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10434.7, "ram_available_mb": 112071.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [35.95, 35.95], "power_watts_avg": 35.95, "power_watts_peak": 35.95, "energy_joules_est": 5.76, "sample_count": 2, "duration_seconds": 0.16}, "timestamp": "2026-01-25T19:59:12.963071"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 363.808, "latencies_ms": [363.808], "images_per_second": 2.749, "prompt_tokens": 744, "response_tokens_est": 42, "n_tiles": 1, "output_text": "\nA black tripod stands on a gray floor, with a laptop and camera attached to it. A white chair is positioned in front of the tripod, and a vending machine can be seen in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10434.5, "ram_available_mb": 112071.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10432.7, "ram_available_mb": 112073.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [35.95, 29.04, 29.04, 29.04], "power_watts_avg": 30.77, "power_watts_peak": 35.95, "energy_joules_est": 11.21, "sample_count": 4, "duration_seconds": 0.364}, "timestamp": "2026-01-25T19:59:13.371951"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 94.137, "latencies_ms": [94.137], "images_per_second": 10.623, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Chair - 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10432.7, "ram_available_mb": 112073.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10431.7, "ram_available_mb": 112074.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [29.04], "power_watts_avg": 29.04, "power_watts_peak": 29.04, "energy_joules_est": 2.74, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T19:59:13.477016"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 115.654, "latencies_ms": [115.654], "images_per_second": 8.646, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. A black tripod with a camera on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10431.7, "ram_available_mb": 112074.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10435.2, "ram_available_mb": 112071.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [29.04, 32.55], "power_watts_avg": 30.79, "power_watts_peak": 32.55, "energy_joules_est": 3.57, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T19:59:13.682060"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 152.361, "latencies_ms": [152.361], "images_per_second": 6.563, "prompt_tokens": 757, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n A tripod with a camera mounted on top stands in an empty room, accompanied by a laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10435.2, "ram_available_mb": 112071.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10432.4, "ram_available_mb": 112073.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [32.55, 32.55], "power_watts_avg": 32.55, "power_watts_peak": 32.55, "energy_joules_est": 4.97, "sample_count": 2, "duration_seconds": 0.153}, "timestamp": "2026-01-25T19:59:13.887371"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 101.129, "latencies_ms": [101.129], "images_per_second": 9.888, "prompt_tokens": 756, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n A black and silver camera on a tripod.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10432.4, "ram_available_mb": 112073.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10435.7, "ram_available_mb": 112070.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 24.0}, "power_stats": {"power_watts_samples": [32.55, 32.55], "power_watts_avg": 32.55, "power_watts_peak": 32.55, "energy_joules_est": 3.3, "sample_count": 2, "duration_seconds": 0.101}, "timestamp": "2026-01-25T19:59:14.093296"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 303.809, "latencies_ms": [303.809], "images_per_second": 3.292, "prompt_tokens": 744, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA white sheep stands in a pen, surrounded by hay and grass. The sheep is positioned near a wire fence that separates it from other animals or people.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10435.7, "ram_available_mb": 112070.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10437.7, "ram_available_mb": 112068.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 24.0}, "power_stats": {"power_watts_samples": [33.08, 33.08, 33.08, 33.08], "power_watts_avg": 33.08, "power_watts_peak": 33.08, "energy_joules_est": 10.07, "sample_count": 4, "duration_seconds": 0.304}, "timestamp": "2026-01-25T19:59:14.504064"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 92.254, "latencies_ms": [92.254], "images_per_second": 10.84, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Sheep: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10437.7, "ram_available_mb": 112068.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10436.9, "ram_available_mb": 112069.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 24.0}, "power_stats": {"power_watts_samples": [33.08], "power_watts_avg": 33.08, "power_watts_peak": 33.08, "energy_joules_est": 3.06, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T19:59:14.608334"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 114.643, "latencies_ms": [114.643], "images_per_second": 8.723, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nA white sheep is standing in front of a fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10436.9, "ram_available_mb": 112069.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10437.9, "ram_available_mb": 112068.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 24.0}, "power_stats": {"power_watts_samples": [30.22, 30.22], "power_watts_avg": 30.22, "power_watts_peak": 30.22, "energy_joules_est": 3.47, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T19:59:14.813826"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 213.236, "latencies_ms": [213.236], "images_per_second": 4.69, "prompt_tokens": 757, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA sheep with a white face stands in front of a wire fence, eating some hay. The sheep appears to be looking at something off-camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10437.9, "ram_available_mb": 112068.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10427.0, "ram_available_mb": 112079.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [30.22, 30.22, 30.71], "power_watts_avg": 30.38, "power_watts_peak": 30.71, "energy_joules_est": 6.49, "sample_count": 3, "duration_seconds": 0.214}, "timestamp": "2026-01-25T19:59:15.120013"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 307.109, "latencies_ms": [307.109], "images_per_second": 3.256, "prompt_tokens": 756, "response_tokens_est": 50, "n_tiles": 1, "output_text": "\n A white sheep with a black face is standing in front of a fence. The sheep has its head down and appears to be eating something on the ground. There are two piles of wool - one pile is gray and the other is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10427.0, "ram_available_mb": 112079.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10428.6, "ram_available_mb": 112077.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [30.71, 30.71, 30.71, 30.71], "power_watts_avg": 30.71, "power_watts_peak": 30.71, "energy_joules_est": 9.44, "sample_count": 4, "duration_seconds": 0.307}, "timestamp": "2026-01-25T19:59:15.525731"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 304.659, "latencies_ms": [304.659], "images_per_second": 3.282, "prompt_tokens": 744, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA woman in a pink dress is playing tennis, holding her racket and preparing to hit an incoming ball on a blue court with spectators in the stands.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10428.6, "ram_available_mb": 112077.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10419.1, "ram_available_mb": 112087.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [28.3, 28.3, 28.3, 28.3], "power_watts_avg": 28.3, "power_watts_peak": 28.3, "energy_joules_est": 8.65, "sample_count": 4, "duration_seconds": 0.306}, "timestamp": "2026-01-25T19:59:15.935156"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 100.204, "latencies_ms": [100.204], "images_per_second": 9.98, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Racket in the air", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10419.1, "ram_available_mb": 112087.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10418.2, "ram_available_mb": 112088.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [28.3, 32.45], "power_watts_avg": 30.38, "power_watts_peak": 32.45, "energy_joules_est": 3.06, "sample_count": 2, "duration_seconds": 0.101}, "timestamp": "2026-01-25T19:59:16.140720"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 122.436, "latencies_ms": [122.436], "images_per_second": 8.168, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nThe woman in a pink dress is playing tennis.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10418.2, "ram_available_mb": 112088.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10418.5, "ram_available_mb": 112087.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [32.45, 32.45], "power_watts_avg": 32.45, "power_watts_peak": 32.45, "energy_joules_est": 4.0, "sample_count": 2, "duration_seconds": 0.123}, "timestamp": "2026-01-25T19:59:16.346015"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 363.571, "latencies_ms": [363.571], "images_per_second": 2.75, "prompt_tokens": 757, "response_tokens_est": 60, "n_tiles": 1, "output_text": "\nIn this image, a woman in a pink dress is playing tennis on a blue court. She is holding a tennis racket and appears to be in motion as she prepares for her next move. The court is surrounded by spectators who are watching the match intently from various positions around the court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10418.5, "ram_available_mb": 112087.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10412.0, "ram_available_mb": 112094.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [32.45, 32.45, 29.31, 29.31], "power_watts_avg": 30.88, "power_watts_peak": 32.45, "energy_joules_est": 11.26, "sample_count": 4, "duration_seconds": 0.364}, "timestamp": "2026-01-25T19:59:16.752857"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 480.686, "latencies_ms": [480.686], "images_per_second": 2.08, "prompt_tokens": 756, "response_tokens_est": 84, "n_tiles": 1, "output_text": "\n The image shows a tennis match in progress with two players on the court. One player is wearing red and white while the other is dressed in purple. They are both holding tennis rackets as they engage in the game. A crowd of spectators can be seen watching from various positions around the court, including some sitting on chairs placed near the sidelines. The scene captures a moment of intense competition between the two players.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10412.0, "ram_available_mb": 112094.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10413.5, "ram_available_mb": 112092.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [29.31, 29.31, 29.31, 30.78, 30.78], "power_watts_avg": 29.9, "power_watts_peak": 30.78, "energy_joules_est": 14.39, "sample_count": 5, "duration_seconds": 0.481}, "timestamp": "2026-01-25T19:59:17.259484"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 246.28, "latencies_ms": [246.28], "images_per_second": 4.06, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA person in a black coat is walking through an airport terminal, pulling a suitcase behind them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10413.5, "ram_available_mb": 112092.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10417.7, "ram_available_mb": 112088.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [30.78, 30.78, 30.78], "power_watts_avg": 30.78, "power_watts_peak": 30.78, "energy_joules_est": 7.59, "sample_count": 3, "duration_seconds": 0.247}, "timestamp": "2026-01-25T19:59:17.570092"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 73.927, "latencies_ms": [73.927], "images_per_second": 13.527, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Escalator", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10417.7, "ram_available_mb": 112088.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10419.8, "ram_available_mb": 112086.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [40.38], "power_watts_avg": 40.38, "power_watts_peak": 40.38, "energy_joules_est": 3.0, "sample_count": 1, "duration_seconds": 0.074}, "timestamp": "2026-01-25T19:59:17.674394"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 121.67, "latencies_ms": [121.67], "images_per_second": 8.219, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nA man with a suitcase is walking through an airport terminal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10419.8, "ram_available_mb": 112086.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10421.5, "ram_available_mb": 112084.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [40.38, 40.38], "power_watts_avg": 40.38, "power_watts_peak": 40.38, "energy_joules_est": 4.92, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T19:59:17.879132"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 213.429, "latencies_ms": [213.429], "images_per_second": 4.685, "prompt_tokens": 757, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA person in a black coat walks through an open door, carrying a suitcase. The doorway leads to another room or hallway with a staircase nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10421.5, "ram_available_mb": 112084.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10420.5, "ram_available_mb": 112085.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_watts_samples": [40.38, 40.38, 34.15], "power_watts_avg": 38.31, "power_watts_peak": 40.38, "energy_joules_est": 8.19, "sample_count": 3, "duration_seconds": 0.214}, "timestamp": "2026-01-25T19:59:18.185851"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 333.23, "latencies_ms": [333.23], "images_per_second": 3.001, "prompt_tokens": 756, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\n The image shows a person walking with luggage in an airport terminal. The person is wearing a black coat and carrying a suitcase on their shoulder. The scene takes place during the day under natural light, which illuminates the area and creates shadows that add depth to the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10420.5, "ram_available_mb": 112085.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10421.9, "ram_available_mb": 112084.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_watts_samples": [34.15, 34.15, 34.15, 34.15], "power_watts_avg": 34.15, "power_watts_peak": 34.15, "energy_joules_est": 11.4, "sample_count": 4, "duration_seconds": 0.334}, "timestamp": "2026-01-25T19:59:18.591282"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 284.09, "latencies_ms": [284.09], "images_per_second": 3.52, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA table is set with two pizzas, a glass of wine and water, and silverware including forks and knives.", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 10421.9, "ram_available_mb": 112084.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10422.6, "ram_available_mb": 112083.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_watts_samples": [29.5, 29.5, 29.5], "power_watts_avg": 29.5, "power_watts_peak": 29.5, "energy_joules_est": 8.4, "sample_count": 3, "duration_seconds": 0.285}, "timestamp": "2026-01-25T19:59:18.903687"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 364.184, "latencies_ms": [364.184], "images_per_second": 2.746, "prompt_tokens": 759, "response_tokens_est": 61, "n_tiles": 1, "output_text": "\n 1. Pizza box  2. Glass of water  3. Glass of soda  4. Glass of wine  5. Glass of beer  6. Glass of juice  7. Glass of milk  8. Glass of tea  9. Glass of coffee  10. Glass of red wine", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10422.6, "ram_available_mb": 112083.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10423.8, "ram_available_mb": 112082.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [29.5, 29.5, 35.33, 35.33], "power_watts_avg": 32.42, "power_watts_peak": 35.33, "energy_joules_est": 11.82, "sample_count": 4, "duration_seconds": 0.365}, "timestamp": "2026-01-25T19:59:19.310080"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 130.784, "latencies_ms": [130.784], "images_per_second": 7.646, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n 1. A pizza box with a pizza on top of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10423.8, "ram_available_mb": 112082.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10420.9, "ram_available_mb": 112085.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [35.33, 35.33], "power_watts_avg": 35.33, "power_watts_peak": 35.33, "energy_joules_est": 4.63, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T19:59:19.516420"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 468.935, "latencies_ms": [468.935], "images_per_second": 2.132, "prompt_tokens": 757, "response_tokens_est": 83, "n_tiles": 1, "output_text": "\nThe image shows a table with two pizzas, one of which has been cut into slices. There are also glasses on the table, suggesting that people may be enjoying drinks along with their meal. A TV can be seen in the background, indicating that this gathering might take place at home or another comfortable setting. The scene appears to be set for a casual and enjoyable dining experience among friends or family members.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10420.9, "ram_available_mb": 112085.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10420.9, "ram_available_mb": 112085.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.37, 35.37, 35.37, 35.37, 35.37], "power_watts_avg": 35.37, "power_watts_peak": 35.37, "energy_joules_est": 16.59, "sample_count": 5, "duration_seconds": 0.469}, "timestamp": "2026-01-25T19:59:20.022211"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 46.922, "latencies_ms": [46.922], "images_per_second": 21.312, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10420.9, "ram_available_mb": 112085.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10421.1, "ram_available_mb": 112085.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.32], "power_watts_avg": 36.32, "power_watts_peak": 36.32, "energy_joules_est": 1.72, "sample_count": 1, "duration_seconds": 0.047}, "timestamp": "2026-01-25T19:59:20.127040"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 372.003, "latencies_ms": [372.003], "images_per_second": 2.688, "prompt_tokens": 744, "response_tokens_est": 45, "n_tiles": 1, "output_text": "\nA young boy in a white shirt and black helmet stands at home plate, holding a baseball bat and ready to swing. Behind him, another player is crouched down with his glove outstretched, prepared for action.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10421.1, "ram_available_mb": 112085.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10426.5, "ram_available_mb": 112079.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.32, 36.32, 36.32, 36.32], "power_watts_avg": 36.32, "power_watts_peak": 36.32, "energy_joules_est": 13.52, "sample_count": 4, "duration_seconds": 0.372}, "timestamp": "2026-01-25T19:59:20.537756"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 69.221, "latencies_ms": [69.221], "images_per_second": 14.446, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Tree trunk", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10426.5, "ram_available_mb": 112079.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10431.3, "ram_available_mb": 112075.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.74], "power_watts_avg": 37.74, "power_watts_peak": 37.74, "energy_joules_est": 2.63, "sample_count": 1, "duration_seconds": 0.07}, "timestamp": "2026-01-25T19:59:20.643290"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 131.611, "latencies_ms": [131.611], "images_per_second": 7.598, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Batter boy in white shirt and black helmet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10431.3, "ram_available_mb": 112075.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10432.5, "ram_available_mb": 112073.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.74, 37.74], "power_watts_avg": 37.74, "power_watts_peak": 37.74, "energy_joules_est": 4.99, "sample_count": 2, "duration_seconds": 0.132}, "timestamp": "2026-01-25T19:59:20.848569"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 319.757, "latencies_ms": [319.757], "images_per_second": 3.127, "prompt_tokens": 757, "response_tokens_est": 52, "n_tiles": 1, "output_text": "\nIn a park, two children are playing baseball. One child is holding a bat and standing at home plate, while another child in a catcher's mitt is crouched behind him. A crowd of spectators watches from the sidelines as the game unfolds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10432.5, "ram_available_mb": 112073.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10427.9, "ram_available_mb": 112078.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [37.74, 37.74, 31.47, 31.47], "power_watts_avg": 34.61, "power_watts_peak": 37.74, "energy_joules_est": 11.09, "sample_count": 4, "duration_seconds": 0.32}, "timestamp": "2026-01-25T19:59:21.254787"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 317.531, "latencies_ms": [317.531], "images_per_second": 3.149, "prompt_tokens": 756, "response_tokens_est": 51, "n_tiles": 1, "output_text": "\n The image shows a young boy in a baseball uniform standing at home plate with his bat. A catcher is crouched behind him, ready to catch the ball if needed. Spectators are watching from various positions around the field and on benches nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10428.1, "ram_available_mb": 112078.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10432.7, "ram_available_mb": 112073.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [31.47, 31.47, 31.47, 30.17], "power_watts_avg": 31.15, "power_watts_peak": 31.47, "energy_joules_est": 9.91, "sample_count": 4, "duration_seconds": 0.318}, "timestamp": "2026-01-25T19:59:21.660888"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 287.724, "latencies_ms": [287.724], "images_per_second": 3.476, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA black telephone with a banana attached to it sits on a desk, accompanied by a piece of paper and a stapler.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10432.7, "ram_available_mb": 112073.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10433.5, "ram_available_mb": 112072.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [30.17, 30.17, 30.17], "power_watts_avg": 30.17, "power_watts_peak": 30.17, "energy_joules_est": 8.69, "sample_count": 3, "duration_seconds": 0.288}, "timestamp": "2026-01-25T19:59:21.968273"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 96.218, "latencies_ms": [96.218], "images_per_second": 10.393, "prompt_tokens": 759, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Banana (0.44)", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10433.5, "ram_available_mb": 112072.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 10433.7, "ram_available_mb": 112072.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [30.17], "power_watts_avg": 30.17, "power_watts_peak": 30.17, "energy_joules_est": 2.91, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T19:59:22.070349"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 91.198, "latencies_ms": [91.198], "images_per_second": 10.965, "prompt_tokens": 763, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n A banana is on a telephone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10433.7, "ram_available_mb": 112072.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10434.7, "ram_available_mb": 112071.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [36.07], "power_watts_avg": 36.07, "power_watts_peak": 36.07, "energy_joules_est": 3.3, "sample_count": 1, "duration_seconds": 0.091}, "timestamp": "2026-01-25T19:59:22.174128"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 327.679, "latencies_ms": [327.679], "images_per_second": 3.052, "prompt_tokens": 757, "response_tokens_est": 54, "n_tiles": 1, "output_text": "\nA black telephone with a banana attached to its cord sits on a desk. The phone displays \"RADIO\" in white letters, indicating that it's an old-fashioned radio. A piece of paper is also present on the desk next to the phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10434.7, "ram_available_mb": 112071.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10439.3, "ram_available_mb": 112067.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [36.07, 36.07, 36.07, 36.07], "power_watts_avg": 36.07, "power_watts_peak": 36.07, "energy_joules_est": 11.83, "sample_count": 4, "duration_seconds": 0.328}, "timestamp": "2026-01-25T19:59:22.579435"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 128.584, "latencies_ms": [128.584], "images_per_second": 7.777, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n The image shows a black telephone with a banana attached to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10439.2, "ram_available_mb": 112067.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10445.2, "ram_available_mb": 112061.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [36.28, 36.28], "power_watts_avg": 36.28, "power_watts_peak": 36.28, "energy_joules_est": 4.69, "sample_count": 2, "duration_seconds": 0.129}, "timestamp": "2026-01-25T19:59:22.786131"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 150.809, "latencies_ms": [150.809], "images_per_second": 6.631, "prompt_tokens": 744, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.2, "ram_available_mb": 112061.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10443.5, "ram_available_mb": 112062.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [36.28, 36.28], "power_watts_avg": 36.28, "power_watts_peak": 36.28, "energy_joules_est": 5.49, "sample_count": 2, "duration_seconds": 0.151}, "timestamp": "2026-01-25T19:59:22.995215"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 96.62, "latencies_ms": [96.62], "images_per_second": 10.35, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Teddy bear: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10443.5, "ram_available_mb": 112062.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10443.5, "ram_available_mb": 112062.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [36.28], "power_watts_avg": 36.28, "power_watts_peak": 36.28, "energy_joules_est": 3.53, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T19:59:23.099767"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 61.428, "latencies_ms": [61.428], "images_per_second": 16.279, "prompt_tokens": 763, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10443.5, "ram_available_mb": 112062.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10444.0, "ram_available_mb": 112062.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [32.06], "power_watts_avg": 32.06, "power_watts_peak": 32.06, "energy_joules_est": 1.97, "sample_count": 1, "duration_seconds": 0.062}, "timestamp": "2026-01-25T19:59:23.204262"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 341.653, "latencies_ms": [341.653], "images_per_second": 2.927, "prompt_tokens": 757, "response_tokens_est": 55, "n_tiles": 1, "output_text": "\nA large group of people are gathered in a public space, with some standing near each other. One person has a teddy bear on their shoulder, adding to the lively atmosphere. The crowd consists of individuals wearing various clothing styles and carrying backpacks or handbags.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10444.0, "ram_available_mb": 112062.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10444.5, "ram_available_mb": 112061.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [32.06, 32.06, 32.06, 32.06], "power_watts_avg": 32.06, "power_watts_peak": 32.06, "energy_joules_est": 10.97, "sample_count": 4, "duration_seconds": 0.342}, "timestamp": "2026-01-25T19:59:23.610119"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 58.64, "latencies_ms": [58.64], "images_per_second": 17.053, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10444.5, "ram_available_mb": 112061.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10448.9, "ram_available_mb": 112057.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [29.93], "power_watts_avg": 29.93, "power_watts_peak": 29.93, "energy_joules_est": 1.77, "sample_count": 1, "duration_seconds": 0.059}, "timestamp": "2026-01-25T19:59:23.715296"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 257.946, "latencies_ms": [257.946], "images_per_second": 3.877, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urn of water sits on a table, with a man and child standing nearby as they interact with the horse.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 10448.9, "ram_available_mb": 112057.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10457.2, "ram_available_mb": 112049.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [29.93, 29.93, 29.93], "power_watts_avg": 29.93, "power_watts_peak": 29.93, "energy_joules_est": 7.74, "sample_count": 3, "duration_seconds": 0.259}, "timestamp": "2026-01-25T19:59:24.026259"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 84.933, "latencies_ms": [84.933], "images_per_second": 11.774, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Horse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.2, "ram_available_mb": 112049.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [32.26], "power_watts_avg": 32.26, "power_watts_peak": 32.26, "energy_joules_est": 2.78, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T19:59:24.131319"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 110.984, "latencies_ms": [110.984], "images_per_second": 9.01, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nA man holding a baby is petting a horse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10459.7, "ram_available_mb": 112046.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [32.26, 32.26], "power_watts_avg": 32.26, "power_watts_peak": 32.26, "energy_joules_est": 3.6, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T19:59:24.337921"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 237.118, "latencies_ms": [237.118], "images_per_second": 4.217, "prompt_tokens": 757, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nA man in a red shirt holds his child while they interact with a brown horse. The man gently pets the horse's nose, and the baby looks up at him with curiosity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.7, "ram_available_mb": 112046.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10460.4, "ram_available_mb": 112045.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [32.26, 32.26, 30.69], "power_watts_avg": 31.73, "power_watts_peak": 32.26, "energy_joules_est": 7.53, "sample_count": 3, "duration_seconds": 0.237}, "timestamp": "2026-01-25T19:59:24.644059"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 102.329, "latencies_ms": [102.329], "images_per_second": 9.772, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urns of water and a brick wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.4, "ram_available_mb": 112045.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10462.1, "ram_available_mb": 112044.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [30.69, 30.69], "power_watts_avg": 30.69, "power_watts_peak": 30.69, "energy_joules_est": 3.15, "sample_count": 2, "duration_seconds": 0.103}, "timestamp": "2026-01-25T19:59:24.849873"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 311.362, "latencies_ms": [311.362], "images_per_second": 3.212, "prompt_tokens": 744, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nA white plate with a brown rim holds a banana and a small amount of peanut butter, creating an interesting contrast between the fruit's natural color and the creamy spread.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 10462.1, "ram_available_mb": 112044.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [30.69, 30.69, 29.42, 29.42], "power_watts_avg": 30.06, "power_watts_peak": 30.69, "energy_joules_est": 9.38, "sample_count": 4, "duration_seconds": 0.312}, "timestamp": "2026-01-25T19:59:25.259394"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 81.467, "latencies_ms": [81.467], "images_per_second": 12.275, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Banana and peanut butter", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10459.2, "ram_available_mb": 112047.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [29.42], "power_watts_avg": 29.42, "power_watts_peak": 29.42, "energy_joules_est": 2.41, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T19:59:25.365015"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 89.072, "latencies_ms": [89.072], "images_per_second": 11.227, "prompt_tokens": 763, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\nThe banana is on a plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.2, "ram_available_mb": 112047.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10459.2, "ram_available_mb": 112047.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [29.42], "power_watts_avg": 29.42, "power_watts_peak": 29.42, "energy_joules_est": 2.63, "sample_count": 1, "duration_seconds": 0.089}, "timestamp": "2026-01-25T19:59:25.469279"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 342.735, "latencies_ms": [342.735], "images_per_second": 2.918, "prompt_tokens": 757, "response_tokens_est": 59, "n_tiles": 1, "output_text": "\nIn the image, there is a white plate with a banana on top of it. The banana appears to be partially eaten or half-eaten, indicating that someone has been enjoying this snack. The plate is placed on a wooden table, which adds warmth and texture to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.2, "ram_available_mb": 112047.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [29.42, 30.44, 30.44, 30.44], "power_watts_avg": 30.19, "power_watts_peak": 30.44, "energy_joules_est": 10.35, "sample_count": 4, "duration_seconds": 0.343}, "timestamp": "2026-01-25T19:59:25.874332"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 336.558, "latencies_ms": [336.558], "images_per_second": 2.971, "prompt_tokens": 756, "response_tokens_est": 59, "n_tiles": 1, "output_text": "\n The image shows a white plate with a banana and some peanut butter on it. The banana is yellow in color, while the peanut butter appears to be brownish-yellow. The background of the image is blurred, making it difficult to determine the exact colors or details of the surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [30.44, 30.44, 32.28, 32.28], "power_watts_avg": 31.36, "power_watts_peak": 32.28, "energy_joules_est": 10.56, "sample_count": 4, "duration_seconds": 0.337}, "timestamp": "2026-01-25T19:59:26.281108"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 247.613, "latencies_ms": [247.613], "images_per_second": 4.039, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA man is kneeling on a sidewalk, working on a motorcycle wheel with tools in his hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [32.28, 32.28, 32.28], "power_watts_avg": 32.28, "power_watts_peak": 32.28, "energy_joules_est": 8.02, "sample_count": 3, "duration_seconds": 0.248}, "timestamp": "2026-01-25T19:59:26.591468"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 301.732, "latencies_ms": [301.732], "images_per_second": 3.314, "prompt_tokens": 759, "response_tokens_est": 49, "n_tiles": 1, "output_text": "\n 1. Motorcycle wheel  2. Motorcycle wheel  3. Motorcycle wheel  4. Motorcycle wheel  5. Motorcycle wheel  6. Motorcycle wheel  7. Motorcycle wheel  8. Motorcycle wheel", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10466.4, "ram_available_mb": 112039.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [38.96, 38.96, 38.96, 38.96], "power_watts_avg": 38.96, "power_watts_peak": 38.96, "energy_joules_est": 11.77, "sample_count": 4, "duration_seconds": 0.302}, "timestamp": "2026-01-25T19:59:26.997543"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 92.323, "latencies_ms": [92.323], "images_per_second": 10.832, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n A man is repairing a bicycle wheel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [38.96], "power_watts_avg": 38.96, "power_watts_peak": 38.96, "energy_joules_est": 3.61, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T19:59:27.101683"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 248.992, "latencies_ms": [248.992], "images_per_second": 4.016, "prompt_tokens": 757, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nA man in a green shirt is working on a motorcycle, crouching down to adjust or fix something. The motorcycle is parked next to him, with another bike visible behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10467.9, "ram_available_mb": 112038.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [35.66, 35.66, 35.66], "power_watts_avg": 35.66, "power_watts_peak": 35.66, "energy_joules_est": 8.88, "sample_count": 3, "duration_seconds": 0.249}, "timestamp": "2026-01-25T19:59:27.407099"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 118.543, "latencies_ms": [118.543], "images_per_second": 8.436, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The man is wearing a green shirt and blue pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.9, "ram_available_mb": 112038.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [35.66, 30.13], "power_watts_avg": 32.9, "power_watts_peak": 35.66, "energy_joules_est": 3.91, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T19:59:27.612800"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 289.935, "latencies_ms": [289.935], "images_per_second": 3.449, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA skateboarder in a black shirt and pants is performing a trick on a ramp, with his arms outstretched for balance.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.13, 30.13, 30.13], "power_watts_avg": 30.13, "power_watts_peak": 30.13, "energy_joules_est": 8.75, "sample_count": 3, "duration_seconds": 0.29}, "timestamp": "2026-01-25T19:59:27.922588"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 195.142, "latencies_ms": [195.142], "images_per_second": 5.124, "prompt_tokens": 759, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\n 1 Skateboarder in black shirt and jeans performing a trick, 2 skateboards on the ground, 3 trees in background", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10465.0, "ram_available_mb": 112041.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [30.13, 33.27], "power_watts_avg": 31.7, "power_watts_peak": 33.27, "energy_joules_est": 6.19, "sample_count": 2, "duration_seconds": 0.195}, "timestamp": "2026-01-25T19:59:28.127624"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 177.326, "latencies_ms": [177.326], "images_per_second": 5.639, "prompt_tokens": 763, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\n 1. Skateboarder in black shirt and jeans riding a skateboard on top of a ramp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.0, "ram_available_mb": 112041.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [33.27, 33.27], "power_watts_avg": 33.27, "power_watts_peak": 33.27, "energy_joules_est": 5.91, "sample_count": 2, "duration_seconds": 0.177}, "timestamp": "2026-01-25T19:59:28.332024"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 170.774, "latencies_ms": [170.774], "images_per_second": 5.856, "prompt_tokens": 757, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA skateboarder in a black shirt and jeans performs a trick on his skateboard at a skate park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10454.4, "ram_available_mb": 112051.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [33.27, 33.27], "power_watts_avg": 33.27, "power_watts_peak": 33.27, "energy_joules_est": 5.69, "sample_count": 2, "duration_seconds": 0.171}, "timestamp": "2026-01-25T19:59:28.538016"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 302.825, "latencies_ms": [302.825], "images_per_second": 3.302, "prompt_tokens": 756, "response_tokens_est": 48, "n_tiles": 1, "output_text": "\n The image shows a skateboarder performing a trick on the side of a ramp. The skateboarder is wearing black clothing and has long dreadlocks. The background features a building with large windows that reflect light onto the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.4, "ram_available_mb": 112051.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10454.1, "ram_available_mb": 112052.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [34.96, 34.96, 34.96, 34.96], "power_watts_avg": 34.96, "power_watts_peak": 34.96, "energy_joules_est": 10.61, "sample_count": 4, "duration_seconds": 0.304}, "timestamp": "2026-01-25T19:59:28.945084"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 357.646, "latencies_ms": [357.646], "images_per_second": 2.796, "prompt_tokens": 744, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\nA group of four people, two men and two women, are posing for a photo on a grassy field at sunset. Each person is holding up a white frisbee with blue designs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.1, "ram_available_mb": 112052.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10454.0, "ram_available_mb": 112052.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [34.96, 31.22, 31.22, 31.22], "power_watts_avg": 32.16, "power_watts_peak": 34.96, "energy_joules_est": 11.52, "sample_count": 4, "duration_seconds": 0.358}, "timestamp": "2026-01-25T19:59:29.355013"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 95.4, "latencies_ms": [95.4], "images_per_second": 10.482, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Frisbee: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.0, "ram_available_mb": 112052.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [31.22], "power_watts_avg": 31.22, "power_watts_peak": 31.22, "energy_joules_est": 2.99, "sample_count": 1, "duration_seconds": 0.096}, "timestamp": "2026-01-25T19:59:29.459638"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 140.265, "latencies_ms": [140.265], "images_per_second": 7.129, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n 1. Person in blue shirt and white headband holding a frisbee.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [31.22, 33.84], "power_watts_avg": 32.53, "power_watts_peak": 33.84, "energy_joules_est": 4.57, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T19:59:29.663723"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 286.997, "latencies_ms": [286.997], "images_per_second": 3.484, "prompt_tokens": 757, "response_tokens_est": 49, "n_tiles": 1, "output_text": "\nA group of four people, two men and two women, are standing on a grassy field with frisbees in their hands. The sky above them is blue, indicating that they might be at sunset or during a warm day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [33.84, 33.84, 33.84], "power_watts_avg": 33.84, "power_watts_peak": 33.84, "energy_joules_est": 9.72, "sample_count": 3, "duration_seconds": 0.287}, "timestamp": "2026-01-25T19:59:29.969251"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 237.899, "latencies_ms": [237.899], "images_per_second": 4.203, "prompt_tokens": 756, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\n The image shows a group of four people posing for a photo on the grass. They are holding frisbees in their hands and appear to be enjoying themselves outdoors during sunset.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10458.5, "ram_available_mb": 112047.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10458.3, "ram_available_mb": 112048.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [33.84, 36.34, 36.34], "power_watts_avg": 35.5, "power_watts_peak": 36.34, "energy_joules_est": 8.46, "sample_count": 3, "duration_seconds": 0.238}, "timestamp": "2026-01-25T19:59:30.274683"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 288.473, "latencies_ms": [288.473], "images_per_second": 3.467, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA large white airplane with red and black accents is parked at an airport, facing towards a runway. The plane has Japanese writing on its side.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10458.3, "ram_available_mb": 112048.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [36.34, 36.34, 36.34], "power_watts_avg": 36.34, "power_watts_peak": 36.34, "energy_joules_est": 10.51, "sample_count": 3, "duration_seconds": 0.289}, "timestamp": "2026-01-25T19:59:30.583026"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 80.873, "latencies_ms": [80.873], "images_per_second": 12.365, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Airplane", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10451.8, "ram_available_mb": 112054.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [37.22], "power_watts_avg": 37.22, "power_watts_peak": 37.22, "energy_joules_est": 3.03, "sample_count": 1, "duration_seconds": 0.081}, "timestamp": "2026-01-25T19:59:30.688459"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 93.244, "latencies_ms": [93.244], "images_per_second": 10.725, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\nThe airplane is parked on a runway.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.8, "ram_available_mb": 112054.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10453.5, "ram_available_mb": 112052.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [37.22], "power_watts_avg": 37.22, "power_watts_peak": 37.22, "energy_joules_est": 3.48, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T19:59:30.792255"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 342.569, "latencies_ms": [342.569], "images_per_second": 2.919, "prompt_tokens": 757, "response_tokens_est": 60, "n_tiles": 1, "output_text": "\nA large white airplane with red accents is parked at an airport, surrounded by people walking around. The plane has a Japanese logo on its side, indicating that it belongs to Japan Airlines. There are several people in the vicinity of the airplane, some carrying luggage or standing near the tarmac.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.5, "ram_available_mb": 112052.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10455.3, "ram_available_mb": 112051.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.22, 37.22, 37.22, 37.04], "power_watts_avg": 37.18, "power_watts_peak": 37.22, "energy_joules_est": 12.74, "sample_count": 4, "duration_seconds": 0.343}, "timestamp": "2026-01-25T19:59:31.197622"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 86.723, "latencies_ms": [86.723], "images_per_second": 11.531, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The airplane is white and red.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.3, "ram_available_mb": 112051.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10455.3, "ram_available_mb": 112051.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.04], "power_watts_avg": 37.04, "power_watts_peak": 37.04, "energy_joules_est": 3.23, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T19:59:31.302133"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 297.867, "latencies_ms": [297.867], "images_per_second": 3.357, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA skateboarder in a yellow shirt and brown pants performs a trick on a concrete ramp, with graffiti-covered walls surrounding them.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10455.3, "ram_available_mb": 112051.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10458.9, "ram_available_mb": 112047.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.04, 37.04, 35.27], "power_watts_avg": 36.45, "power_watts_peak": 37.04, "energy_joules_est": 10.86, "sample_count": 3, "duration_seconds": 0.298}, "timestamp": "2026-01-25T19:59:31.613861"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 160.813, "latencies_ms": [160.813], "images_per_second": 6.218, "prompt_tokens": 759, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\n 1. Skateboarder in yellow shirt and brown pants riding a skateboard down a ramp at the park", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.9, "ram_available_mb": 112047.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10456.8, "ram_available_mb": 112049.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.27, 35.27], "power_watts_avg": 35.27, "power_watts_peak": 35.27, "energy_joules_est": 5.68, "sample_count": 2, "duration_seconds": 0.161}, "timestamp": "2026-01-25T19:59:31.818343"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 145.449, "latencies_ms": [145.449], "images_per_second": 6.875, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n 1. Skateboarder on a ramp in front of a trash can and bench.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.8, "ram_available_mb": 112049.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [35.27, 35.27], "power_watts_avg": 35.27, "power_watts_peak": 35.27, "energy_joules_est": 5.15, "sample_count": 2, "duration_seconds": 0.146}, "timestamp": "2026-01-25T19:59:32.024055"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 170.587, "latencies_ms": [170.587], "images_per_second": 5.862, "prompt_tokens": 757, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA skateboarder in a yellow shirt performs a trick on a concrete ramp, with graffiti-covered walls surrounding them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [35.24, 35.24], "power_watts_avg": 35.24, "power_watts_peak": 35.24, "energy_joules_est": 6.03, "sample_count": 2, "duration_seconds": 0.171}, "timestamp": "2026-01-25T19:59:32.230142"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 127.488, "latencies_ms": [127.488], "images_per_second": 7.844, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n The skateboarder is wearing a yellow shirt and brown pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [35.24, 35.24], "power_watts_avg": 35.24, "power_watts_peak": 35.24, "energy_joules_est": 4.5, "sample_count": 2, "duration_seconds": 0.128}, "timestamp": "2026-01-25T19:59:32.437433"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 293.703, "latencies_ms": [293.703], "images_per_second": 3.405, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "iced chocolate cake with white stripes on a plate, accompanied by caramel sauce.\n\n2. Exercise: What is the main ingredient in this dessert?", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [35.24, 30.74, 30.74], "power_watts_avg": 32.24, "power_watts_peak": 35.24, "energy_joules_est": 9.48, "sample_count": 3, "duration_seconds": 0.294}, "timestamp": "2026-01-25T19:59:32.748489"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 81.803, "latencies_ms": [81.803], "images_per_second": 12.225, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Chocolate cake slice on plate", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [30.74], "power_watts_avg": 30.74, "power_watts_peak": 30.74, "energy_joules_est": 2.52, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T19:59:32.852949"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 100.55, "latencies_ms": [100.55], "images_per_second": 9.945, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\nThe chocolate cake is on a plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10457.4, "ram_available_mb": 112048.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [30.74], "power_watts_avg": 30.74, "power_watts_peak": 30.74, "energy_joules_est": 3.1, "sample_count": 1, "duration_seconds": 0.101}, "timestamp": "2026-01-25T19:59:32.959083"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 358.227, "latencies_ms": [358.227], "images_per_second": 2.792, "prompt_tokens": 757, "response_tokens_est": 63, "n_tiles": 1, "output_text": "\nThe image shows a slice of chocolate cake on a white plate with gold trim. The cake has a drizzle of caramel sauce on top, adding to its visual appeal. The plate is placed on a dining table, suggesting that the scene takes place in a home or restaurant setting where people enjoy desserts together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.4, "ram_available_mb": 112048.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10455.9, "ram_available_mb": 112050.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [30.74, 33.01, 33.01, 33.01], "power_watts_avg": 32.44, "power_watts_peak": 33.01, "energy_joules_est": 11.63, "sample_count": 4, "duration_seconds": 0.358}, "timestamp": "2026-01-25T19:59:33.364827"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 315.089, "latencies_ms": [315.089], "images_per_second": 3.174, "prompt_tokens": 756, "response_tokens_est": 53, "n_tiles": 1, "output_text": "\n The image shows a slice of chocolate cake on a white plate with gold trim. The cake has a rich brown color and is topped with caramel sauce drizzled over it. The plate is placed on a wooden table, which adds warmth to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.9, "ram_available_mb": 112050.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10454.8, "ram_available_mb": 112051.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [33.01, 33.01, 37.42, 37.42], "power_watts_avg": 35.22, "power_watts_peak": 37.42, "energy_joules_est": 11.11, "sample_count": 4, "duration_seconds": 0.315}, "timestamp": "2026-01-25T19:59:33.771337"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 231.002, "latencies_ms": [231.002], "images_per_second": 4.329, "prompt_tokens": 744, "response_tokens_est": 18, "n_tiles": 1, "output_text": "urns of water on a desk, with two people working in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.8, "ram_available_mb": 112051.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [37.42, 37.42, 37.42], "power_watts_avg": 37.42, "power_watts_peak": 37.42, "energy_joules_est": 8.66, "sample_count": 3, "duration_seconds": 0.232}, "timestamp": "2026-01-25T19:59:34.082805"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 701.323, "latencies_ms": [701.323], "images_per_second": 1.426, "prompt_tokens": 759, "response_tokens_est": 128, "n_tiles": 1, "output_text": "\n 1. Chair 2. Desk 3. Desk 4. Desk 5. Chair 6. Chair 7. Chair 8. Chair 9. Chair 10. Chair 11. Chair 12. Chair 13. Chair 14. Chair 15. Chair 16. Chair 17. Chair 18. Chair 19. Chair 20. Chair 21. Chair 22. Chair 23. Chair 24. Chair 25. Chair 26. Chair 27. Chair 28. Chair 29. Chair 30. Chair 31. Chair 32. Chair 33. Chair 34. Chair 35. Chair 36. Chair 37. Chair 38. Chair 39. Chair 40. Chair 41. Chair 42. Chair 43", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10459.8, "ram_available_mb": 112046.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [38.47, 38.47, 38.47, 38.47, 38.47, 37.94, 37.94], "power_watts_avg": 38.32, "power_watts_peak": 38.47, "energy_joules_est": 26.89, "sample_count": 7, "duration_seconds": 0.702}, "timestamp": "2026-01-25T19:59:34.788369"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 171.616, "latencies_ms": [171.616], "images_per_second": 5.827, "prompt_tokens": 763, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\n 1. Man in tan shirt sitting at a desk with laptop computer and other people working on their own laptops.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 10459.8, "ram_available_mb": 112046.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10459.8, "ram_available_mb": 112046.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [37.94, 37.94], "power_watts_avg": 37.94, "power_watts_peak": 37.94, "energy_joules_est": 6.52, "sample_count": 2, "duration_seconds": 0.172}, "timestamp": "2026-01-25T19:59:34.993691"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 139.054, "latencies_ms": [139.054], "images_per_second": 7.191, "prompt_tokens": 757, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nA group of people are working on computers in a room with yellow walls.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.8, "ram_available_mb": 112046.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10460.3, "ram_available_mb": 112046.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [37.94, 36.41], "power_watts_avg": 37.17, "power_watts_peak": 37.94, "energy_joules_est": 5.18, "sample_count": 2, "duration_seconds": 0.139}, "timestamp": "2026-01-25T19:59:35.200378"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 539.046, "latencies_ms": [539.046], "images_per_second": 1.855, "prompt_tokens": 756, "response_tokens_est": 97, "n_tiles": 1, "output_text": "\n The image shows a group of people working in an office with multiple computers and laptops. There are at least six visible computer screens on desks throughout the room. Some chairs are occupied by individuals who appear to be focused on their work. A few other objects can also be seen in the scene, such as a cup placed near one of the desks and a bottle located close to another desk. The lighting appears to be bright, suggesting that it is daytime or well-lit indoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.3, "ram_available_mb": 112046.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10446.9, "ram_available_mb": 112059.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [36.41, 36.41, 36.41, 36.41, 35.59, 35.59], "power_watts_avg": 36.14, "power_watts_peak": 36.41, "energy_joules_est": 19.49, "sample_count": 6, "duration_seconds": 0.539}, "timestamp": "2026-01-25T19:59:35.807355"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 286.304, "latencies_ms": [286.304], "images_per_second": 3.493, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA group of people are playing a video game in a living room, with two men standing on opposite sides of the room and holding controllers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.9, "ram_available_mb": 112059.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10446.3, "ram_available_mb": 112060.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.59, 35.59, 37.87], "power_watts_avg": 36.35, "power_watts_peak": 37.87, "energy_joules_est": 10.42, "sample_count": 3, "duration_seconds": 0.287}, "timestamp": "2026-01-25T19:59:36.120452"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 70.711, "latencies_ms": [70.711], "images_per_second": 14.142, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Couch - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.3, "ram_available_mb": 112060.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10449.9, "ram_available_mb": 112056.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [37.87], "power_watts_avg": 37.87, "power_watts_peak": 37.87, "energy_joules_est": 2.69, "sample_count": 1, "duration_seconds": 0.071}, "timestamp": "2026-01-25T19:59:36.226445"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 150.576, "latencies_ms": [150.576], "images_per_second": 6.641, "prompt_tokens": 763, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n 1. A man in a green shirt playing wii with two other men standing behind him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.9, "ram_available_mb": 112056.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10441.7, "ram_available_mb": 112064.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [37.87, 37.87], "power_watts_avg": 37.87, "power_watts_peak": 37.87, "energy_joules_est": 5.72, "sample_count": 2, "duration_seconds": 0.151}, "timestamp": "2026-01-25T19:59:36.432682"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 349.818, "latencies_ms": [349.818], "images_per_second": 2.859, "prompt_tokens": 757, "response_tokens_est": 60, "n_tiles": 1, "output_text": "\nA group of people are playing a video game in a living room, with two men standing on opposite sides of the room. One man is holding a Wii remote while another man stands next to him. The other players are also engaged in the game, creating an enjoyable atmosphere for everyone present.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10441.7, "ram_available_mb": 112064.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10444.8, "ram_available_mb": 112061.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [37.87, 35.77, 35.77, 35.77], "power_watts_avg": 36.29, "power_watts_peak": 37.87, "energy_joules_est": 12.7, "sample_count": 4, "duration_seconds": 0.35}, "timestamp": "2026-01-25T19:59:36.841217"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 441.331, "latencies_ms": [441.331], "images_per_second": 2.266, "prompt_tokens": 756, "response_tokens_est": 78, "n_tiles": 1, "output_text": "\n The image shows a group of people playing video games in a living room. There are four people standing and two sitting on the couch watching as they play Wii. One person is holding a remote control while another one has a drink in their hand. The room appears to be well-lit, with natural light coming from the windows. A chair can also be seen near the couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10444.8, "ram_available_mb": 112061.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10445.5, "ram_available_mb": 112060.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.77, 35.77, 33.31, 33.31, 33.31], "power_watts_avg": 34.3, "power_watts_peak": 35.77, "energy_joules_est": 15.16, "sample_count": 5, "duration_seconds": 0.442}, "timestamp": "2026-01-25T19:59:37.348933"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 255.162, "latencies_ms": [255.162], "images_per_second": 3.919, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA person stands on a beach, facing away from the camera and holding a frisbee in their hand.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10445.5, "ram_available_mb": 112060.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.31, 33.31, 39.21], "power_watts_avg": 35.28, "power_watts_peak": 39.21, "energy_joules_est": 9.04, "sample_count": 3, "duration_seconds": 0.256}, "timestamp": "2026-01-25T19:59:37.662618"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 92.903, "latencies_ms": [92.903], "images_per_second": 10.764, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Person in the foreground", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10448.7, "ram_available_mb": 112057.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [39.21], "power_watts_avg": 39.21, "power_watts_peak": 39.21, "energy_joules_est": 3.65, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T19:59:37.766570"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 97.089, "latencies_ms": [97.089], "images_per_second": 10.3, "prompt_tokens": 763, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n Person in front of sun.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.7, "ram_available_mb": 112057.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10442.8, "ram_available_mb": 112063.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [39.21], "power_watts_avg": 39.21, "power_watts_peak": 39.21, "energy_joules_est": 3.82, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T19:59:37.871794"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 146.96, "latencies_ms": [146.96], "images_per_second": 6.805, "prompt_tokens": 757, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA person stands on a beach at sunset, with the sun setting behind them.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10442.8, "ram_available_mb": 112063.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10447.6, "ram_available_mb": 112058.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [39.21, 39.21], "power_watts_avg": 39.21, "power_watts_peak": 39.21, "energy_joules_est": 5.77, "sample_count": 2, "duration_seconds": 0.147}, "timestamp": "2026-01-25T19:59:38.077394"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 115.646, "latencies_ms": [115.646], "images_per_second": 8.647, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n The sun is setting over the ocean and casting a warm orange glow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.6, "ram_available_mb": 112058.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10447.6, "ram_available_mb": 112058.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [35.62, 35.62], "power_watts_avg": 35.62, "power_watts_peak": 35.62, "energy_joules_est": 4.13, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T19:59:38.283074"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 258.56, "latencies_ms": [258.56], "images_per_second": 3.868, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urns of flowers sit on a table in front of a window, with two red chairs and a white couch nearby.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10447.6, "ram_available_mb": 112058.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10448.1, "ram_available_mb": 112058.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [35.62, 35.62, 35.62], "power_watts_avg": 35.62, "power_watts_peak": 35.62, "energy_joules_est": 9.24, "sample_count": 3, "duration_seconds": 0.259}, "timestamp": "2026-01-25T19:59:38.597455"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 92.879, "latencies_ms": [92.879], "images_per_second": 10.767, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Couch - 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.1, "ram_available_mb": 112058.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10448.1, "ram_available_mb": 112058.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [32.19], "power_watts_avg": 32.19, "power_watts_peak": 32.19, "energy_joules_est": 3.01, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T19:59:38.703172"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 154.351, "latencies_ms": [154.351], "images_per_second": 6.479, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n A round table with a vase of flowers on it is in front of two chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.1, "ram_available_mb": 112058.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10447.4, "ram_available_mb": 112058.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [32.19, 32.19], "power_watts_avg": 32.19, "power_watts_peak": 32.19, "energy_joules_est": 4.98, "sample_count": 2, "duration_seconds": 0.155}, "timestamp": "2026-01-25T19:59:38.908831"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 548.143, "latencies_ms": [548.143], "images_per_second": 1.824, "prompt_tokens": 757, "response_tokens_est": 96, "n_tiles": 1, "output_text": "\nThe image shows a modern living room with white walls, black and red accents, and various pieces of furniture. The room features a large window that allows natural light to flood in, creating an inviting atmosphere. A television is mounted on the wall, while a coffee table sits nearby. Two chairs are positioned around the dining table, which has a vase of flowers as its centerpiece. The room also contains a couch and several decorative items such as potted plants and paintings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.4, "ram_available_mb": 112058.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10445.6, "ram_available_mb": 112060.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [32.19, 30.78, 30.78, 30.78, 30.78, 30.78], "power_watts_avg": 31.02, "power_watts_peak": 32.19, "energy_joules_est": 17.01, "sample_count": 6, "duration_seconds": 0.548}, "timestamp": "2026-01-25T19:59:39.515821"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 269.097, "latencies_ms": [269.097], "images_per_second": 3.716, "prompt_tokens": 756, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\n The room is painted white and has a black and white rug. There are two red chairs at the table with a vase of flowers on it. A television is mounted on the wall above the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.6, "ram_available_mb": 112060.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10448.8, "ram_available_mb": 112057.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [33.45, 33.45, 33.45], "power_watts_avg": 33.45, "power_watts_peak": 33.45, "energy_joules_est": 9.01, "sample_count": 3, "duration_seconds": 0.269}, "timestamp": "2026-01-25T19:59:39.822014"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 257.259, "latencies_ms": [257.259], "images_per_second": 3.887, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urns on a shelf in a kitchen, with a cat sitting atop them and looking down at its surroundings.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10448.8, "ram_available_mb": 112057.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10452.5, "ram_available_mb": 112053.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [33.45, 33.45, 39.73], "power_watts_avg": 35.54, "power_watts_peak": 39.73, "energy_joules_est": 9.17, "sample_count": 3, "duration_seconds": 0.258}, "timestamp": "2026-01-25T19:59:40.135686"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 101.234, "latencies_ms": [101.234], "images_per_second": 9.878, "prompt_tokens": 759, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Refrigerator  2. Drawer", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.5, "ram_available_mb": 112053.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10451.4, "ram_available_mb": 112054.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [39.73, 39.73], "power_watts_avg": 39.73, "power_watts_peak": 39.73, "energy_joules_est": 4.05, "sample_count": 2, "duration_seconds": 0.102}, "timestamp": "2026-01-25T19:59:40.342643"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 129.34, "latencies_ms": [129.34], "images_per_second": 7.732, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA brown and black cat is sitting on top of a blue refrigerator.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.4, "ram_available_mb": 112054.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10449.7, "ram_available_mb": 112056.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [39.73, 39.73], "power_watts_avg": 39.73, "power_watts_peak": 39.73, "energy_joules_est": 5.15, "sample_count": 2, "duration_seconds": 0.13}, "timestamp": "2026-01-25T19:59:40.549436"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 277.94, "latencies_ms": [277.94], "images_per_second": 3.598, "prompt_tokens": 757, "response_tokens_est": 45, "n_tiles": 1, "output_text": "\nIn a kitchen, there are two refrigerators. One of them has its door open, revealing a cat sitting on top of it. The other refrigerator appears to be closed but is located next to the open one.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.7, "ram_available_mb": 112056.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10453.4, "ram_available_mb": 112052.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [33.17, 33.17, 33.17], "power_watts_avg": 33.17, "power_watts_peak": 33.17, "energy_joules_est": 9.24, "sample_count": 3, "duration_seconds": 0.279}, "timestamp": "2026-01-25T19:59:40.857171"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 393.357, "latencies_ms": [393.357], "images_per_second": 2.542, "prompt_tokens": 756, "response_tokens_est": 68, "n_tiles": 1, "output_text": "\n A brown and white cat is sitting on top of a blue refrigerator. The cat's fur appears to be grayish-brown in color. The room has a tan wall with a light fixture hanging from the ceiling that illuminates the space. There are also some objects placed around the room, including a bowl and a cup.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.4, "ram_available_mb": 112052.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10453.6, "ram_available_mb": 112052.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [33.17, 33.17, 30.12, 30.12], "power_watts_avg": 31.65, "power_watts_peak": 33.17, "energy_joules_est": 12.47, "sample_count": 4, "duration_seconds": 0.394}, "timestamp": "2026-01-25T19:59:41.264002"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 306.009, "latencies_ms": [306.009], "images_per_second": 3.268, "prompt_tokens": 744, "response_tokens_est": 35, "n_tiles": 1, "output_text": "urns of flowers sit on a shelf in a room with yellow walls and white flooring, accompanied by a bed, desk, bookshelf, and various decorations.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10453.6, "ram_available_mb": 112052.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [30.12, 30.12, 30.12, 39.21], "power_watts_avg": 32.39, "power_watts_peak": 39.21, "energy_joules_est": 9.92, "sample_count": 4, "duration_seconds": 0.306}, "timestamp": "2026-01-25T19:59:41.676986"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.401, "latencies_ms": [79.401], "images_per_second": 12.594, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\nBalloon 1: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10460.4, "ram_available_mb": 112045.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [39.21], "power_watts_avg": 39.21, "power_watts_peak": 39.21, "energy_joules_est": 3.12, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T19:59:41.781324"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 101.589, "latencies_ms": [101.589], "images_per_second": 9.844, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\nBalloons are tied to a refrigerator.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.4, "ram_available_mb": 112045.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [39.21, 39.21], "power_watts_avg": 39.21, "power_watts_peak": 39.21, "energy_joules_est": 4.01, "sample_count": 2, "duration_seconds": 0.102}, "timestamp": "2026-01-25T19:59:41.987556"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 424.921, "latencies_ms": [424.921], "images_per_second": 2.353, "prompt_tokens": 757, "response_tokens_est": 74, "n_tiles": 1, "output_text": "\nThe image shows a small, cluttered room with yellow balloons floating in front of a bed. The room has white walls and wooden floors, giving it a cozy atmosphere. A refrigerator can be seen on one side of the room, while a desk sits against another wall. There are also several books scattered around the room, adding to its lived-in feel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [39.21, 34.18, 34.18, 34.18, 34.18], "power_watts_avg": 35.19, "power_watts_peak": 39.21, "energy_joules_est": 14.97, "sample_count": 5, "duration_seconds": 0.425}, "timestamp": "2026-01-25T19:59:42.494108"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 414.594, "latencies_ms": [414.594], "images_per_second": 2.412, "prompt_tokens": 756, "response_tokens_est": 70, "n_tiles": 1, "output_text": "\n The room is filled with balloons in various colors and sizes. There are yellow and blue balloons on the floor as well as white ones hanging from the ceiling. A chandelier hangs above a bed that has a yellow smiley face balloon tied to it. The walls of the room have pictures, and there's a refrigerator against one wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10457.3, "ram_available_mb": 112049.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [34.18, 29.37, 29.37, 29.37, 29.37], "power_watts_avg": 30.33, "power_watts_peak": 34.18, "energy_joules_est": 12.6, "sample_count": 5, "duration_seconds": 0.415}, "timestamp": "2026-01-25T19:59:43.001258"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 184.556, "latencies_ms": [184.556], "images_per_second": 5.418, "prompt_tokens": 744, "response_tokens_est": 8, "n_tiles": 1, "output_text": "urn of water on a stove top", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.3, "ram_available_mb": 112049.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 1.0}, "power_stats": {"power_watts_samples": [29.37, 34.94], "power_watts_avg": 32.16, "power_watts_peak": 34.94, "energy_joules_est": 5.96, "sample_count": 2, "duration_seconds": 0.185}, "timestamp": "2026-01-25T19:59:43.213258"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 96.256, "latencies_ms": [96.256], "images_per_second": 10.389, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Window - 0.17", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10453.1, "ram_available_mb": 112053.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 1.0}, "power_stats": {"power_watts_samples": [34.94], "power_watts_avg": 34.94, "power_watts_peak": 34.94, "energy_joules_est": 3.37, "sample_count": 1, "duration_seconds": 0.096}, "timestamp": "2026-01-25T19:59:43.318608"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 98.604, "latencies_ms": [98.604], "images_per_second": 10.142, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Man with headphones on laptop computer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.1, "ram_available_mb": 112053.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10453.3, "ram_available_mb": 112053.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 1.0}, "power_stats": {"power_watts_samples": [34.94], "power_watts_avg": 34.94, "power_watts_peak": 34.94, "energy_joules_est": 3.45, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T19:59:43.422411"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 167.545, "latencies_ms": [167.545], "images_per_second": 5.969, "prompt_tokens": 757, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA man in a suit is sitting at a table with an open laptop computer, wearing headphones.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.3, "ram_available_mb": 112053.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10451.4, "ram_available_mb": 112054.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 1.0}, "power_stats": {"power_watts_samples": [34.94, 34.75], "power_watts_avg": 34.85, "power_watts_peak": 34.94, "energy_joules_est": 5.85, "sample_count": 2, "duration_seconds": 0.168}, "timestamp": "2026-01-25T19:59:43.627609"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 114.526, "latencies_ms": [114.526], "images_per_second": 8.732, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A man wearing headphones and sitting at a table with an open laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.4, "ram_available_mb": 112054.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10452.4, "ram_available_mb": 112053.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 1.0}, "power_stats": {"power_watts_samples": [34.75, 34.75], "power_watts_avg": 34.75, "power_watts_peak": 34.75, "energy_joules_est": 4.0, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T19:59:43.834422"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 263.621, "latencies_ms": [263.621], "images_per_second": 3.793, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA white train is traveling on a track, with another train visible in the background and a bridge spanning across it.", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 10452.4, "ram_available_mb": 112053.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [34.75, 34.75, 35.4], "power_watts_avg": 34.96, "power_watts_peak": 35.4, "energy_joules_est": 9.23, "sample_count": 3, "duration_seconds": 0.264}, "timestamp": "2026-01-25T19:59:44.148857"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 70.302, "latencies_ms": [70.302], "images_per_second": 14.224, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Bridge", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [35.4], "power_watts_avg": 35.4, "power_watts_peak": 35.4, "energy_joules_est": 2.5, "sample_count": 1, "duration_seconds": 0.071}, "timestamp": "2026-01-25T19:59:44.255391"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 130.169, "latencies_ms": [130.169], "images_per_second": 7.682, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nA white train is on a track next to a bridge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10458.4, "ram_available_mb": 112047.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [35.4, 35.4], "power_watts_avg": 35.4, "power_watts_peak": 35.4, "energy_joules_est": 4.63, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T19:59:44.462458"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 241.352, "latencies_ms": [241.352], "images_per_second": 4.143, "prompt_tokens": 757, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nA white train with a red stripe is traveling on tracks next to a bridge. The train appears to be moving from left to right, while several cars are visible in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10458.4, "ram_available_mb": 112047.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10454.3, "ram_available_mb": 112052.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [35.4, 30.71, 30.71], "power_watts_avg": 32.27, "power_watts_peak": 35.4, "energy_joules_est": 7.8, "sample_count": 3, "duration_seconds": 0.242}, "timestamp": "2026-01-25T19:59:44.769017"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 349.178, "latencies_ms": [349.178], "images_per_second": 2.864, "prompt_tokens": 756, "response_tokens_est": 58, "n_tiles": 1, "output_text": "\n The image shows a train station with two trains on the tracks. One of the trains is silver and black, while the other one has a white body. They are both passing under a bridge that spans across the scene. The sky above them is blue with some clouds scattered throughout.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.3, "ram_available_mb": 112052.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10455.2, "ram_available_mb": 112051.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [30.71, 30.71, 30.71, 30.46], "power_watts_avg": 30.65, "power_watts_peak": 30.71, "energy_joules_est": 10.72, "sample_count": 4, "duration_seconds": 0.35}, "timestamp": "2026-01-25T19:59:45.177001"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 281.071, "latencies_ms": [281.071], "images_per_second": 3.558, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA large, colorful kite with a dragon design soars in the sky above a green field where people are gathered to fly it.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10455.2, "ram_available_mb": 112051.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10456.6, "ram_available_mb": 112049.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [30.46, 30.46, 30.46], "power_watts_avg": 30.46, "power_watts_peak": 30.46, "energy_joules_est": 8.58, "sample_count": 3, "duration_seconds": 0.282}, "timestamp": "2026-01-25T19:59:45.493206"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 87.256, "latencies_ms": [87.256], "images_per_second": 11.461, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Kite: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.6, "ram_available_mb": 112049.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [30.46], "power_watts_avg": 30.46, "power_watts_peak": 30.46, "energy_joules_est": 2.68, "sample_count": 1, "duration_seconds": 0.088}, "timestamp": "2026-01-25T19:59:45.599599"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 156.989, "latencies_ms": [156.989], "images_per_second": 6.37, "prompt_tokens": 763, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA large kite with a blue and purple tail is flying in front of a group of people.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10463.7, "ram_available_mb": 112042.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.6, 37.6], "power_watts_avg": 37.6, "power_watts_peak": 37.6, "energy_joules_est": 5.91, "sample_count": 2, "duration_seconds": 0.157}, "timestamp": "2026-01-25T19:59:45.806059"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 233.609, "latencies_ms": [233.609], "images_per_second": 4.281, "prompt_tokens": 757, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nA group of people are flying a large, colorful kite in a park. The kite has blue, purple, yellow, and orange colors on its wings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.7, "ram_available_mb": 112042.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10456.5, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [37.6, 37.6, 37.6], "power_watts_avg": 37.6, "power_watts_peak": 37.6, "energy_joules_est": 8.8, "sample_count": 3, "duration_seconds": 0.234}, "timestamp": "2026-01-25T19:59:46.112718"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 271.07, "latencies_ms": [271.07], "images_per_second": 3.689, "prompt_tokens": 756, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\n A large kite with a blue and purple tail is flying in the sky. The kite has orange and yellow colors on its body. There are several people standing around the field enjoying their time outdoors.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10456.5, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.72, 35.72, 35.72], "power_watts_avg": 35.72, "power_watts_peak": 35.72, "energy_joules_est": 9.7, "sample_count": 3, "duration_seconds": 0.271}, "timestamp": "2026-01-25T19:59:46.419933"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 265.083, "latencies_ms": [265.083], "images_per_second": 3.772, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA red and black Virgin Express train is traveling down a track, with several workers standing on the tracks next to it.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.72, 31.92, 31.92], "power_watts_avg": 33.19, "power_watts_peak": 35.72, "energy_joules_est": 8.81, "sample_count": 3, "duration_seconds": 0.266}, "timestamp": "2026-01-25T19:59:46.732846"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 90.365, "latencies_ms": [90.365], "images_per_second": 11.066, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Train car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10465.4, "ram_available_mb": 112040.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.92], "power_watts_avg": 31.92, "power_watts_peak": 31.92, "energy_joules_est": 2.9, "sample_count": 1, "duration_seconds": 0.091}, "timestamp": "2026-01-25T19:59:46.837771"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 128.505, "latencies_ms": [128.505], "images_per_second": 7.782, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nA train is on a track with workers standing next to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.4, "ram_available_mb": 112040.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [31.92, 31.92], "power_watts_avg": 31.92, "power_watts_peak": 31.92, "energy_joules_est": 4.11, "sample_count": 2, "duration_seconds": 0.129}, "timestamp": "2026-01-25T19:59:47.043494"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 241.36, "latencies_ms": [241.36], "images_per_second": 4.143, "prompt_tokens": 757, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\nA red Virgin Trains UK train is traveling down a track, with several workers standing on the tracks next to it. The train appears to be passing through an open field or countryside area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [33.56, 33.56, 33.56], "power_watts_avg": 33.56, "power_watts_peak": 33.56, "energy_joules_est": 8.11, "sample_count": 3, "duration_seconds": 0.242}, "timestamp": "2026-01-25T19:59:47.349111"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 96.413, "latencies_ms": [96.413], "images_per_second": 10.372, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The train is red and black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [33.56], "power_watts_avg": 33.56, "power_watts_peak": 33.56, "energy_joules_est": 3.25, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T19:59:47.453132"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 230.349, "latencies_ms": [230.349], "images_per_second": 4.341, "prompt_tokens": 744, "response_tokens_est": 17, "n_tiles": 1, "output_text": "urn of a cat's fur, with white and brown patches on its back.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [33.56, 31.22, 31.22], "power_watts_avg": 32.0, "power_watts_peak": 33.56, "energy_joules_est": 7.38, "sample_count": 3, "duration_seconds": 0.231}, "timestamp": "2026-01-25T19:59:47.764282"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 73.934, "latencies_ms": [73.934], "images_per_second": 13.526, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Blanket", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10456.0, "ram_available_mb": 112050.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [31.22], "power_watts_avg": 31.22, "power_watts_peak": 31.22, "energy_joules_est": 2.32, "sample_count": 1, "duration_seconds": 0.074}, "timestamp": "2026-01-25T19:59:47.869676"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 138.618, "latencies_ms": [138.618], "images_per_second": 7.214, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nA brown and white cat's fur is visible in a closeup image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.0, "ram_available_mb": 112050.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10456.5, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [31.22, 31.22], "power_watts_avg": 31.22, "power_watts_peak": 31.22, "energy_joules_est": 4.34, "sample_count": 2, "duration_seconds": 0.139}, "timestamp": "2026-01-25T19:59:48.075135"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 159.27, "latencies_ms": [159.27], "images_per_second": 6.279, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA brown and white cat's fur is visible, with a geometric patterned blanket in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.5, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10449.9, "ram_available_mb": 112056.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [32.45, 32.45], "power_watts_avg": 32.45, "power_watts_peak": 32.45, "energy_joules_est": 5.18, "sample_count": 2, "duration_seconds": 0.16}, "timestamp": "2026-01-25T19:59:48.280513"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 91.769, "latencies_ms": [91.769], "images_per_second": 10.897, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n A brown and white cat's fur.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.9, "ram_available_mb": 112056.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10450.4, "ram_available_mb": 112055.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [32.45], "power_watts_avg": 32.45, "power_watts_peak": 32.45, "energy_joules_est": 2.99, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T19:59:48.385309"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 259.687, "latencies_ms": [259.687], "images_per_second": 3.851, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urn of milk with red and white plastic tubes attached to it, indicating a milking machine is in use.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.4, "ram_available_mb": 112055.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10450.4, "ram_available_mb": 112055.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [32.45, 32.45, 34.1], "power_watts_avg": 33.0, "power_watts_peak": 34.1, "energy_joules_est": 8.58, "sample_count": 3, "duration_seconds": 0.26}, "timestamp": "2026-01-25T19:59:48.695390"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 99.384, "latencies_ms": [99.384], "images_per_second": 10.062, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Cow's udders: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.4, "ram_available_mb": 112055.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10453.8, "ram_available_mb": 112052.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [34.1], "power_watts_avg": 34.1, "power_watts_peak": 34.1, "energy_joules_est": 3.41, "sample_count": 1, "duration_seconds": 0.1}, "timestamp": "2026-01-25T19:59:48.799803"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 140.123, "latencies_ms": [140.123], "images_per_second": 7.137, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n 1. A cow with red and white tubes in its udders.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.8, "ram_available_mb": 112052.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10453.1, "ram_available_mb": 112053.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [34.1, 34.1], "power_watts_avg": 34.1, "power_watts_peak": 34.1, "energy_joules_est": 4.79, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T19:59:49.004338"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 287.438, "latencies_ms": [287.438], "images_per_second": 3.479, "prompt_tokens": 757, "response_tokens_est": 48, "n_tiles": 1, "output_text": "\nIn a barn, a cow with white patches on its back stands next to a black object. The cow has red plastic tubes attached to its udders, suggesting that it may be receiving treatment or medication through an automated system.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.1, "ram_available_mb": 112053.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10453.6, "ram_available_mb": 112052.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [34.1, 31.87, 31.87], "power_watts_avg": 32.62, "power_watts_peak": 34.1, "energy_joules_est": 9.39, "sample_count": 3, "duration_seconds": 0.288}, "timestamp": "2026-01-25T19:59:49.309827"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 92.548, "latencies_ms": [92.548], "images_per_second": 10.805, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "urns of red and white color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.6, "ram_available_mb": 112052.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10454.4, "ram_available_mb": 112051.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [31.87], "power_watts_avg": 31.87, "power_watts_peak": 31.87, "energy_joules_est": 2.95, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T19:59:49.413986"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 317.046, "latencies_ms": [317.046], "images_per_second": 3.154, "prompt_tokens": 744, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA white plate holds two peanut butter and jelly sandwiches, with a knife resting on top of them. The sandwiches are cut in half, revealing their fillings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.4, "ram_available_mb": 112051.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [31.87, 34.44, 34.44, 34.44], "power_watts_avg": 33.8, "power_watts_peak": 34.44, "energy_joules_est": 10.74, "sample_count": 4, "duration_seconds": 0.318}, "timestamp": "2026-01-25T19:59:49.824517"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.257, "latencies_ms": [79.257], "images_per_second": 12.617, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Sandwich: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10457.6, "ram_available_mb": 112048.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [34.44], "power_watts_avg": 34.44, "power_watts_peak": 34.44, "energy_joules_est": 2.75, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T19:59:49.929475"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 136.699, "latencies_ms": [136.699], "images_per_second": 7.315, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n 1. A plate with a sandwich on it and a knife beside it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.6, "ram_available_mb": 112048.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10457.8, "ram_available_mb": 112048.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [34.44, 34.4], "power_watts_avg": 34.42, "power_watts_peak": 34.44, "energy_joules_est": 4.72, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T19:59:50.134869"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 370.185, "latencies_ms": [370.185], "images_per_second": 2.701, "prompt_tokens": 757, "response_tokens_est": 64, "n_tiles": 1, "output_text": "\nThe image shows a white plate with two sandwiches on it, one of which has been cut in half. The plate is placed on a green tablecloth, and there are also some peanut butter and jelly spreadable items nearby. A knife can be seen resting next to the plate, ready for use when needed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.8, "ram_available_mb": 112048.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10455.2, "ram_available_mb": 112051.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [34.4, 34.4, 34.4, 34.4], "power_watts_avg": 34.4, "power_watts_peak": 34.4, "energy_joules_est": 12.75, "sample_count": 4, "duration_seconds": 0.371}, "timestamp": "2026-01-25T19:59:50.540368"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 341.986, "latencies_ms": [341.986], "images_per_second": 2.924, "prompt_tokens": 756, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\n The image shows a plate with two sandwiches on it. One of the sandwiches has been cut in half and is placed next to a knife that is resting on top of the plate. The background appears to be dark, which makes the sandwiches stand out more prominently against this backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.2, "ram_available_mb": 112051.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [35.6, 35.6, 35.6, 35.6], "power_watts_avg": 35.6, "power_watts_peak": 35.6, "energy_joules_est": 12.19, "sample_count": 4, "duration_seconds": 0.342}, "timestamp": "2026-01-25T19:59:50.946723"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 442.405, "latencies_ms": [442.405], "images_per_second": 2.26, "prompt_tokens": 744, "response_tokens_est": 62, "n_tiles": 1, "output_text": "\nA purple bento box holds four compartments, each containing a different type of food item. The top left compartment contains a salad with lettuce and tomatoes. The top right compartment has carrots in it. The bottom left compartment features pasta covered in cheese sauce. The bottom right compartment is filled with grapes.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10457.4, "ram_available_mb": 112048.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [35.6, 34.84, 34.84, 34.84, 34.84], "power_watts_avg": 34.99, "power_watts_peak": 35.6, "energy_joules_est": 15.5, "sample_count": 5, "duration_seconds": 0.443}, "timestamp": "2026-01-25T19:59:51.457081"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 96.777, "latencies_ms": [96.777], "images_per_second": 10.333, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Carrots and grapes (1)", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.4, "ram_available_mb": 112048.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [34.84], "power_watts_avg": 34.84, "power_watts_peak": 34.84, "energy_joules_est": 3.39, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T19:59:51.561166"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 153.239, "latencies_ms": [153.239], "images_per_second": 6.526, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n 1. A purple container with a green lid and orange bowl on top of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [37.26, 37.26], "power_watts_avg": 37.26, "power_watts_peak": 37.26, "energy_joules_est": 5.72, "sample_count": 2, "duration_seconds": 0.153}, "timestamp": "2026-01-25T19:59:51.766140"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 193.042, "latencies_ms": [193.042], "images_per_second": 5.18, "prompt_tokens": 757, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA purple bento box holds four compartments filled with a variety of food items, including pasta salad, carrots, grapes, and cheese.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [37.26, 37.26], "power_watts_avg": 37.26, "power_watts_peak": 37.26, "energy_joules_est": 7.2, "sample_count": 2, "duration_seconds": 0.193}, "timestamp": "2026-01-25T19:59:51.970725"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 113.068, "latencies_ms": [113.068], "images_per_second": 8.844, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Purple plastic container with food in it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [37.26, 36.72], "power_watts_avg": 36.99, "power_watts_peak": 37.26, "energy_joules_est": 4.19, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T19:59:52.175788"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 429.716, "latencies_ms": [429.716], "images_per_second": 2.327, "prompt_tokens": 744, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\nThe image features a traffic light with three lights, two of which are red and one is green, hanging from a tree branch above a street scene. The traffic light is surrounded by trees in full bloom, creating a picturesque view for those passing by or standing underneath them.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10450.2, "ram_available_mb": 112056.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [36.72, 36.72, 36.72, 36.72, 35.1], "power_watts_avg": 36.39, "power_watts_peak": 36.72, "energy_joules_est": 15.65, "sample_count": 5, "duration_seconds": 0.43}, "timestamp": "2026-01-25T19:59:52.687204"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.242, "latencies_ms": [79.242], "images_per_second": 12.62, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Traffic light", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.2, "ram_available_mb": 112056.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10453.8, "ram_available_mb": 112052.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [35.1], "power_watts_avg": 35.1, "power_watts_peak": 35.1, "energy_joules_est": 2.8, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T19:59:52.791154"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 131.769, "latencies_ms": [131.769], "images_per_second": 7.589, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. A red traffic light is above a tree with pink flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.8, "ram_available_mb": 112052.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10457.6, "ram_available_mb": 112048.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [35.1, 35.1], "power_watts_avg": 35.1, "power_watts_peak": 35.1, "energy_joules_est": 4.66, "sample_count": 2, "duration_seconds": 0.133}, "timestamp": "2026-01-25T19:59:52.997287"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 222.604, "latencies_ms": [222.604], "images_per_second": 4.492, "prompt_tokens": 757, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA traffic light hangs from a tree branch, with three lights glowing red. The traffic light is located in front of a large tree filled with pink flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.6, "ram_available_mb": 112048.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10454.8, "ram_available_mb": 112051.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [35.1, 32.37, 32.37], "power_watts_avg": 33.28, "power_watts_peak": 35.1, "energy_joules_est": 7.43, "sample_count": 3, "duration_seconds": 0.223}, "timestamp": "2026-01-25T19:59:53.303934"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 200.824, "latencies_ms": [200.824], "images_per_second": 4.979, "prompt_tokens": 756, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\n A red traffic light is visible in the image. The trees are covered with pink and white flowers that create a beautiful contrast against the gray sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.8, "ram_available_mb": 112051.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10453.1, "ram_available_mb": 112053.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [32.37, 32.37], "power_watts_avg": 32.37, "power_watts_peak": 32.37, "energy_joules_est": 6.51, "sample_count": 2, "duration_seconds": 0.201}, "timestamp": "2026-01-25T19:59:53.509657"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 274.434, "latencies_ms": [274.434], "images_per_second": 3.644, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA white plate holds a serving of broccoli and red pepper flakes, with a piece of salmon on the side.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10453.1, "ram_available_mb": 112053.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10469.5, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [29.73, 29.73, 29.73], "power_watts_avg": 29.73, "power_watts_peak": 29.73, "energy_joules_est": 8.17, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T19:59:53.819463"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.97, "latencies_ms": [76.97], "images_per_second": 12.992, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Broccoli", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.5, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [29.73], "power_watts_avg": 29.73, "power_watts_peak": 29.73, "energy_joules_est": 2.31, "sample_count": 1, "duration_seconds": 0.078}, "timestamp": "2026-01-25T19:59:53.925254"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 98.059, "latencies_ms": [98.059], "images_per_second": 10.198, "prompt_tokens": 763, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n A plate of broccoli and fish.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [29.73], "power_watts_avg": 29.73, "power_watts_peak": 29.73, "energy_joules_est": 2.93, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T19:59:54.029210"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 210.875, "latencies_ms": [210.875], "images_per_second": 4.742, "prompt_tokens": 757, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nA white plate holds a serving of broccoli with red pepper flakes, accompanied by a piece of fish. The dish appears to be freshly cooked and ready to be enjoyed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [34.27, 34.27, 34.27], "power_watts_avg": 34.27, "power_watts_peak": 34.27, "energy_joules_est": 7.25, "sample_count": 3, "duration_seconds": 0.211}, "timestamp": "2026-01-25T19:59:54.334740"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 42.49, "latencies_ms": [42.49], "images_per_second": 23.535, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10460.6, "ram_available_mb": 112045.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [34.27], "power_watts_avg": 34.27, "power_watts_peak": 34.27, "energy_joules_est": 1.46, "sample_count": 1, "duration_seconds": 0.043}, "timestamp": "2026-01-25T19:59:54.439912"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 247.785, "latencies_ms": [247.785], "images_per_second": 4.036, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urns of water on a table, with a man and woman sitting at another table in a restaurant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.6, "ram_available_mb": 112045.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10462.6, "ram_available_mb": 112043.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [34.27, 31.82, 31.82], "power_watts_avg": 32.64, "power_watts_peak": 34.27, "energy_joules_est": 8.1, "sample_count": 3, "duration_seconds": 0.248}, "timestamp": "2026-01-25T19:59:54.750628"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 176.734, "latencies_ms": [176.734], "images_per_second": 5.658, "prompt_tokens": 759, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\n 1. Chair in background 2. Chair in front of boy 3. Chair behind the boy 4. Chair behind the man", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.6, "ram_available_mb": 112043.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [31.82, 31.82], "power_watts_avg": 31.82, "power_watts_peak": 31.82, "energy_joules_est": 5.64, "sample_count": 2, "duration_seconds": 0.177}, "timestamp": "2026-01-25T19:59:54.956928"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 126.182, "latencies_ms": [126.182], "images_per_second": 7.925, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nThe boy is eating in front of a woman and another man.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [31.82, 32.59], "power_watts_avg": 32.2, "power_watts_peak": 32.59, "energy_joules_est": 4.07, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T19:59:55.163336"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 138.291, "latencies_ms": [138.291], "images_per_second": 7.231, "prompt_tokens": 757, "response_tokens_est": 19, "n_tiles": 1, "output_text": "urns of water on a table, with people sitting at a booth in a restaurant.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [32.59, 32.59], "power_watts_avg": 32.59, "power_watts_peak": 32.59, "energy_joules_est": 4.52, "sample_count": 2, "duration_seconds": 0.139}, "timestamp": "2026-01-25T19:59:55.368958"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 96.022, "latencies_ms": [96.022], "images_per_second": 10.414, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "urns of red and brown color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10473.7, "ram_available_mb": 112032.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [32.59], "power_watts_avg": 32.59, "power_watts_peak": 32.59, "energy_joules_est": 3.14, "sample_count": 1, "duration_seconds": 0.096}, "timestamp": "2026-01-25T19:59:55.474147"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 316.903, "latencies_ms": [316.903], "images_per_second": 3.156, "prompt_tokens": 744, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nA yellow and white bus is driving down a street, passing by a large building with many windows. The bus is in motion on the right side of the road.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10473.7, "ram_available_mb": 112032.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [32.59, 32.44, 32.44, 32.44], "power_watts_avg": 32.48, "power_watts_peak": 32.59, "energy_joules_est": 10.31, "sample_count": 4, "duration_seconds": 0.317}, "timestamp": "2026-01-25T19:59:55.888834"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 63.699, "latencies_ms": [63.699], "images_per_second": 15.699, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Bus", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [32.44], "power_watts_avg": 32.44, "power_watts_peak": 32.44, "energy_joules_est": 2.1, "sample_count": 1, "duration_seconds": 0.065}, "timestamp": "2026-01-25T19:59:55.994097"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 137.002, "latencies_ms": [137.002], "images_per_second": 7.299, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA white bus is driving down a street next to a tall building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [32.44, 31.39], "power_watts_avg": 31.91, "power_watts_peak": 32.44, "energy_joules_est": 4.39, "sample_count": 2, "duration_seconds": 0.138}, "timestamp": "2026-01-25T19:59:56.199777"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 221.441, "latencies_ms": [221.441], "images_per_second": 4.516, "prompt_tokens": 757, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA yellow bus is driving down a street, passing by tall buildings. The bus appears to be in motion, possibly making its way through an urban area or city center.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [31.39, 31.39, 31.39], "power_watts_avg": 31.39, "power_watts_peak": 31.39, "energy_joules_est": 6.96, "sample_count": 3, "duration_seconds": 0.222}, "timestamp": "2026-01-25T19:59:56.506097"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 83.981, "latencies_ms": [83.981], "images_per_second": 11.907, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The bus is yellow and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10473.2, "ram_available_mb": 112033.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [31.39], "power_watts_avg": 31.39, "power_watts_peak": 31.39, "energy_joules_est": 2.65, "sample_count": 1, "duration_seconds": 0.085}, "timestamp": "2026-01-25T19:59:56.612204"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 265.836, "latencies_ms": [265.836], "images_per_second": 3.762, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA stop sign is mounted on a metal pole in an urban setting, with a building and cars visible in the background.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10473.6, "ram_available_mb": 112032.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10479.6, "ram_available_mb": 112026.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [32.93, 32.93, 32.93], "power_watts_avg": 32.93, "power_watts_peak": 32.93, "energy_joules_est": 8.76, "sample_count": 3, "duration_seconds": 0.266}, "timestamp": "2026-01-25T19:59:56.925775"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 77.251, "latencies_ms": [77.251], "images_per_second": 12.945, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n Stop sign 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.6, "ram_available_mb": 112026.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10480.5, "ram_available_mb": 112025.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [32.93], "power_watts_avg": 32.93, "power_watts_peak": 32.93, "energy_joules_est": 2.55, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T19:59:57.030903"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 121.261, "latencies_ms": [121.261], "images_per_second": 8.247, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA stop sign is located on a pole in front of a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.5, "ram_available_mb": 112025.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10479.8, "ram_available_mb": 112026.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [33.01, 33.01], "power_watts_avg": 33.01, "power_watts_peak": 33.01, "energy_joules_est": 4.01, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T19:59:57.237500"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 271.363, "latencies_ms": [271.363], "images_per_second": 3.685, "prompt_tokens": 757, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nA stop sign stands on a metal pole in an urban area, with a building visible behind it. The sun casts shadows of trees and buildings onto the street below, creating a contrast between light and dark areas.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10479.8, "ram_available_mb": 112026.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [33.01, 33.01, 33.01], "power_watts_avg": 33.01, "power_watts_peak": 33.01, "energy_joules_est": 8.98, "sample_count": 3, "duration_seconds": 0.272}, "timestamp": "2026-01-25T19:59:57.542872"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 104.384, "latencies_ms": [104.384], "images_per_second": 9.58, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n The stop sign is red and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10475.2, "ram_available_mb": 112031.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [34.21, 34.21], "power_watts_avg": 34.21, "power_watts_peak": 34.21, "energy_joules_est": 3.59, "sample_count": 2, "duration_seconds": 0.105}, "timestamp": "2026-01-25T19:59:57.748331"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 259.775, "latencies_ms": [259.775], "images_per_second": 3.849, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA brown and white cat is lying on a black couch, with its paws resting on a computer mouse.", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 10475.2, "ram_available_mb": 112031.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10476.8, "ram_available_mb": 112029.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.21, 34.21, 34.21], "power_watts_avg": 34.21, "power_watts_peak": 34.21, "energy_joules_est": 8.89, "sample_count": 3, "duration_seconds": 0.26}, "timestamp": "2026-01-25T19:59:58.059982"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 83.449, "latencies_ms": [83.449], "images_per_second": 11.983, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Cat: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.8, "ram_available_mb": 112029.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10475.8, "ram_available_mb": 112030.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.85], "power_watts_avg": 32.85, "power_watts_peak": 32.85, "energy_joules_est": 2.76, "sample_count": 1, "duration_seconds": 0.084}, "timestamp": "2026-01-25T19:59:58.165114"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 142.651, "latencies_ms": [142.651], "images_per_second": 7.01, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n 1. A cat laying on a couch with its paw around a computer mouse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.8, "ram_available_mb": 112030.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.85, 32.85], "power_watts_avg": 32.85, "power_watts_peak": 32.85, "energy_joules_est": 4.7, "sample_count": 2, "duration_seconds": 0.143}, "timestamp": "2026-01-25T19:59:58.371837"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 164.771, "latencies_ms": [164.771], "images_per_second": 6.069, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA brown and white cat is lying on a black couch, with its paws around a computer mouse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.85, 32.85], "power_watts_avg": 32.85, "power_watts_peak": 32.85, "energy_joules_est": 5.43, "sample_count": 2, "duration_seconds": 0.165}, "timestamp": "2026-01-25T19:59:58.578762"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 159.474, "latencies_ms": [159.474], "images_per_second": 6.271, "prompt_tokens": 756, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n A cat with green eyes and a brown and white striped coat is laying on the couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10474.4, "ram_available_mb": 112031.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.55, 30.55], "power_watts_avg": 30.55, "power_watts_peak": 30.55, "energy_joules_est": 4.88, "sample_count": 2, "duration_seconds": 0.16}, "timestamp": "2026-01-25T19:59:58.785900"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 266.296, "latencies_ms": [266.296], "images_per_second": 3.755, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA large city street scene features a variety of vehicles, including buses and cars, with buildings in the background.", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 10474.4, "ram_available_mb": 112031.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10477.4, "ram_available_mb": 112028.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [30.55, 30.55, 30.55], "power_watts_avg": 30.55, "power_watts_peak": 30.55, "energy_joules_est": 8.15, "sample_count": 3, "duration_seconds": 0.267}, "timestamp": "2026-01-25T19:59:59.100163"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.94, "latencies_ms": [79.94], "images_per_second": 12.509, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bus: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.4, "ram_available_mb": 112028.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.6], "power_watts_avg": 31.6, "power_watts_peak": 31.6, "energy_joules_est": 2.55, "sample_count": 1, "duration_seconds": 0.081}, "timestamp": "2026-01-25T19:59:59.205916"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 124.175, "latencies_ms": [124.175], "images_per_second": 8.053, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Green bus on left side of road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10474.7, "ram_available_mb": 112031.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.6, 31.6], "power_watts_avg": 31.6, "power_watts_peak": 31.6, "energy_joules_est": 3.93, "sample_count": 2, "duration_seconds": 0.125}, "timestamp": "2026-01-25T19:59:59.412168"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 143.344, "latencies_ms": [143.344], "images_per_second": 6.976, "prompt_tokens": 757, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA busy city street with many buses, including a green bus and white bus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.7, "ram_available_mb": 112031.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.6, 30.81], "power_watts_avg": 31.21, "power_watts_peak": 31.6, "energy_joules_est": 4.49, "sample_count": 2, "duration_seconds": 0.144}, "timestamp": "2026-01-25T19:59:59.617725"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 473.672, "latencies_ms": [473.672], "images_per_second": 2.111, "prompt_tokens": 756, "response_tokens_est": 81, "n_tiles": 1, "output_text": "\n The image shows a city street with several buses and cars. There are two buses in the foreground - one green bus on the left side of the road and another white bus further back to the right. A few cars can also be seen parked or driving along the street. The sky is blue, indicating good weather conditions, and there are buildings visible in the background, suggesting an urban setting.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10468.5, "ram_available_mb": 112037.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.81, 30.81, 30.81, 30.81, 30.39], "power_watts_avg": 30.73, "power_watts_peak": 30.81, "energy_joules_est": 14.56, "sample_count": 5, "duration_seconds": 0.474}, "timestamp": "2026-01-25T20:00:00.123632"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 271.016, "latencies_ms": [271.016], "images_per_second": 3.69, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA man in a cowboy hat is performing a skateboard trick on a ramp, with two large green balloons floating above him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.5, "ram_available_mb": 112037.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.39, 30.39, 30.39], "power_watts_avg": 30.39, "power_watts_peak": 30.39, "energy_joules_est": 8.24, "sample_count": 3, "duration_seconds": 0.271}, "timestamp": "2026-01-25T20:00:00.432819"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 91.994, "latencies_ms": [91.994], "images_per_second": 10.87, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Skateboarder", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10472.0, "ram_available_mb": 112034.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.39], "power_watts_avg": 30.39, "power_watts_peak": 30.39, "energy_joules_est": 2.83, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:00:00.538602"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 164.922, "latencies_ms": [164.922], "images_per_second": 6.063, "prompt_tokens": 763, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n 1. Skateboarder on a ramp with a large green balloon in front of him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.0, "ram_available_mb": 112034.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [37.44, 37.44], "power_watts_avg": 37.44, "power_watts_peak": 37.44, "energy_joules_est": 6.18, "sample_count": 2, "duration_seconds": 0.165}, "timestamp": "2026-01-25T20:00:00.750327"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 249.545, "latencies_ms": [249.545], "images_per_second": 4.007, "prompt_tokens": 757, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\nA man in a cowboy hat is skateboarding on a ramp, performing tricks while wearing shorts. The ramp appears to be made of wood or concrete, with two large green balloons nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10461.4, "ram_available_mb": 112044.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [37.44, 37.44, 37.44], "power_watts_avg": 37.44, "power_watts_peak": 37.44, "energy_joules_est": 9.37, "sample_count": 3, "duration_seconds": 0.25}, "timestamp": "2026-01-25T20:00:01.056829"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 265.041, "latencies_ms": [265.041], "images_per_second": 3.773, "prompt_tokens": 756, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\n The image features a man wearing a cowboy hat and riding a skateboard on top of a ramp. He is performing a trick in front of two large green balloons that are floating in the air behind him.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10461.4, "ram_available_mb": 112044.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10462.2, "ram_available_mb": 112044.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [36.16, 36.16, 36.16], "power_watts_avg": 36.16, "power_watts_peak": 36.16, "energy_joules_est": 9.61, "sample_count": 3, "duration_seconds": 0.266}, "timestamp": "2026-01-25T20:00:01.362518"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 265.655, "latencies_ms": [265.655], "images_per_second": 3.764, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA person is windsurfing on a body of water, surrounded by other people and kites in the sky.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10462.2, "ram_available_mb": 112044.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [36.16, 36.16, 35.33], "power_watts_avg": 35.88, "power_watts_peak": 36.16, "energy_joules_est": 9.57, "sample_count": 3, "duration_seconds": 0.267}, "timestamp": "2026-01-25T20:00:01.672593"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 166.005, "latencies_ms": [166.005], "images_per_second": 6.024, "prompt_tokens": 759, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\n 1. Person in black wetsuit and red shirt standing on surfboard near the water's edge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [35.33, 35.33], "power_watts_avg": 35.33, "power_watts_peak": 35.33, "energy_joules_est": 5.87, "sample_count": 2, "duration_seconds": 0.166}, "timestamp": "2026-01-25T20:00:01.877334"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 142.674, "latencies_ms": [142.674], "images_per_second": 7.009, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n 1. A person parasailing in the ocean with a sail attached to their body.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [35.33, 35.33], "power_watts_avg": 35.33, "power_watts_peak": 35.33, "energy_joules_est": 5.05, "sample_count": 2, "duration_seconds": 0.143}, "timestamp": "2026-01-25T20:00:02.080770"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 291.389, "latencies_ms": [291.389], "images_per_second": 3.432, "prompt_tokens": 757, "response_tokens_est": 48, "n_tiles": 1, "output_text": "\nA person in a wetsuit stands on a surfboard, holding onto a sail while standing in the ocean. The sky above them is blue with white clouds, creating an ideal day for windsurfing or kiteboarding.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [35.42, 35.42, 35.42], "power_watts_avg": 35.42, "power_watts_peak": 35.42, "energy_joules_est": 10.34, "sample_count": 3, "duration_seconds": 0.292}, "timestamp": "2026-01-25T20:00:02.386431"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 155.515, "latencies_ms": [155.515], "images_per_second": 6.43, "prompt_tokens": 756, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n The image features a person windsurfing on the ocean with kites flying in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [35.42, 35.42], "power_watts_avg": 35.42, "power_watts_peak": 35.42, "energy_joules_est": 5.51, "sample_count": 2, "duration_seconds": 0.156}, "timestamp": "2026-01-25T20:00:02.591909"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 254.544, "latencies_ms": [254.544], "images_per_second": 3.929, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "urn with a black top and red body, sitting in front of a house on a green lawn.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10454.3, "ram_available_mb": 112052.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [33.02, 33.02, 33.02], "power_watts_avg": 33.02, "power_watts_peak": 33.02, "energy_joules_est": 8.42, "sample_count": 3, "duration_seconds": 0.255}, "timestamp": "2026-01-25T20:00:02.902277"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 155.245, "latencies_ms": [155.245], "images_per_second": 6.441, "prompt_tokens": 759, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n 1. House  2. Flowers 3. Bush 4. Tree 5. Fire hydrant", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10454.3, "ram_available_mb": 112052.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10447.9, "ram_available_mb": 112058.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [33.02, 33.02], "power_watts_avg": 33.02, "power_watts_peak": 33.02, "energy_joules_est": 5.14, "sample_count": 2, "duration_seconds": 0.156}, "timestamp": "2026-01-25T20:00:03.107966"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 138.139, "latencies_ms": [138.139], "images_per_second": 7.239, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nThe fire hydrant is in front of a house and there are flowers behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.9, "ram_available_mb": 112058.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [34.55, 34.55], "power_watts_avg": 34.55, "power_watts_peak": 34.55, "energy_joules_est": 4.78, "sample_count": 2, "duration_seconds": 0.138}, "timestamp": "2026-01-25T20:00:03.312692"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 143.552, "latencies_ms": [143.552], "images_per_second": 6.966, "prompt_tokens": 757, "response_tokens_est": 18, "n_tiles": 1, "output_text": "urn with a black top sits in front of a house, surrounded by green grass.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10453.3, "ram_available_mb": 112053.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [34.55, 34.55], "power_watts_avg": 34.55, "power_watts_peak": 34.55, "energy_joules_est": 4.97, "sample_count": 2, "duration_seconds": 0.144}, "timestamp": "2026-01-25T20:00:03.520411"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 201.7, "latencies_ms": [201.7], "images_per_second": 4.958, "prompt_tokens": 756, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nThe fire hydrant is red and black with a silver top. It's located in the grass near a house on a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.3, "ram_available_mb": 112053.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [32.05, 32.05, 32.05], "power_watts_avg": 32.05, "power_watts_peak": 32.05, "energy_joules_est": 6.48, "sample_count": 3, "duration_seconds": 0.202}, "timestamp": "2026-01-25T20:00:03.826880"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 390.225, "latencies_ms": [390.225], "images_per_second": 2.563, "prompt_tokens": 744, "response_tokens_est": 49, "n_tiles": 1, "output_text": "\nA small bird is captured in mid-flight, with its wings spread wide as it soars over a blue wooden structure. The bird's body is angled towards the right side of the frame, while its wings are extended outwards.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10446.9, "ram_available_mb": 112059.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [32.05, 32.05, 31.38, 31.38], "power_watts_avg": 31.72, "power_watts_peak": 32.05, "energy_joules_est": 12.39, "sample_count": 4, "duration_seconds": 0.391}, "timestamp": "2026-01-25T20:00:04.239045"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 88.721, "latencies_ms": [88.721], "images_per_second": 11.271, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Bird: 0.5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.9, "ram_available_mb": 112059.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10451.0, "ram_available_mb": 112055.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [31.38], "power_watts_avg": 31.38, "power_watts_peak": 31.38, "energy_joules_est": 2.79, "sample_count": 1, "duration_seconds": 0.089}, "timestamp": "2026-01-25T20:00:04.343704"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 106.807, "latencies_ms": [106.807], "images_per_second": 9.363, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nA bird is flying over a blue roof.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.0, "ram_available_mb": 112055.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10453.8, "ram_available_mb": 112052.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [31.38, 31.38], "power_watts_avg": 31.38, "power_watts_peak": 31.38, "energy_joules_est": 3.36, "sample_count": 2, "duration_seconds": 0.107}, "timestamp": "2026-01-25T20:00:04.549487"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 350.175, "latencies_ms": [350.175], "images_per_second": 2.856, "prompt_tokens": 757, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\nIn a serene blue sky, two birds are captured in mid-flight. One bird is flying towards the left side of the image while the other is flying towards the right side. The birds appear to be enjoying their flight as they soar above some houses with green roofs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.8, "ram_available_mb": 112052.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10454.1, "ram_available_mb": 112052.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [33.86, 33.86, 33.86, 33.86], "power_watts_avg": 33.86, "power_watts_peak": 33.86, "energy_joules_est": 11.87, "sample_count": 4, "duration_seconds": 0.35}, "timestamp": "2026-01-25T20:00:04.956737"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 105.879, "latencies_ms": [105.879], "images_per_second": 9.445, "prompt_tokens": 756, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n A bird is flying over a blue roof.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 10454.1, "ram_available_mb": 112052.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10457.1, "ram_available_mb": 112049.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [33.86, 31.92], "power_watts_avg": 32.89, "power_watts_peak": 33.86, "energy_joules_est": 3.5, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T20:00:05.162921"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 306.658, "latencies_ms": [306.658], "images_per_second": 3.261, "prompt_tokens": 744, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA woman in a gray shirt and blue jeans walks her brown horse through an indoor arena, with a red door visible on the left side of the room.", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 10457.1, "ram_available_mb": 112049.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10464.3, "ram_available_mb": 112042.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [31.92, 31.92, 31.92, 31.92], "power_watts_avg": 31.92, "power_watts_peak": 31.92, "energy_joules_est": 9.81, "sample_count": 4, "duration_seconds": 0.307}, "timestamp": "2026-01-25T20:00:05.577281"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 226.6, "latencies_ms": [226.6], "images_per_second": 4.413, "prompt_tokens": 759, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\n 1. Doorway  2. Ladder  3. Bucket  4. Bucket  5. Bucket  6. Bucket  7. Bucket  8. Bucket", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.3, "ram_available_mb": 112042.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10463.8, "ram_available_mb": 112042.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [33.72, 33.72, 33.72], "power_watts_avg": 33.72, "power_watts_peak": 33.72, "energy_joules_est": 7.66, "sample_count": 3, "duration_seconds": 0.227}, "timestamp": "2026-01-25T20:00:05.883211"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 143.256, "latencies_ms": [143.256], "images_per_second": 6.98, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nA woman in jeans is walking a brown horse inside of an open barn.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.8, "ram_available_mb": 112042.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10461.4, "ram_available_mb": 112044.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 64.0}, "power_stats": {"power_watts_samples": [33.72, 33.72], "power_watts_avg": 33.72, "power_watts_peak": 33.72, "energy_joules_est": 4.85, "sample_count": 2, "duration_seconds": 0.144}, "timestamp": "2026-01-25T20:00:06.090579"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 201.127, "latencies_ms": [201.127], "images_per_second": 4.972, "prompt_tokens": 757, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nIn a barn, a woman in blue jeans walks her brown horse with a red harness. The horse strides confidently across the dirt floor of the barn.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.4, "ram_available_mb": 112044.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10458.0, "ram_available_mb": 112048.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 64.0}, "power_stats": {"power_watts_samples": [32.77, 32.77, 32.77], "power_watts_avg": 32.77, "power_watts_peak": 32.77, "energy_joules_est": 6.61, "sample_count": 3, "duration_seconds": 0.202}, "timestamp": "2026-01-25T20:00:06.397699"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 321.443, "latencies_ms": [321.443], "images_per_second": 3.111, "prompt_tokens": 756, "response_tokens_est": 54, "n_tiles": 1, "output_text": "\n The image shows a woman walking her brown horse in an indoor stable. The horse is wearing a harness and the woman has on blue jeans. They are moving through a room with wooden walls and flooring. There is also a red door visible in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.0, "ram_available_mb": 112048.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 64.0}, "power_stats": {"power_watts_samples": [32.77, 32.77, 28.11, 28.11], "power_watts_avg": 30.44, "power_watts_peak": 32.77, "energy_joules_est": 9.79, "sample_count": 4, "duration_seconds": 0.322}, "timestamp": "2026-01-25T20:00:06.806190"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 332.84, "latencies_ms": [332.84], "images_per_second": 3.004, "prompt_tokens": 744, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nIn a grassy field, there are several zebras grazing and a rhino standing nearby. The scene is set against a backdrop of trees in the distance.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10455.6, "ram_available_mb": 112050.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [28.11, 28.11, 33.64, 33.64], "power_watts_avg": 30.87, "power_watts_peak": 33.64, "energy_joules_est": 10.29, "sample_count": 4, "duration_seconds": 0.333}, "timestamp": "2026-01-25T20:00:07.221117"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 94.676, "latencies_ms": [94.676], "images_per_second": 10.562, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Zebra: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.6, "ram_available_mb": 112050.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10463.4, "ram_available_mb": 112042.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [33.64], "power_watts_avg": 33.64, "power_watts_peak": 33.64, "energy_joules_est": 3.2, "sample_count": 1, "duration_seconds": 0.095}, "timestamp": "2026-01-25T20:00:07.326354"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 141.802, "latencies_ms": [141.802], "images_per_second": 7.052, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n A zebra is eating grass in a field with other zebras.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.4, "ram_available_mb": 112042.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [33.64, 33.64], "power_watts_avg": 33.64, "power_watts_peak": 33.64, "energy_joules_est": 4.78, "sample_count": 2, "duration_seconds": 0.142}, "timestamp": "2026-01-25T20:00:07.530914"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 284.848, "latencies_ms": [284.848], "images_per_second": 3.511, "prompt_tokens": 757, "response_tokens_est": 47, "n_tiles": 1, "output_text": "\nIn a grassy field, there are several zebras grazing peacefully. A few rhinos can be seen in the background as well. The sky above them appears to be cloudy, casting a soft light over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10461.3, "ram_available_mb": 112045.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [34.48, 34.48, 34.48], "power_watts_avg": 34.48, "power_watts_peak": 34.48, "energy_joules_est": 9.83, "sample_count": 3, "duration_seconds": 0.285}, "timestamp": "2026-01-25T20:00:07.836340"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 96.331, "latencies_ms": [96.331], "images_per_second": 10.381, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The sky is blue and cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.3, "ram_available_mb": 112045.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10462.8, "ram_available_mb": 112043.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [34.48], "power_watts_avg": 34.48, "power_watts_peak": 34.48, "energy_joules_est": 3.34, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T20:00:07.941918"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 301.838, "latencies_ms": [301.838], "images_per_second": 3.313, "prompt_tokens": 744, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA horse-drawn carriage with a green and gold canopy is pulling a cart full of passengers down a street lined with trees, benches, and umbrellas.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10462.8, "ram_available_mb": 112043.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10464.6, "ram_available_mb": 112041.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [34.48, 31.77, 31.77, 31.77], "power_watts_avg": 32.45, "power_watts_peak": 34.48, "energy_joules_est": 9.8, "sample_count": 4, "duration_seconds": 0.302}, "timestamp": "2026-01-25T20:00:08.355242"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 105.697, "latencies_ms": [105.697], "images_per_second": 9.461, "prompt_tokens": 759, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Horse pulling a trolley car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.6, "ram_available_mb": 112041.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [31.77, 31.77], "power_watts_avg": 31.77, "power_watts_peak": 31.77, "energy_joules_est": 3.37, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T20:00:08.560901"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 98.218, "latencies_ms": [98.218], "images_per_second": 10.181, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\nThe horse is pulling a trolley.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [34.15], "power_watts_avg": 34.15, "power_watts_peak": 34.15, "energy_joules_est": 3.37, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T20:00:08.666075"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 304.096, "latencies_ms": [304.096], "images_per_second": 3.288, "prompt_tokens": 757, "response_tokens_est": 51, "n_tiles": 1, "output_text": "\nIn a park, there is a horse pulling a trolley with people on board. The horse appears to be walking down a street next to a sidewalk lined with trees. There are several umbrellas set up along the sidewalk for shade purposes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10465.5, "ram_available_mb": 112040.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [34.15, 34.15, 34.15, 34.15], "power_watts_avg": 34.15, "power_watts_peak": 34.15, "energy_joules_est": 10.39, "sample_count": 4, "duration_seconds": 0.304}, "timestamp": "2026-01-25T20:00:09.072063"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 239.917, "latencies_ms": [239.917], "images_per_second": 4.168, "prompt_tokens": 756, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\n The image features a horse pulling a trolley with people on it. There are several umbrellas in the scene as well, suggesting that this might be an outdoor event or gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.5, "ram_available_mb": 112040.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [34.15, 34.15, 34.15], "power_watts_avg": 34.15, "power_watts_peak": 34.15, "energy_joules_est": 8.22, "sample_count": 3, "duration_seconds": 0.241}, "timestamp": "2026-01-25T20:00:09.378168"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 253.959, "latencies_ms": [253.959], "images_per_second": 3.938, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nTwo men are sitting on a bench, with one reading a newspaper and the other looking at his phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [34.15, 34.15, 33.01], "power_watts_avg": 33.77, "power_watts_peak": 34.15, "energy_joules_est": 8.6, "sample_count": 3, "duration_seconds": 0.255}, "timestamp": "2026-01-25T20:00:09.690223"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 77.231, "latencies_ms": [77.231], "images_per_second": 12.948, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Chair 1: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [33.01], "power_watts_avg": 33.01, "power_watts_peak": 33.01, "energy_joules_est": 2.56, "sample_count": 1, "duration_seconds": 0.078}, "timestamp": "2026-01-25T20:00:09.796037"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 165.102, "latencies_ms": [165.102], "images_per_second": 6.057, "prompt_tokens": 763, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\n 1. Man reading newspaper on bench in front of a building with a sign that says \"suce\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10465.8, "ram_available_mb": 112040.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [33.01, 33.01], "power_watts_avg": 33.01, "power_watts_peak": 33.01, "energy_joules_est": 5.46, "sample_count": 2, "duration_seconds": 0.165}, "timestamp": "2026-01-25T20:00:10.002271"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 239.472, "latencies_ms": [239.472], "images_per_second": 4.176, "prompt_tokens": 757, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nA group of people are sitting on benches outside a building, with one man reading a newspaper. The benches are arranged in rows, creating an orderly line for seating.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.8, "ram_available_mb": 112040.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10466.8, "ram_available_mb": 112039.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [33.01, 32.83, 32.83], "power_watts_avg": 32.89, "power_watts_peak": 33.01, "energy_joules_est": 7.89, "sample_count": 3, "duration_seconds": 0.24}, "timestamp": "2026-01-25T20:00:10.307975"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 395.845, "latencies_ms": [395.845], "images_per_second": 2.526, "prompt_tokens": 756, "response_tokens_est": 70, "n_tiles": 1, "output_text": "\n The image shows a group of people sitting on benches outside. One man is reading a newspaper while another man stands nearby. A woman can be seen in the background as well. There are several benches visible, with one being occupied by a person wearing a brown shirt and jeans. The scene appears to take place during daytime hours under bright sunlight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.8, "ram_available_mb": 112039.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10470.5, "ram_available_mb": 112035.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [32.83, 32.83, 32.83, 32.12], "power_watts_avg": 32.66, "power_watts_peak": 32.83, "energy_joules_est": 12.94, "sample_count": 4, "duration_seconds": 0.396}, "timestamp": "2026-01-25T20:00:10.713205"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 228.686, "latencies_ms": [228.686], "images_per_second": 4.373, "prompt_tokens": 744, "response_tokens_est": 17, "n_tiles": 1, "output_text": "urn of flowers sits on a desk, accompanied by a laptop and lamp.\n", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 10470.5, "ram_available_mb": 112035.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10470.7, "ram_available_mb": 112035.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 34.0}, "power_stats": {"power_watts_samples": [32.12, 32.12, 32.12], "power_watts_avg": 32.12, "power_watts_peak": 32.12, "energy_joules_est": 7.35, "sample_count": 3, "duration_seconds": 0.229}, "timestamp": "2026-01-25T20:00:11.022955"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 66.977, "latencies_ms": [66.977], "images_per_second": 14.93, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Laptop: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.7, "ram_available_mb": 112035.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 34.0}, "power_stats": {"power_watts_samples": [36.99], "power_watts_avg": 36.99, "power_watts_peak": 36.99, "energy_joules_est": 2.49, "sample_count": 1, "duration_seconds": 0.067}, "timestamp": "2026-01-25T20:00:11.128632"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 153.641, "latencies_ms": [153.641], "images_per_second": 6.509, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n 1. A laptop on a desk with a glass of juice next to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 34.0}, "power_stats": {"power_watts_samples": [36.99, 36.99], "power_watts_avg": 36.99, "power_watts_peak": 36.99, "energy_joules_est": 5.69, "sample_count": 2, "duration_seconds": 0.154}, "timestamp": "2026-01-25T20:00:11.333277"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 151.154, "latencies_ms": [151.154], "images_per_second": 6.616, "prompt_tokens": 757, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nA wooden desk with a laptop, glass of orange juice, and lamp on top.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 34.0}, "power_stats": {"power_watts_samples": [36.99, 36.99], "power_watts_avg": 36.99, "power_watts_peak": 36.99, "energy_joules_est": 5.6, "sample_count": 2, "duration_seconds": 0.151}, "timestamp": "2026-01-25T20:00:11.536549"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 87.002, "latencies_ms": [87.002], "images_per_second": 11.494, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urns of flowers on the wall.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 34.0}, "power_stats": {"power_watts_samples": [34.23], "power_watts_avg": 34.23, "power_watts_peak": 34.23, "energy_joules_est": 2.98, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T20:00:11.641497"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 303.084, "latencies_ms": [303.084], "images_per_second": 3.299, "prompt_tokens": 744, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA beach scene features a blue and white striped towel, two surfboards - one pink and one blue - and an umbrella set up on the sand.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [34.23, 34.23, 34.23, 34.23], "power_watts_avg": 34.23, "power_watts_peak": 34.23, "energy_joules_est": 10.38, "sample_count": 4, "duration_seconds": 0.303}, "timestamp": "2026-01-25T20:00:12.053668"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 88.966, "latencies_ms": [88.966], "images_per_second": 11.24, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Umbrella: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10468.5, "ram_available_mb": 112037.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [31.68], "power_watts_avg": 31.68, "power_watts_peak": 31.68, "energy_joules_est": 2.83, "sample_count": 1, "duration_seconds": 0.089}, "timestamp": "2026-01-25T20:00:12.158260"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 152.908, "latencies_ms": [152.908], "images_per_second": 6.54, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n 1. A blue and white striped beach towel on the sand next to a surfboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.5, "ram_available_mb": 112037.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10470.2, "ram_available_mb": 112036.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [31.68, 31.68], "power_watts_avg": 31.68, "power_watts_peak": 31.68, "energy_joules_est": 4.85, "sample_count": 2, "duration_seconds": 0.153}, "timestamp": "2026-01-25T20:00:12.363398"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 169.791, "latencies_ms": [169.791], "images_per_second": 5.89, "prompt_tokens": 757, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA beach with a blue and white striped towel, two surfboards (one pink and one blue), and an umbrella.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.2, "ram_available_mb": 112036.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [31.68, 31.68], "power_watts_avg": 31.68, "power_watts_peak": 31.68, "energy_joules_est": 5.39, "sample_count": 2, "duration_seconds": 0.17}, "timestamp": "2026-01-25T20:00:12.568759"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 115.024, "latencies_ms": [115.024], "images_per_second": 8.694, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n A blue and white striped beach towel is on the sand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10463.0, "ram_available_mb": 112043.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [32.95, 32.95], "power_watts_avg": 32.95, "power_watts_peak": 32.95, "energy_joules_est": 3.8, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T20:00:12.773956"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 261.555, "latencies_ms": [261.555], "images_per_second": 3.823, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA white sheep stands on a rocky outcropping, facing away from the camera and gazing into the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.0, "ram_available_mb": 112043.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10459.2, "ram_available_mb": 112047.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.95, 32.95, 32.95], "power_watts_avg": 32.95, "power_watts_peak": 32.95, "energy_joules_est": 8.64, "sample_count": 3, "duration_seconds": 0.262}, "timestamp": "2026-01-25T20:00:13.082294"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 85.353, "latencies_ms": [85.353], "images_per_second": 11.716, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Sheep: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.2, "ram_available_mb": 112047.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10457.5, "ram_available_mb": 112048.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.1], "power_watts_avg": 32.1, "power_watts_peak": 32.1, "energy_joules_est": 2.75, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T20:00:13.186905"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 132.395, "latencies_ms": [132.395], "images_per_second": 7.553, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nA sheep standing on a rock in front of a blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.5, "ram_available_mb": 112048.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10461.4, "ram_available_mb": 112044.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.1, 32.1], "power_watts_avg": 32.1, "power_watts_peak": 32.1, "energy_joules_est": 4.26, "sample_count": 2, "duration_seconds": 0.133}, "timestamp": "2026-01-25T20:00:13.392491"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 241.115, "latencies_ms": [241.115], "images_per_second": 4.147, "prompt_tokens": 757, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nA white sheep stands on a rock in an open field, facing away from the camera. The sky above has some clouds, suggesting that it might be a slightly overcast day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.4, "ram_available_mb": 112044.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.1, 32.1, 30.96], "power_watts_avg": 31.72, "power_watts_peak": 32.1, "energy_joules_est": 7.67, "sample_count": 3, "duration_seconds": 0.242}, "timestamp": "2026-01-25T20:00:13.698813"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 105.049, "latencies_ms": [105.049], "images_per_second": 9.519, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n The sky is blue with white clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10459.1, "ram_available_mb": 112047.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [30.96, 30.96], "power_watts_avg": 30.96, "power_watts_peak": 30.96, "energy_joules_est": 3.26, "sample_count": 2, "duration_seconds": 0.105}, "timestamp": "2026-01-25T20:00:13.904369"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 287.675, "latencies_ms": [287.675], "images_per_second": 3.476, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA woman with blue hair is taking a selfie in front of a mirror, holding up her phone to capture the moment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.1, "ram_available_mb": 112047.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10457.4, "ram_available_mb": 112048.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [30.96, 30.96, 27.69], "power_watts_avg": 29.87, "power_watts_peak": 30.96, "energy_joules_est": 8.61, "sample_count": 3, "duration_seconds": 0.288}, "timestamp": "2026-01-25T20:00:14.215875"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.37, "latencies_ms": [76.37], "images_per_second": 13.094, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Cell phone", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.4, "ram_available_mb": 112048.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10457.6, "ram_available_mb": 112048.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [27.69], "power_watts_avg": 27.69, "power_watts_peak": 27.69, "energy_joules_est": 2.12, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T20:00:14.321091"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 124.845, "latencies_ms": [124.845], "images_per_second": 8.01, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe woman is taking a selfie in front of a mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.6, "ram_available_mb": 112048.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10457.6, "ram_available_mb": 112048.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [27.69, 27.69], "power_watts_avg": 27.69, "power_watts_peak": 27.69, "energy_joules_est": 3.46, "sample_count": 2, "duration_seconds": 0.125}, "timestamp": "2026-01-25T20:00:14.527165"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 363.556, "latencies_ms": [363.556], "images_per_second": 2.751, "prompt_tokens": 757, "response_tokens_est": 62, "n_tiles": 1, "output_text": "\nIn this image, a woman with blue hair stands in front of a mirror, holding up her phone to take a selfie. She is wearing a blue shirt and tie, which adds a pop of color to the scene. The background features a bathroom setting, complete with a sink visible behind the woman.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.6, "ram_available_mb": 112048.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10455.4, "ram_available_mb": 112050.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [31.43, 31.43, 31.43, 31.43], "power_watts_avg": 31.43, "power_watts_peak": 31.43, "energy_joules_est": 11.45, "sample_count": 4, "duration_seconds": 0.364}, "timestamp": "2026-01-25T20:00:14.934025"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 114.08, "latencies_ms": [114.08], "images_per_second": 8.766, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The woman is wearing a blue shirt and tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.5, "ram_available_mb": 112050.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10456.5, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [31.43, 33.71], "power_watts_avg": 32.57, "power_watts_peak": 33.71, "energy_joules_est": 3.74, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T20:00:15.140567"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 267.271, "latencies_ms": [267.271], "images_per_second": 3.742, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "urns, vases, and a painting adorn the walls of an elegant living room with a fireplace at its center.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10456.5, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10459.0, "ram_available_mb": 112047.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [33.71, 33.71, 33.71], "power_watts_avg": 33.71, "power_watts_peak": 33.71, "energy_joules_est": 9.03, "sample_count": 3, "duration_seconds": 0.268}, "timestamp": "2026-01-25T20:00:15.450063"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 72.294, "latencies_ms": [72.294], "images_per_second": 13.832, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Chair 1: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.0, "ram_available_mb": 112047.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [33.71], "power_watts_avg": 33.71, "power_watts_peak": 33.71, "energy_joules_est": 2.46, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T20:00:15.556627"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 72.527, "latencies_ms": [72.527], "images_per_second": 13.788, "prompt_tokens": 763, "response_tokens_est": 6, "n_tiles": 1, "output_text": "urns on a mantle", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [34.65], "power_watts_avg": 34.65, "power_watts_peak": 34.65, "energy_joules_est": 2.53, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T20:00:15.662063"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 414.791, "latencies_ms": [414.791], "images_per_second": 2.411, "prompt_tokens": 757, "response_tokens_est": 74, "n_tiles": 1, "output_text": "\nThe image shows a room with white walls, featuring a fireplace on one side. The room has two chairs placed near each other, facing the fireplace. A desk can be seen in the middle of the room, along with a chair to its right. There are also several vases and pictures adorning the space, adding character to the room's decor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.65, 34.65, 34.65, 34.65, 35.1], "power_watts_avg": 34.74, "power_watts_peak": 35.1, "energy_joules_est": 14.42, "sample_count": 5, "duration_seconds": 0.415}, "timestamp": "2026-01-25T20:00:16.169112"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 109.355, "latencies_ms": [109.355], "images_per_second": 9.145, "prompt_tokens": 756, "response_tokens_est": 11, "n_tiles": 1, "output_text": "urns on the mantle and a painting.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.1, 35.1], "power_watts_avg": 35.1, "power_watts_peak": 35.1, "energy_joules_est": 3.85, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T20:00:16.375063"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 303.984, "latencies_ms": [303.984], "images_per_second": 3.29, "prompt_tokens": 744, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA brown dog leaps into the air to catch a red frisbee in its mouth, with a black car parked nearby and a tree in the background.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.1, 35.1, 32.81, 32.81], "power_watts_avg": 33.95, "power_watts_peak": 35.1, "energy_joules_est": 10.33, "sample_count": 4, "duration_seconds": 0.304}, "timestamp": "2026-01-25T20:00:16.788346"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 83.987, "latencies_ms": [83.987], "images_per_second": 11.907, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Car - 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10457.7, "ram_available_mb": 112048.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.81], "power_watts_avg": 32.81, "power_watts_peak": 32.81, "energy_joules_est": 2.77, "sample_count": 1, "duration_seconds": 0.084}, "timestamp": "2026-01-25T20:00:16.894133"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 139.892, "latencies_ms": [139.892], "images_per_second": 7.148, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA brown dog is jumping in the air to catch a red frisbee.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.7, "ram_available_mb": 112048.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10455.1, "ram_available_mb": 112051.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [32.81, 32.81], "power_watts_avg": 32.81, "power_watts_peak": 32.81, "energy_joules_est": 4.6, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T20:00:17.100919"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 170.533, "latencies_ms": [170.533], "images_per_second": 5.864, "prompt_tokens": 757, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA brown dog jumps in the air to catch a red frisbee, with its front paws on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.1, "ram_available_mb": 112051.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [31.02, 31.02], "power_watts_avg": 31.02, "power_watts_peak": 31.02, "energy_joules_est": 5.31, "sample_count": 2, "duration_seconds": 0.171}, "timestamp": "2026-01-25T20:00:17.306697"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 98.868, "latencies_ms": [98.868], "images_per_second": 10.114, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The dog is brown and black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10457.2, "ram_available_mb": 112049.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [31.02], "power_watts_avg": 31.02, "power_watts_peak": 31.02, "energy_joules_est": 3.08, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T20:00:17.411334"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 252.406, "latencies_ms": [252.406], "images_per_second": 3.962, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urn of water is on a table, with a giraffe standing next to it in an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.1, "ram_available_mb": 112049.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10457.7, "ram_available_mb": 112048.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [31.02, 30.63, 30.63], "power_watts_avg": 30.76, "power_watts_peak": 31.02, "energy_joules_est": 7.77, "sample_count": 3, "duration_seconds": 0.253}, "timestamp": "2026-01-25T20:00:17.721965"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 92.952, "latencies_ms": [92.952], "images_per_second": 10.758, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Leafy tree branch", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [30.63], "power_watts_avg": 30.63, "power_watts_peak": 30.63, "energy_joules_est": 2.86, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:00:17.827138"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 142.29, "latencies_ms": [142.29], "images_per_second": 7.028, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA giraffe with a brown and white spotted coat standing in front of trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10464.7, "ram_available_mb": 112041.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [30.63, 30.63], "power_watts_avg": 30.63, "power_watts_peak": 30.63, "energy_joules_est": 4.36, "sample_count": 2, "duration_seconds": 0.142}, "timestamp": "2026-01-25T20:00:18.031924"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 177.377, "latencies_ms": [177.377], "images_per_second": 5.638, "prompt_tokens": 757, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA giraffe with a brown and white spotted coat stands in front of trees, appearing to be looking at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.7, "ram_available_mb": 112041.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [32.54, 32.54], "power_watts_avg": 32.54, "power_watts_peak": 32.54, "energy_joules_est": 5.79, "sample_count": 2, "duration_seconds": 0.178}, "timestamp": "2026-01-25T20:00:18.237570"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 110.497, "latencies_ms": [110.497], "images_per_second": 9.05, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The giraffe has brown and white spots on its body.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10471.0, "ram_available_mb": 112035.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [32.54, 32.54], "power_watts_avg": 32.54, "power_watts_peak": 32.54, "energy_joules_est": 3.6, "sample_count": 2, "duration_seconds": 0.111}, "timestamp": "2026-01-25T20:00:18.443061"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 308.257, "latencies_ms": [308.257], "images_per_second": 3.244, "prompt_tokens": 744, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nTwo zebras stand side by side in a fenced-in area, facing away from each other and looking at something off to their left.", "error": null, "sys_before": {"cpu_percent": 3.4, "ram_used_mb": 10471.0, "ram_available_mb": 112035.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [32.54, 29.83, 29.83, 29.83], "power_watts_avg": 30.51, "power_watts_peak": 32.54, "energy_joules_est": 9.42, "sample_count": 4, "duration_seconds": 0.309}, "timestamp": "2026-01-25T20:00:18.859808"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 86.311, "latencies_ms": [86.311], "images_per_second": 11.586, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Fence", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [29.83], "power_watts_avg": 29.83, "power_watts_peak": 29.83, "energy_joules_est": 2.6, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T20:00:18.964534"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 80.017, "latencies_ms": [80.017], "images_per_second": 12.497, "prompt_tokens": 763, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Right zebra.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10462.8, "ram_available_mb": 112043.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [29.83], "power_watts_avg": 29.83, "power_watts_peak": 29.83, "energy_joules_est": 2.39, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T20:00:19.069434"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 343.079, "latencies_ms": [343.079], "images_per_second": 2.915, "prompt_tokens": 757, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\nTwo zebras stand in a grassy field, facing each other. They are positioned behind a fence that separates them from an observer who is watching their interaction. The zebras appear to be engaged in some sort of social activity or communication with the person observing them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.8, "ram_available_mb": 112043.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [30.36, 30.36, 30.36, 30.36], "power_watts_avg": 30.36, "power_watts_peak": 30.36, "energy_joules_est": 10.43, "sample_count": 4, "duration_seconds": 0.343}, "timestamp": "2026-01-25T20:00:19.476139"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 332.179, "latencies_ms": [332.179], "images_per_second": 3.01, "prompt_tokens": 756, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\n The image shows two zebras standing next to each other in a grassy area. One zebra has its tail raised high while the other one is facing away from the camera. The background features a fence and trees, providing an outdoor setting for the zebras.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.0, "ram_available_mb": 112050.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10459.0, "ram_available_mb": 112047.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [30.36, 33.9, 33.9, 33.9], "power_watts_avg": 33.01, "power_watts_peak": 33.9, "energy_joules_est": 10.98, "sample_count": 4, "duration_seconds": 0.333}, "timestamp": "2026-01-25T20:00:19.883270"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 301.074, "latencies_ms": [301.074], "images_per_second": 3.321, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA car is parked on a street, with two horses walking down the road in front of it and another horse further away behind them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.0, "ram_available_mb": 112047.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [33.9, 33.9, 35.12, 35.12], "power_watts_avg": 34.51, "power_watts_peak": 35.12, "energy_joules_est": 10.42, "sample_count": 4, "duration_seconds": 0.302}, "timestamp": "2026-01-25T20:00:20.297247"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 85.301, "latencies_ms": [85.301], "images_per_second": 11.723, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Horse: 2", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10459.6, "ram_available_mb": 112046.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [35.12], "power_watts_avg": 35.12, "power_watts_peak": 35.12, "energy_joules_est": 3.02, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T20:00:20.402043"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 100.975, "latencies_ms": [100.975], "images_per_second": 9.903, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Horse on right side of road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.6, "ram_available_mb": 112046.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [35.12, 35.12], "power_watts_avg": 35.12, "power_watts_peak": 35.12, "energy_joules_est": 3.55, "sample_count": 2, "duration_seconds": 0.101}, "timestamp": "2026-01-25T20:00:20.608225"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 223.468, "latencies_ms": [223.468], "images_per_second": 4.475, "prompt_tokens": 757, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nIn a rural area, two horses are walking down a street next to a car. The horses appear to be blocking the road as they walk towards each other.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [34.38, 34.38, 34.38], "power_watts_avg": 34.38, "power_watts_peak": 34.38, "energy_joules_est": 7.7, "sample_count": 3, "duration_seconds": 0.224}, "timestamp": "2026-01-25T20:00:20.914210"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 519.735, "latencies_ms": [519.735], "images_per_second": 1.924, "prompt_tokens": 756, "response_tokens_est": 91, "n_tiles": 1, "output_text": "\n The image shows a car parked on the side of the road with two horses standing in front of it. One horse is brown and white, while the other is black and brown. They are positioned close to each other, possibly grazing or resting. In addition to these two horses, there are also three more horses visible further down the street. The scene takes place on a sunny day, with trees lining the road and providing shade for the animals.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [34.38, 28.95, 28.95, 28.95, 28.95, 28.95], "power_watts_avg": 29.86, "power_watts_peak": 34.38, "energy_joules_est": 15.55, "sample_count": 6, "duration_seconds": 0.521}, "timestamp": "2026-01-25T20:00:21.520631"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 273.026, "latencies_ms": [273.026], "images_per_second": 3.663, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "urn of fruit sits on a desk in front of an old-fashioned chalkboard, with books and papers scattered around it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [32.19, 32.19, 32.19], "power_watts_avg": 32.19, "power_watts_peak": 32.19, "energy_joules_est": 8.81, "sample_count": 3, "duration_seconds": 0.274}, "timestamp": "2026-01-25T20:00:21.828908"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 276.669, "latencies_ms": [276.669], "images_per_second": 3.614, "prompt_tokens": 759, "response_tokens_est": 45, "n_tiles": 1, "output_text": "\n 1. Desk and chair set against a blackboard  2. Blackboard  3. Bookshelf  4. Desk  5. Chair  6. Desk lamp  7. Book  8. Desk lamp", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10466.5, "ram_available_mb": 112039.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.19, 32.19, 39.47], "power_watts_avg": 34.62, "power_watts_peak": 39.47, "energy_joules_est": 9.61, "sample_count": 3, "duration_seconds": 0.278}, "timestamp": "2026-01-25T20:00:22.136115"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 146.864, "latencies_ms": [146.864], "images_per_second": 6.809, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n 1. A chalkboard on a wall with books sitting on top of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.5, "ram_available_mb": 112039.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10466.5, "ram_available_mb": 112039.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [39.47, 39.47], "power_watts_avg": 39.47, "power_watts_peak": 39.47, "energy_joules_est": 5.81, "sample_count": 2, "duration_seconds": 0.147}, "timestamp": "2026-01-25T20:00:22.342795"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 447.421, "latencies_ms": [447.421], "images_per_second": 2.235, "prompt_tokens": 757, "response_tokens_est": 78, "n_tiles": 1, "output_text": "\nIn a classroom, there is an old-fashioned wooden desk with a chalkboard on top. The chalkboard has been used for teaching lessons in the past but now sits empty. A book rests on the desk next to the chalkboard. In front of the desk, there are two chairs and a bell, suggesting that this room was once part of a school or educational setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.5, "ram_available_mb": 112039.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [39.47, 39.47, 36.03, 36.03, 36.03], "power_watts_avg": 37.4, "power_watts_peak": 39.47, "energy_joules_est": 16.76, "sample_count": 5, "duration_seconds": 0.448}, "timestamp": "2026-01-25T20:00:22.849872"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 107.605, "latencies_ms": [107.605], "images_per_second": 9.293, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n A blackboard with white chalk writing on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10468.5, "ram_available_mb": 112037.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [36.03, 36.03], "power_watts_avg": 36.03, "power_watts_peak": 36.03, "energy_joules_est": 3.9, "sample_count": 2, "duration_seconds": 0.108}, "timestamp": "2026-01-25T20:00:23.056306"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 348.189, "latencies_ms": [348.189], "images_per_second": 2.872, "prompt_tokens": 744, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\nA yellow and white bus is driving on a city street, with another car following behind it. The back of the bus displays text in an Indian language, including numbers 477 and Anand.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10468.5, "ram_available_mb": 112037.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [32.95, 32.95, 32.95, 32.95], "power_watts_avg": 32.95, "power_watts_peak": 32.95, "energy_joules_est": 11.48, "sample_count": 4, "duration_seconds": 0.348}, "timestamp": "2026-01-25T20:00:23.471416"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 87.363, "latencies_ms": [87.363], "images_per_second": 11.446, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Bus: 473", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10475.7, "ram_available_mb": 112030.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [32.95], "power_watts_avg": 32.95, "power_watts_peak": 32.95, "energy_joules_est": 2.89, "sample_count": 1, "duration_seconds": 0.088}, "timestamp": "2026-01-25T20:00:23.576322"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 87.816, "latencies_ms": [87.816], "images_per_second": 11.387, "prompt_tokens": 763, "response_tokens_est": 7, "n_tiles": 1, "output_text": "ids on back of bus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.7, "ram_available_mb": 112030.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [35.5], "power_watts_avg": 35.5, "power_watts_peak": 35.5, "energy_joules_est": 3.13, "sample_count": 1, "duration_seconds": 0.088}, "timestamp": "2026-01-25T20:00:23.681236"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 148.551, "latencies_ms": [148.551], "images_per_second": 6.732, "prompt_tokens": 757, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA yellow bus with a white stripe on its back, displaying text in both English and Hindi.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10471.8, "ram_available_mb": 112034.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [35.5, 35.5], "power_watts_avg": 35.5, "power_watts_peak": 35.5, "energy_joules_est": 5.29, "sample_count": 2, "duration_seconds": 0.149}, "timestamp": "2026-01-25T20:00:23.886739"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 119.189, "latencies_ms": [119.189], "images_per_second": 8.39, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "ids 473 and 475 are on the back of a bus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.8, "ram_available_mb": 112034.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10475.7, "ram_available_mb": 112030.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [35.5, 35.5], "power_watts_avg": 35.5, "power_watts_peak": 35.5, "energy_joules_est": 4.24, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T20:00:24.091175"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 274.98, "latencies_ms": [274.98], "images_per_second": 3.637, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA bathroom with a large mirror and two sinks, one of which has a television screen mounted above it displaying a football game.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 10475.9, "ram_available_mb": 112030.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10475.3, "ram_available_mb": 112031.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [32.54, 32.54, 32.54], "power_watts_avg": 32.54, "power_watts_peak": 32.54, "energy_joules_est": 8.96, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T20:00:24.401929"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 104.482, "latencies_ms": [104.482], "images_per_second": 9.571, "prompt_tokens": 759, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Mirror  2. Sink", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.3, "ram_available_mb": 112031.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [32.54, 32.54], "power_watts_avg": 32.54, "power_watts_peak": 32.54, "energy_joules_est": 3.42, "sample_count": 2, "duration_seconds": 0.105}, "timestamp": "2026-01-25T20:00:24.607316"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 121.647, "latencies_ms": [121.647], "images_per_second": 8.221, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n A mirror with a television screen in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [32.43, 32.43], "power_watts_avg": 32.43, "power_watts_peak": 32.43, "energy_joules_est": 3.95, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:00:24.812319"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 272.224, "latencies_ms": [272.224], "images_per_second": 3.673, "prompt_tokens": 757, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\nThe image shows a bathroom with two sinks, one of which has a television mounted above it. The TV screen displays a football game, likely the Super Bowl, as there are two people on the screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10469.8, "ram_available_mb": 112036.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.43, 32.43, 30.86], "power_watts_avg": 31.91, "power_watts_peak": 32.43, "energy_joules_est": 8.69, "sample_count": 3, "duration_seconds": 0.273}, "timestamp": "2026-01-25T20:00:25.119762"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 82.969, "latencies_ms": [82.969], "images_per_second": 12.053, "prompt_tokens": 756, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urns on the counter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.8, "ram_available_mb": 112036.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10472.9, "ram_available_mb": 112033.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.86], "power_watts_avg": 30.86, "power_watts_peak": 30.86, "energy_joules_est": 2.57, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:00:25.223336"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 261.478, "latencies_ms": [261.478], "images_per_second": 3.824, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA man is sitting on a bench in front of a large tree, with a church spire visible behind him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.9, "ram_available_mb": 112033.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10476.3, "ram_available_mb": 112030.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.86, 30.86, 30.86], "power_watts_avg": 30.86, "power_watts_peak": 30.86, "energy_joules_est": 8.07, "sample_count": 3, "duration_seconds": 0.262}, "timestamp": "2026-01-25T20:00:25.532590"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.586, "latencies_ms": [76.586], "images_per_second": 13.057, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Bench 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.3, "ram_available_mb": 112030.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10483.6, "ram_available_mb": 112022.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.56], "power_watts_avg": 32.56, "power_watts_peak": 32.56, "energy_joules_est": 2.51, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T20:00:25.638681"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 147.415, "latencies_ms": [147.415], "images_per_second": 6.784, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n 1. A man sitting on a bench in front of a tree and a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.6, "ram_available_mb": 112022.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10487.9, "ram_available_mb": 112018.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.56, 32.56], "power_watts_avg": 32.56, "power_watts_peak": 32.56, "energy_joules_est": 4.82, "sample_count": 2, "duration_seconds": 0.148}, "timestamp": "2026-01-25T20:00:25.844775"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 270.362, "latencies_ms": [270.362], "images_per_second": 3.699, "prompt_tokens": 757, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\nA man sits on a bench in front of a large tree, with a church spire visible behind him. The image has been edited to appear black and white, adding an artistic touch to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10487.9, "ram_available_mb": 112018.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10490.1, "ram_available_mb": 112016.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.56, 32.56, 33.65], "power_watts_avg": 32.92, "power_watts_peak": 33.65, "energy_joules_est": 8.91, "sample_count": 3, "duration_seconds": 0.271}, "timestamp": "2026-01-25T20:00:26.150426"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 155.478, "latencies_ms": [155.478], "images_per_second": 6.432, "prompt_tokens": 756, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\n The image is a black and white photo of a man sitting on a bench in front of a church.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10490.1, "ram_available_mb": 112016.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10496.2, "ram_available_mb": 112010.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [33.65, 33.65], "power_watts_avg": 33.65, "power_watts_peak": 33.65, "energy_joules_est": 5.25, "sample_count": 2, "duration_seconds": 0.156}, "timestamp": "2026-01-25T20:00:26.355444"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 276.36, "latencies_ms": [276.36], "images_per_second": 3.618, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA group of cars are parked in a line on a street, with some people walking nearby and others standing around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10496.2, "ram_available_mb": 112010.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10499.9, "ram_available_mb": 112006.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [33.65, 33.65, 32.95], "power_watts_avg": 33.41, "power_watts_peak": 33.65, "energy_joules_est": 9.26, "sample_count": 3, "duration_seconds": 0.277}, "timestamp": "2026-01-25T20:00:26.665802"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 335.058, "latencies_ms": [335.058], "images_per_second": 2.985, "prompt_tokens": 759, "response_tokens_est": 55, "n_tiles": 1, "output_text": "\n 1. Bus stop sign  2. Street light  3. Car parked at bus stop  4. Car parked in lot  5. Car parked in lot  6. Car parked in lot  7. Car parked in lot  8. Car parked in lot", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10499.9, "ram_available_mb": 112006.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10500.2, "ram_available_mb": 112006.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [32.95, 32.95, 32.95, 32.95], "power_watts_avg": 32.95, "power_watts_peak": 32.95, "energy_joules_est": 11.05, "sample_count": 4, "duration_seconds": 0.335}, "timestamp": "2026-01-25T20:00:27.071863"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 102.287, "latencies_ms": [102.287], "images_per_second": 9.776, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. White van on right side of street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10500.2, "ram_available_mb": 112006.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10498.8, "ram_available_mb": 112007.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [37.94, 37.94], "power_watts_avg": 37.94, "power_watts_peak": 37.94, "energy_joules_est": 3.89, "sample_count": 2, "duration_seconds": 0.103}, "timestamp": "2026-01-25T20:00:27.277233"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 258.175, "latencies_ms": [258.175], "images_per_second": 3.873, "prompt_tokens": 757, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\nA group of cars are parked in front of a building, with one car having its trunk open. A bus stop sign can be seen nearby, indicating that this is likely a busy street or parking area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10498.8, "ram_available_mb": 112007.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10501.3, "ram_available_mb": 112005.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [37.94, 37.94, 37.94], "power_watts_avg": 37.94, "power_watts_peak": 37.94, "energy_joules_est": 9.82, "sample_count": 3, "duration_seconds": 0.259}, "timestamp": "2026-01-25T20:00:27.582802"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 395.196, "latencies_ms": [395.196], "images_per_second": 2.53, "prompt_tokens": 756, "response_tokens_est": 69, "n_tiles": 1, "output_text": "\n The image shows a street scene with several cars parked on the side of the road. There is a bus and a car in front of it, as well as other vehicles further down the street. A person can be seen standing near one of the cars. The sky appears to be clear blue, suggesting good weather conditions for outdoor activities.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10501.3, "ram_available_mb": 112005.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10500.3, "ram_available_mb": 112006.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [33.94, 33.94, 33.94, 33.94], "power_watts_avg": 33.94, "power_watts_peak": 33.94, "energy_joules_est": 13.42, "sample_count": 4, "duration_seconds": 0.396}, "timestamp": "2026-01-25T20:00:27.988627"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 258.34, "latencies_ms": [258.34], "images_per_second": 3.871, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\n A breakfast spread is laid out on a wooden table, featuring a plate of food and two cups of coffee.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10500.3, "ram_available_mb": 112006.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10500.0, "ram_available_mb": 112006.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [33.94, 32.21, 32.21], "power_watts_avg": 32.79, "power_watts_peak": 33.94, "energy_joules_est": 8.48, "sample_count": 3, "duration_seconds": 0.259}, "timestamp": "2026-01-25T20:00:28.296860"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 96.648, "latencies_ms": [96.648], "images_per_second": 10.347, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Plate of food - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10500.0, "ram_available_mb": 112006.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10498.3, "ram_available_mb": 112008.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [32.21], "power_watts_avg": 32.21, "power_watts_peak": 32.21, "energy_joules_est": 3.13, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T20:00:28.401240"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 128.246, "latencies_ms": [128.246], "images_per_second": 7.798, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A plate of food and a cup of tea are on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10498.3, "ram_available_mb": 112008.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10496.7, "ram_available_mb": 112009.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [32.21, 32.21], "power_watts_avg": 32.21, "power_watts_peak": 32.21, "energy_joules_est": 4.14, "sample_count": 2, "duration_seconds": 0.128}, "timestamp": "2026-01-25T20:00:28.605785"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 136.252, "latencies_ms": [136.252], "images_per_second": 7.339, "prompt_tokens": 757, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n A wooden table with a plate of food, including fruit, bread, and tea.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10496.7, "ram_available_mb": 112009.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10496.2, "ram_available_mb": 112010.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [37.38, 37.38], "power_watts_avg": 37.38, "power_watts_peak": 37.38, "energy_joules_est": 5.1, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T20:00:28.811029"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 130.618, "latencies_ms": [130.618], "images_per_second": 7.656, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A plate of food and a cup of tea on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10496.2, "ram_available_mb": 112010.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10496.3, "ram_available_mb": 112010.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [37.38, 37.38], "power_watts_avg": 37.38, "power_watts_peak": 37.38, "energy_joules_est": 4.9, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T20:00:29.016569"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 285.037, "latencies_ms": [285.037], "images_per_second": 3.508, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "urns of water on a table, with an older woman in a striped shirt and apron preparing food at a kitchen counter.\n", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10496.3, "ram_available_mb": 112010.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10497.6, "ram_available_mb": 112008.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [31.55, 31.55, 31.55], "power_watts_avg": 31.55, "power_watts_peak": 31.55, "energy_joules_est": 9.0, "sample_count": 3, "duration_seconds": 0.285}, "timestamp": "2026-01-25T20:00:29.327555"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 83.872, "latencies_ms": [83.872], "images_per_second": 11.923, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bread: 2", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10497.6, "ram_available_mb": 112008.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10493.1, "ram_available_mb": 112013.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [31.55], "power_watts_avg": 31.55, "power_watts_peak": 31.55, "energy_joules_est": 2.65, "sample_count": 1, "duration_seconds": 0.084}, "timestamp": "2026-01-25T20:00:29.431855"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 91.053, "latencies_ms": [91.053], "images_per_second": 10.983, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Woman making cookies on table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10493.1, "ram_available_mb": 112013.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10494.9, "ram_available_mb": 112011.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [31.55], "power_watts_avg": 31.55, "power_watts_peak": 31.55, "energy_joules_est": 2.88, "sample_count": 1, "duration_seconds": 0.091}, "timestamp": "2026-01-25T20:00:29.536848"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 399.021, "latencies_ms": [399.021], "images_per_second": 2.506, "prompt_tokens": 757, "response_tokens_est": 68, "n_tiles": 1, "output_text": "\nAn elderly woman in a striped shirt is preparing food on a table, surrounded by various baked goods. The table has an assortment of pastries, including donuts, cookies, and breads. There are also cups and bowls placed around the table, suggesting that they might be enjoying some refreshments while indulging in their treats.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10494.9, "ram_available_mb": 112011.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10494.9, "ram_available_mb": 112011.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [31.42, 31.42, 31.42, 31.42], "power_watts_avg": 31.42, "power_watts_peak": 31.42, "energy_joules_est": 12.55, "sample_count": 4, "duration_seconds": 0.399}, "timestamp": "2026-01-25T20:00:29.941808"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 95.962, "latencies_ms": [95.962], "images_per_second": 10.421, "prompt_tokens": 756, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n The woman is making cookies on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10494.9, "ram_available_mb": 112011.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10498.0, "ram_available_mb": 112008.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [31.42], "power_watts_avg": 31.42, "power_watts_peak": 31.42, "energy_joules_est": 3.02, "sample_count": 1, "duration_seconds": 0.096}, "timestamp": "2026-01-25T20:00:30.046782"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 299.888, "latencies_ms": [299.888], "images_per_second": 3.335, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA man stands in front of a traffic light, wearing a white t-shirt and khaki shorts with a smile on his face.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10498.0, "ram_available_mb": 112008.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10495.1, "ram_available_mb": 112011.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [37.08, 37.08, 37.08], "power_watts_avg": 37.08, "power_watts_peak": 37.08, "energy_joules_est": 11.14, "sample_count": 3, "duration_seconds": 0.301}, "timestamp": "2026-01-25T20:00:30.361749"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 116.387, "latencies_ms": [116.387], "images_per_second": 8.592, "prompt_tokens": 759, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Traffic light  2. Sign  3. Man", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10495.1, "ram_available_mb": 112011.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10494.9, "ram_available_mb": 112011.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [37.08, 37.08], "power_watts_avg": 37.08, "power_watts_peak": 37.08, "energy_joules_est": 4.32, "sample_count": 2, "duration_seconds": 0.117}, "timestamp": "2026-01-25T20:00:30.567286"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 143.168, "latencies_ms": [143.168], "images_per_second": 6.985, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n 1. A man in a white shirt and shorts standing next to a traffic light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10494.9, "ram_available_mb": 112011.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10492.9, "ram_available_mb": 112013.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [38.08, 38.08], "power_watts_avg": 38.08, "power_watts_peak": 38.08, "energy_joules_est": 5.49, "sample_count": 2, "duration_seconds": 0.144}, "timestamp": "2026-01-25T20:00:30.774074"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 293.918, "latencies_ms": [293.918], "images_per_second": 3.402, "prompt_tokens": 757, "response_tokens_est": 47, "n_tiles": 1, "output_text": "\nA man in a white t-shirt stands next to a traffic light, with a sign that reads \"Australia Traffic Light\" behind him. The area around them features trees and bushes, creating an outdoor setting for this scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10492.9, "ram_available_mb": 112013.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10493.3, "ram_available_mb": 112013.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [38.08, 38.08, 38.08], "power_watts_avg": 38.08, "power_watts_peak": 38.08, "energy_joules_est": 11.22, "sample_count": 3, "duration_seconds": 0.295}, "timestamp": "2026-01-25T20:00:31.080825"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 120.874, "latencies_ms": [120.874], "images_per_second": 8.273, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n The man is standing in front of a traffic light and a sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10493.3, "ram_available_mb": 112013.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10493.1, "ram_available_mb": 112013.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.87, 34.87], "power_watts_avg": 34.87, "power_watts_peak": 34.87, "energy_joules_est": 4.22, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T20:00:31.287381"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 286.456, "latencies_ms": [286.456], "images_per_second": 3.491, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA group of kites in various colors, including red and blue, are flying high in a park-like setting with people walking around.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10493.1, "ram_available_mb": 112013.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10489.4, "ram_available_mb": 112016.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.87, 34.87, 34.87], "power_watts_avg": 34.87, "power_watts_peak": 34.87, "energy_joules_est": 10.0, "sample_count": 3, "duration_seconds": 0.287}, "timestamp": "2026-01-25T20:00:31.600900"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 111.245, "latencies_ms": [111.245], "images_per_second": 8.989, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Red fish kite (2)", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10489.4, "ram_available_mb": 112016.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10490.0, "ram_available_mb": 112016.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.9, 33.9], "power_watts_avg": 33.9, "power_watts_peak": 33.9, "energy_joules_est": 3.81, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T20:00:31.807535"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 116.171, "latencies_ms": [116.171], "images_per_second": 8.608, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nThe kites are flying in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10489.9, "ram_available_mb": 112016.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10481.6, "ram_available_mb": 112024.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.9, 33.9], "power_watts_avg": 33.9, "power_watts_peak": 33.9, "energy_joules_est": 3.95, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T20:00:32.014117"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 397.519, "latencies_ms": [397.519], "images_per_second": 2.516, "prompt_tokens": 757, "response_tokens_est": 66, "n_tiles": 1, "output_text": "\nIn a park, there are several kites flying in the sky. The kites have different colors including red, blue, purple, and orange. Some of them are shaped like fish or other sea creatures. People can be seen standing around, enjoying the sight of these unique and colorful kites soaring above them.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.6, "ram_available_mb": 112024.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10477.6, "ram_available_mb": 112028.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [33.9, 31.53, 31.53, 31.53], "power_watts_avg": 32.12, "power_watts_peak": 33.9, "energy_joules_est": 12.79, "sample_count": 4, "duration_seconds": 0.398}, "timestamp": "2026-01-25T20:00:32.420212"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 138.03, "latencies_ms": [138.03], "images_per_second": 7.245, "prompt_tokens": 756, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n A group of kites in the sky with a fish-shaped kite.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.6, "ram_available_mb": 112028.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10477.7, "ram_available_mb": 112028.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [31.53, 31.53], "power_watts_avg": 31.53, "power_watts_peak": 31.53, "energy_joules_est": 4.36, "sample_count": 2, "duration_seconds": 0.138}, "timestamp": "2026-01-25T20:00:32.625213"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 266.573, "latencies_ms": [266.573], "images_per_second": 3.751, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urn of water sits on a table between two people sharing pizza slices in a room with orange walls and a brown door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.7, "ram_available_mb": 112028.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10478.9, "ram_available_mb": 112027.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [30.32, 30.32, 30.32], "power_watts_avg": 30.32, "power_watts_peak": 30.32, "energy_joules_est": 8.1, "sample_count": 3, "duration_seconds": 0.267}, "timestamp": "2026-01-25T20:00:32.940230"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 81.363, "latencies_ms": [81.363], "images_per_second": 12.291, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n Man: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.9, "ram_available_mb": 112027.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10481.8, "ram_available_mb": 112024.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 12.0}, "power_stats": {"power_watts_samples": [30.32], "power_watts_avg": 30.32, "power_watts_peak": 30.32, "energy_joules_est": 2.49, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T20:00:33.045841"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 67.589, "latencies_ms": [67.589], "images_per_second": 14.795, "prompt_tokens": 763, "response_tokens_est": 6, "n_tiles": 1, "output_text": "urn of water on floor", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.8, "ram_available_mb": 112024.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10483.5, "ram_available_mb": 112022.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 12.0}, "power_stats": {"power_watts_samples": [35.91], "power_watts_avg": 35.91, "power_watts_peak": 35.91, "energy_joules_est": 2.44, "sample_count": 1, "duration_seconds": 0.068}, "timestamp": "2026-01-25T20:00:33.151315"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 169.25, "latencies_ms": [169.25], "images_per_second": 5.908, "prompt_tokens": 757, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\n A man in a blue jacket holds a slice of pizza for his young son, who is sitting on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.5, "ram_available_mb": 112022.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10484.5, "ram_available_mb": 112021.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 12.0}, "power_stats": {"power_watts_samples": [35.91, 35.91], "power_watts_avg": 35.91, "power_watts_peak": 35.91, "energy_joules_est": 6.1, "sample_count": 2, "duration_seconds": 0.17}, "timestamp": "2026-01-25T20:00:33.357249"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 90.709, "latencies_ms": [90.709], "images_per_second": 11.024, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "urn of water on the floor.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.5, "ram_available_mb": 112021.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10485.3, "ram_available_mb": 112021.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 12.0}, "power_stats": {"power_watts_samples": [35.91], "power_watts_avg": 35.91, "power_watts_peak": 35.91, "energy_joules_est": 3.27, "sample_count": 1, "duration_seconds": 0.091}, "timestamp": "2026-01-25T20:00:33.462106"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 268.689, "latencies_ms": [268.689], "images_per_second": 3.722, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urn of orange liquid sits on a campfire, with a woman in a striped shirt eating food from a plate nearby.", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 10485.3, "ram_available_mb": 112021.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10482.6, "ram_available_mb": 112023.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 12.0}, "power_stats": {"power_watts_samples": [35.91, 31.48, 31.48], "power_watts_avg": 32.96, "power_watts_peak": 35.91, "energy_joules_est": 8.86, "sample_count": 3, "duration_seconds": 0.269}, "timestamp": "2026-01-25T20:00:33.774319"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 74.924, "latencies_ms": [74.924], "images_per_second": 13.347, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Chair", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.6, "ram_available_mb": 112023.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10479.4, "ram_available_mb": 112026.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 12.0}, "power_stats": {"power_watts_samples": [31.48], "power_watts_avg": 31.48, "power_watts_peak": 31.48, "energy_joules_est": 2.38, "sample_count": 1, "duration_seconds": 0.076}, "timestamp": "2026-01-25T20:00:33.880038"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 109.987, "latencies_ms": [109.987], "images_per_second": 9.092, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nA woman eating a snack in front of her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.4, "ram_available_mb": 112026.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10479.4, "ram_available_mb": 112026.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [31.48, 31.48], "power_watts_avg": 31.48, "power_watts_peak": 31.48, "energy_joules_est": 3.47, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T20:00:34.085233"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 218.866, "latencies_ms": [218.866], "images_per_second": 4.569, "prompt_tokens": 757, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA woman in a striped shirt sits on a chair, eating food. She has a plate of chips in front of her with an orange leaf design.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10479.4, "ram_available_mb": 112026.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [32.25, 32.25, 32.25], "power_watts_avg": 32.25, "power_watts_peak": 32.25, "energy_joules_est": 7.07, "sample_count": 3, "duration_seconds": 0.219}, "timestamp": "2026-01-25T20:00:34.392082"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 121.562, "latencies_ms": [121.562], "images_per_second": 8.226, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The woman is eating a piece of food in the dark.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10474.6, "ram_available_mb": 112031.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [32.25, 32.25], "power_watts_avg": 32.25, "power_watts_peak": 32.25, "energy_joules_est": 3.94, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:00:34.598122"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 267.239, "latencies_ms": [267.239], "images_per_second": 3.742, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urns of water on a table, with plates and glasses filled with food in front of people at a family dinner.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10474.6, "ram_available_mb": 112031.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [32.3, 32.3, 32.3], "power_watts_avg": 32.3, "power_watts_peak": 32.3, "energy_joules_est": 8.64, "sample_count": 3, "duration_seconds": 0.268}, "timestamp": "2026-01-25T20:00:34.909799"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 87.949, "latencies_ms": [87.949], "images_per_second": 11.37, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\nplate 1: 2", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [32.3], "power_watts_avg": 32.3, "power_watts_peak": 32.3, "energy_joules_est": 2.86, "sample_count": 1, "duration_seconds": 0.088}, "timestamp": "2026-01-25T20:00:35.015422"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 130.991, "latencies_ms": [130.991], "images_per_second": 7.634, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. Woman in front of table with blue cloths on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [32.3, 30.44], "power_watts_avg": 31.37, "power_watts_peak": 32.3, "energy_joules_est": 4.12, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T20:00:35.221083"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 123.814, "latencies_ms": [123.814], "images_per_second": 8.077, "prompt_tokens": 757, "response_tokens_est": 15, "n_tiles": 1, "output_text": "urns of wine on a table, with people sitting around it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10478.4, "ram_available_mb": 112027.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [30.44, 30.44], "power_watts_avg": 30.44, "power_watts_peak": 30.44, "energy_joules_est": 3.79, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T20:00:35.427069"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 606.669, "latencies_ms": [606.669], "images_per_second": 1.648, "prompt_tokens": 756, "response_tokens_est": 109, "n_tiles": 1, "output_text": "\n The image shows a group of people gathered around a dining table with plates and glasses. There are at least 12 people in the scene, including both adults and children. They appear to be enjoying their time together as they eat dinner and converse. The room is well-lit by natural light coming through windows on one side, creating a warm atmosphere. The table is covered with various dishes, plates of food, and glasses filled with drinks such as wine or water. There are also some decorative elements like vases placed around the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.4, "ram_available_mb": 112027.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10471.8, "ram_available_mb": 112034.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.44, 29.97, 29.97, 29.97, 29.97, 29.97, 33.91], "power_watts_avg": 30.6, "power_watts_peak": 33.91, "energy_joules_est": 18.58, "sample_count": 7, "duration_seconds": 0.607}, "timestamp": "2026-01-25T20:00:36.134878"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 330.303, "latencies_ms": [330.303], "images_per_second": 3.028, "prompt_tokens": 744, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA baseball game is in progress, with a batter sliding into home plate and attempting to score. The catcher is crouched behind him, ready to catch the ball if necessary.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10471.8, "ram_available_mb": 112034.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10470.2, "ram_available_mb": 112036.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.91, 33.91, 33.91, 33.91], "power_watts_avg": 33.91, "power_watts_peak": 33.91, "energy_joules_est": 11.22, "sample_count": 4, "duration_seconds": 0.331}, "timestamp": "2026-01-25T20:00:36.548432"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 135.42, "latencies_ms": [135.42], "images_per_second": 7.384, "prompt_tokens": 759, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n 1. Pitcher  2. Catcher 3. Batter 4. Umpire", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.2, "ram_available_mb": 112036.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10471.0, "ram_available_mb": 112035.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [40.11, 40.11], "power_watts_avg": 40.11, "power_watts_peak": 40.11, "energy_joules_est": 5.46, "sample_count": 2, "duration_seconds": 0.136}, "timestamp": "2026-01-25T20:00:36.754010"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 67.655, "latencies_ms": [67.655], "images_per_second": 14.781, "prompt_tokens": 763, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Catcher", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10476.5, "ram_available_mb": 112029.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [40.11], "power_watts_avg": 40.11, "power_watts_peak": 40.11, "energy_joules_est": 2.73, "sample_count": 1, "duration_seconds": 0.068}, "timestamp": "2026-01-25T20:00:36.858982"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 249.442, "latencies_ms": [249.442], "images_per_second": 4.009, "prompt_tokens": 757, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\nA baseball game is in progress, with a batter sliding into home plate. The catcher is crouched behind him, ready to catch the ball, while an umpire stands nearby observing the play.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.5, "ram_available_mb": 112029.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [40.11, 40.11, 32.53], "power_watts_avg": 37.59, "power_watts_peak": 40.11, "energy_joules_est": 9.39, "sample_count": 3, "duration_seconds": 0.25}, "timestamp": "2026-01-25T20:00:37.164204"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 318.313, "latencies_ms": [318.313], "images_per_second": 3.142, "prompt_tokens": 756, "response_tokens_est": 53, "n_tiles": 1, "output_text": "\n The image shows a baseball game in progress. A player is sliding into home plate while another player attempts to tag him out with his glove. There are several other players on the field and around it, some of whom appear to be watching the play unfold.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10474.0, "ram_available_mb": 112032.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.53, 32.53, 32.53, 32.53], "power_watts_avg": 32.53, "power_watts_peak": 32.53, "energy_joules_est": 10.36, "sample_count": 4, "duration_seconds": 0.319}, "timestamp": "2026-01-25T20:00:37.569933"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 277.973, "latencies_ms": [277.973], "images_per_second": 3.597, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA skateboarder in a black shirt and helmet is performing a trick on a rail, with their shadow visible below them.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 10474.0, "ram_available_mb": 112032.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10474.4, "ram_available_mb": 112031.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.19, 31.19, 31.19], "power_watts_avg": 31.19, "power_watts_peak": 31.19, "energy_joules_est": 8.69, "sample_count": 3, "duration_seconds": 0.279}, "timestamp": "2026-01-25T20:00:37.884045"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 65.151, "latencies_ms": [65.151], "images_per_second": 15.349, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Tree", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.4, "ram_available_mb": 112031.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.19], "power_watts_avg": 31.19, "power_watts_peak": 31.19, "energy_joules_est": 2.05, "sample_count": 1, "duration_seconds": 0.066}, "timestamp": "2026-01-25T20:00:37.991259"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 154.526, "latencies_ms": [154.526], "images_per_second": 6.471, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nThe skateboarder is performing a trick on his skateboard while wearing protective gear.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [31.19, 35.05], "power_watts_avg": 33.12, "power_watts_peak": 35.05, "energy_joules_est": 5.13, "sample_count": 2, "duration_seconds": 0.155}, "timestamp": "2026-01-25T20:00:38.199823"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 285.697, "latencies_ms": [285.697], "images_per_second": 3.5, "prompt_tokens": 757, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\nA skateboarder in a black shirt and helmet is performing a trick on a rail at a skate park. The skateboarder's shadow is visible, indicating that they are skating outdoors during daylight hours.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [35.05, 35.05, 35.05], "power_watts_avg": 35.05, "power_watts_peak": 35.05, "energy_joules_est": 10.02, "sample_count": 3, "duration_seconds": 0.286}, "timestamp": "2026-01-25T20:00:38.505994"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 122.74, "latencies_ms": [122.74], "images_per_second": 8.147, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The skateboarder is wearing a black shirt and helmet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [35.05, 33.01], "power_watts_avg": 34.03, "power_watts_peak": 35.05, "energy_joules_est": 4.2, "sample_count": 2, "duration_seconds": 0.123}, "timestamp": "2026-01-25T20:00:38.712845"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 257.068, "latencies_ms": [257.068], "images_per_second": 3.89, "prompt_tokens": 744, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nA white plate holds a sandwich, fries, and pickles on a gray tablecloth.", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10473.9, "ram_available_mb": 112032.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [33.01, 33.01, 33.01], "power_watts_avg": 33.01, "power_watts_peak": 33.01, "energy_joules_est": 8.51, "sample_count": 3, "duration_seconds": 0.258}, "timestamp": "2026-01-25T20:00:39.028284"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.513, "latencies_ms": [76.513], "images_per_second": 13.07, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n Burger: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.9, "ram_available_mb": 112032.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10474.1, "ram_available_mb": 112032.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [33.68], "power_watts_avg": 33.68, "power_watts_peak": 33.68, "energy_joules_est": 2.59, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T20:00:39.135276"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 148.122, "latencies_ms": [148.122], "images_per_second": 6.751, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n A white plate with a sandwich and fries on it sits in front of two other plates.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.1, "ram_available_mb": 112032.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10474.1, "ram_available_mb": 112032.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [33.68, 33.68], "power_watts_avg": 33.68, "power_watts_peak": 33.68, "energy_joules_est": 5.01, "sample_count": 2, "duration_seconds": 0.149}, "timestamp": "2026-01-25T20:00:39.342128"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 246.0, "latencies_ms": [246.0], "images_per_second": 4.065, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\n A white tray holds a meal with a sandwich, fries, salad, pickle, and sauce. The plate has a name tag that reads \"Chez-Homes\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.1, "ram_available_mb": 112032.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10474.7, "ram_available_mb": 112031.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [33.68, 33.68, 31.39], "power_watts_avg": 32.92, "power_watts_peak": 33.68, "energy_joules_est": 8.11, "sample_count": 3, "duration_seconds": 0.246}, "timestamp": "2026-01-25T20:00:39.648767"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 119.574, "latencies_ms": [119.574], "images_per_second": 8.363, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The image shows a white plate with a sandwich and fries.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.7, "ram_available_mb": 112031.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [31.39, 31.39], "power_watts_avg": 31.39, "power_watts_peak": 31.39, "energy_joules_est": 3.77, "sample_count": 2, "duration_seconds": 0.12}, "timestamp": "2026-01-25T20:00:39.855647"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 277.474, "latencies_ms": [277.474], "images_per_second": 3.604, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA red motorcycle is parked on a sandy beach, with palm trees in the background and a fence visible to its left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10463.0, "ram_available_mb": 112043.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [31.39, 31.39, 30.76], "power_watts_avg": 31.18, "power_watts_peak": 31.39, "energy_joules_est": 8.7, "sample_count": 3, "duration_seconds": 0.279}, "timestamp": "2026-01-25T20:00:40.170495"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.113, "latencies_ms": [82.113], "images_per_second": 12.178, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Palm tree", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.0, "ram_available_mb": 112043.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [30.76], "power_watts_avg": 30.76, "power_watts_peak": 30.76, "energy_joules_est": 2.54, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:00:40.278311"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 147.164, "latencies_ms": [147.164], "images_per_second": 6.795, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n 1. A red motorcycle parked on a road with palm trees in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10469.4, "ram_available_mb": 112036.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [30.76, 30.76], "power_watts_avg": 30.76, "power_watts_peak": 30.76, "energy_joules_est": 4.54, "sample_count": 2, "duration_seconds": 0.147}, "timestamp": "2026-01-25T20:00:40.484351"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 331.69, "latencies_ms": [331.69], "images_per_second": 3.015, "prompt_tokens": 757, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\nA red motorcycle with a black seat and saddlebags is parked on a sandy beach, facing away from the camera. The motorcycle has its back seat down and appears to be in good condition. In the background, there are palm trees swaying gently under the open sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.4, "ram_available_mb": 112036.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [30.76, 31.26, 31.26, 31.26], "power_watts_avg": 31.13, "power_watts_peak": 31.26, "energy_joules_est": 10.34, "sample_count": 4, "duration_seconds": 0.332}, "timestamp": "2026-01-25T20:00:40.890285"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 89.288, "latencies_ms": [89.288], "images_per_second": 11.2, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The motorcycle is red and black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.7, "ram_available_mb": 112035.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10470.2, "ram_available_mb": 112036.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [31.26], "power_watts_avg": 31.26, "power_watts_peak": 31.26, "energy_joules_est": 2.81, "sample_count": 1, "duration_seconds": 0.09}, "timestamp": "2026-01-25T20:00:40.994694"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 256.099, "latencies_ms": [256.099], "images_per_second": 3.905, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "urn of water is on a table in front of a white wall with a light switch and a doorknob.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10470.2, "ram_available_mb": 112036.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10478.9, "ram_available_mb": 112027.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [31.26, 31.82, 31.82], "power_watts_avg": 31.63, "power_watts_peak": 31.82, "energy_joules_est": 8.12, "sample_count": 3, "duration_seconds": 0.257}, "timestamp": "2026-01-25T20:00:41.303415"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 86.531, "latencies_ms": [86.531], "images_per_second": 11.557, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Suitcase 0", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10478.9, "ram_available_mb": 112027.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10482.6, "ram_available_mb": 112023.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [31.82], "power_watts_avg": 31.82, "power_watts_peak": 31.82, "energy_joules_est": 2.76, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T20:00:41.408812"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 98.915, "latencies_ms": [98.915], "images_per_second": 10.11, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nThe man is wearing a suit and tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.6, "ram_available_mb": 112023.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10483.3, "ram_available_mb": 112023.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [31.82], "power_watts_avg": 31.82, "power_watts_peak": 31.82, "energy_joules_est": 3.16, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T20:00:41.513184"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 364.47, "latencies_ms": [364.47], "images_per_second": 2.744, "prompt_tokens": 757, "response_tokens_est": 64, "n_tiles": 1, "output_text": "\nIn the image, a man in a suit stands against a dark background. He is wearing a black suit jacket with a white collar and tie, which are both gray in color. The man's face is partially visible, but his expression appears to be serious or focused as he looks off into the distance.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.3, "ram_available_mb": 112023.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10490.7, "ram_available_mb": 112015.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [31.82, 35.02, 35.02, 35.02], "power_watts_avg": 34.22, "power_watts_peak": 35.02, "energy_joules_est": 12.48, "sample_count": 4, "duration_seconds": 0.365}, "timestamp": "2026-01-25T20:00:41.920020"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 108.425, "latencies_ms": [108.425], "images_per_second": 9.223, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The suit is black and the tie is blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10490.7, "ram_available_mb": 112015.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10489.6, "ram_available_mb": 112016.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [35.02, 35.02], "power_watts_avg": 35.02, "power_watts_peak": 35.02, "energy_joules_est": 3.82, "sample_count": 2, "duration_seconds": 0.109}, "timestamp": "2026-01-25T20:00:42.126648"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 290.494, "latencies_ms": [290.494], "images_per_second": 3.442, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA gray and white cat is sleeping on a pair of black and white sneakers, with its head resting comfortably on top of them.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10489.6, "ram_available_mb": 112016.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10489.9, "ram_available_mb": 112016.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [36.53, 36.53, 36.53], "power_watts_avg": 36.53, "power_watts_peak": 36.53, "energy_joules_est": 10.64, "sample_count": 3, "duration_seconds": 0.291}, "timestamp": "2026-01-25T20:00:42.437820"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 74.45, "latencies_ms": [74.45], "images_per_second": 13.432, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Sneakers", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10489.9, "ram_available_mb": 112016.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10489.9, "ram_available_mb": 112016.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [36.53], "power_watts_avg": 36.53, "power_watts_peak": 36.53, "energy_joules_est": 2.73, "sample_count": 1, "duration_seconds": 0.075}, "timestamp": "2026-01-25T20:00:42.543538"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 124.224, "latencies_ms": [124.224], "images_per_second": 8.05, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nA gray and white cat is sleeping on a pair of sneakers.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10490.1, "ram_available_mb": 112016.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10490.4, "ram_available_mb": 112015.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [35.69, 35.69], "power_watts_avg": 35.69, "power_watts_peak": 35.69, "energy_joules_est": 4.44, "sample_count": 2, "duration_seconds": 0.125}, "timestamp": "2026-01-25T20:00:42.750427"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 383.556, "latencies_ms": [383.556], "images_per_second": 2.607, "prompt_tokens": 757, "response_tokens_est": 64, "n_tiles": 1, "output_text": "\nIn the image, a gray cat is sleeping on top of an old pair of sneakers. The sneakers are placed next to each other with one on the left side and another on the right side of the frame. The cat appears to be curled up in a relaxed position as it takes a nap on the shoes.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 10490.4, "ram_available_mb": 112015.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10490.5, "ram_available_mb": 112015.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.69, 35.69, 35.69, 33.32], "power_watts_avg": 35.09, "power_watts_peak": 35.69, "energy_joules_est": 13.48, "sample_count": 4, "duration_seconds": 0.384}, "timestamp": "2026-01-25T20:00:43.157395"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 113.134, "latencies_ms": [113.134], "images_per_second": 8.839, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The cat is sleeping on a pair of sneakers.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10490.3, "ram_available_mb": 112016.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10487.3, "ram_available_mb": 112019.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.32, 33.32], "power_watts_avg": 33.32, "power_watts_peak": 33.32, "energy_joules_est": 3.78, "sample_count": 2, "duration_seconds": 0.114}, "timestamp": "2026-01-25T20:00:43.363398"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 790.318, "latencies_ms": [790.318], "images_per_second": 1.265, "prompt_tokens": 744, "response_tokens_est": 128, "n_tiles": 1, "output_text": "\u0e23\u0e31\u0e1a\u0e40\u0e21\u0e25\u0e32\u0e22\u0e19\u0e35\u0e01\u0e23\u0e30\u0e2a\u0e21\u0e34\u0e21\u0e27\u0e31\u0e14\u0e17\u0e2d\u0e07\u0e04\u0e23\u0e38\u0e21\u0e23\u0e39\u0e1b\u0e23\u0e32\u0e21\u0e32\u0e22\u0e19\u0e35\u0e49\u0e21\u0e32\u0e22\u0e19\u0e35\u0e49\u0e21\u0e32\u0e22\u0e19\u0e35\u0e49\u0e21\u0e32\u0e22\u0e19\u0e35\u0e49\u0e21\u0e32\u0e22\u0e19\u0e35\u0e49\u0e21\u0e32", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10487.2, "ram_available_mb": 112019.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10488.0, "ram_available_mb": 112018.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.32, 33.32, 32.99, 32.99, 32.99, 32.99, 32.99, 39.44], "power_watts_avg": 33.88, "power_watts_peak": 39.44, "energy_joules_est": 26.79, "sample_count": 8, "duration_seconds": 0.791}, "timestamp": "2026-01-25T20:00:44.180205"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 69.659, "latencies_ms": [69.659], "images_per_second": 14.356, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n Truck: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.0, "ram_available_mb": 112018.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10483.3, "ram_available_mb": 112023.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [39.44], "power_watts_avg": 39.44, "power_watts_peak": 39.44, "energy_joules_est": 2.76, "sample_count": 1, "duration_seconds": 0.07}, "timestamp": "2026-01-25T20:00:44.284825"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 131.119, "latencies_ms": [131.119], "images_per_second": 7.627, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n 1. Man in green vest and hat on top of truck.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.3, "ram_available_mb": 112023.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10483.9, "ram_available_mb": 112022.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [39.44, 39.44], "power_watts_avg": 39.44, "power_watts_peak": 39.44, "energy_joules_est": 5.18, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T20:00:44.490885"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 277.192, "latencies_ms": [277.192], "images_per_second": 3.608, "prompt_tokens": 757, "response_tokens_est": 47, "n_tiles": 1, "output_text": "\nA green dump truck with a white stripe on its side is parked in front of a building, with two people standing on top. One person is wearing a yellow vest, while the other has a red and black striped shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.9, "ram_available_mb": 112022.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10489.1, "ram_available_mb": 112017.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [39.44, 36.17, 36.17], "power_watts_avg": 37.26, "power_watts_peak": 39.44, "energy_joules_est": 10.34, "sample_count": 3, "duration_seconds": 0.277}, "timestamp": "2026-01-25T20:00:44.797341"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 86.18, "latencies_ms": [86.18], "images_per_second": 11.604, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The truck is green and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10489.1, "ram_available_mb": 112017.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10489.9, "ram_available_mb": 112016.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.17], "power_watts_avg": 36.17, "power_watts_peak": 36.17, "energy_joules_est": 3.14, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T20:00:44.902587"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 247.038, "latencies_ms": [247.038], "images_per_second": 4.048, "prompt_tokens": 744, "response_tokens_est": 20, "n_tiles": 1, "output_text": "urns of water are in a river, with rocks and boulders scattered throughout its surface.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10489.9, "ram_available_mb": 112016.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10491.9, "ram_available_mb": 112014.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [36.17, 36.17, 32.95], "power_watts_avg": 35.1, "power_watts_peak": 36.17, "energy_joules_est": 8.68, "sample_count": 3, "duration_seconds": 0.247}, "timestamp": "2026-01-25T20:00:45.211948"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 204.892, "latencies_ms": [204.892], "images_per_second": 4.881, "prompt_tokens": 759, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\n 1. Bridge  2. Bridge  3. Bridge  4. Bridge  5. Bridge  6. Bridge  7. Bridge  8. Bridge", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10491.9, "ram_available_mb": 112014.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10492.6, "ram_available_mb": 112013.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [32.95, 32.95, 32.95], "power_watts_avg": 32.95, "power_watts_peak": 32.95, "energy_joules_est": 6.77, "sample_count": 3, "duration_seconds": 0.206}, "timestamp": "2026-01-25T20:00:45.518404"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 126.449, "latencies_ms": [126.449], "images_per_second": 7.908, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nA bird is standing on a rock in front of a bridge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10492.6, "ram_available_mb": 112013.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10490.3, "ram_available_mb": 112016.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [32.95, 35.22], "power_watts_avg": 34.08, "power_watts_peak": 35.22, "energy_joules_est": 4.32, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:00:45.723751"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 334.189, "latencies_ms": [334.189], "images_per_second": 2.992, "prompt_tokens": 757, "response_tokens_est": 55, "n_tiles": 1, "output_text": "\nA bird stands on a rock in a river, surrounded by rocks. The water appears to be calm with no visible movement or disturbance. In the background, there are trees and a bridge, suggesting that this location might be near a park or a similar natural setting.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10490.3, "ram_available_mb": 112016.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10489.4, "ram_available_mb": 112016.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.22, 35.22, 35.22, 33.35], "power_watts_avg": 34.75, "power_watts_peak": 35.22, "energy_joules_est": 11.63, "sample_count": 4, "duration_seconds": 0.335}, "timestamp": "2026-01-25T20:00:46.129863"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 114.6, "latencies_ms": [114.6], "images_per_second": 8.726, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The image shows a bird standing on rocks in the water.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10489.4, "ram_available_mb": 112016.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10485.8, "ram_available_mb": 112020.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.35, 33.35], "power_watts_avg": 33.35, "power_watts_peak": 33.35, "energy_joules_est": 3.83, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T20:00:46.335041"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 255.849, "latencies_ms": [255.849], "images_per_second": 3.909, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA row of motorcycles is parked in front of a building, with a sign that reads \"press\".", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 10485.8, "ram_available_mb": 112020.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10487.1, "ram_available_mb": 112019.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.35, 33.35, 32.44], "power_watts_avg": 33.05, "power_watts_peak": 33.35, "energy_joules_est": 8.47, "sample_count": 3, "duration_seconds": 0.256}, "timestamp": "2026-01-25T20:00:46.649025"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 230.024, "latencies_ms": [230.024], "images_per_second": 4.347, "prompt_tokens": 759, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\n 1. Building  2. Building  3. Building  4. Building  5. Building  6. Building  7. Building  8. Building", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10487.1, "ram_available_mb": 112019.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10484.8, "ram_available_mb": 112021.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.44, 32.44, 32.44], "power_watts_avg": 32.44, "power_watts_peak": 32.44, "energy_joules_est": 7.47, "sample_count": 3, "duration_seconds": 0.23}, "timestamp": "2026-01-25T20:00:46.954998"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 183.454, "latencies_ms": [183.454], "images_per_second": 5.451, "prompt_tokens": 763, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\n 1. Front row of motorcycles is parked in front of a building with a green roof and red awning.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.8, "ram_available_mb": 112021.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10483.6, "ram_available_mb": 112022.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [32.44, 31.37], "power_watts_avg": 31.9, "power_watts_peak": 32.44, "energy_joules_est": 5.87, "sample_count": 2, "duration_seconds": 0.184}, "timestamp": "2026-01-25T20:00:47.160897"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 358.724, "latencies_ms": [358.724], "images_per_second": 2.788, "prompt_tokens": 757, "response_tokens_est": 62, "n_tiles": 1, "output_text": "\nA row of motorcycles, including a few scooters, are parked in front of a building with a green dome. The motorcycles are lined up neatly on the street, creating an organized display for passersby to admire. In addition to the motorcycles, there are several people walking around and observing the scene.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10483.6, "ram_available_mb": 112022.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10478.6, "ram_available_mb": 112027.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [31.37, 31.37, 31.37, 31.37], "power_watts_avg": 31.37, "power_watts_peak": 31.37, "energy_joules_est": 11.26, "sample_count": 4, "duration_seconds": 0.359}, "timestamp": "2026-01-25T20:00:47.567603"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 449.176, "latencies_ms": [449.176], "images_per_second": 2.226, "prompt_tokens": 756, "response_tokens_est": 79, "n_tiles": 1, "output_text": "\n The image shows a row of parked motorcycles lined up on the street. There are at least 12 motorcycles in various colors and styles, including black, silver, and green. Some of them have windshields while others do not. The motorcycles appear to be well-maintained and ready for use. The scene is set against an urban backdrop with buildings lining both sides of the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.6, "ram_available_mb": 112027.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10479.8, "ram_available_mb": 112026.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.28, 34.28, 34.28, 34.28, 34.28], "power_watts_avg": 34.28, "power_watts_peak": 34.28, "energy_joules_est": 15.41, "sample_count": 5, "duration_seconds": 0.449}, "timestamp": "2026-01-25T20:00:48.074435"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 314.097, "latencies_ms": [314.097], "images_per_second": 3.184, "prompt_tokens": 744, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA hand is holding a piece of broccoli, with the stem pointing towards the top left corner and the florets extending to the bottom right corner.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10479.8, "ram_available_mb": 112026.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [38.35, 38.35, 38.35, 38.35], "power_watts_avg": 38.35, "power_watts_peak": 38.35, "energy_joules_est": 12.06, "sample_count": 4, "duration_seconds": 0.314}, "timestamp": "2026-01-25T20:00:48.486765"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 77.832, "latencies_ms": [77.832], "images_per_second": 12.848, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Broccoli", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10470.1, "ram_available_mb": 112036.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [38.35], "power_watts_avg": 38.35, "power_watts_peak": 38.35, "energy_joules_est": 3.01, "sample_count": 1, "duration_seconds": 0.078}, "timestamp": "2026-01-25T20:00:48.592287"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 105.965, "latencies_ms": [105.965], "images_per_second": 9.437, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nThe hand is holding a piece of broccoli.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.1, "ram_available_mb": 112036.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10466.4, "ram_available_mb": 112039.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.36, 36.36], "power_watts_avg": 36.36, "power_watts_peak": 36.36, "energy_joules_est": 3.87, "sample_count": 2, "duration_seconds": 0.107}, "timestamp": "2026-01-25T20:00:48.798924"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 218.97, "latencies_ms": [218.97], "images_per_second": 4.567, "prompt_tokens": 757, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA person's hand is holding a piece of broccoli, with the stem pointing towards the top left corner. The broccoli appears to be fresh and green in color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.4, "ram_available_mb": 112039.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10482.3, "ram_available_mb": 112024.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.36, 36.36, 36.36], "power_watts_avg": 36.36, "power_watts_peak": 36.36, "energy_joules_est": 7.97, "sample_count": 3, "duration_seconds": 0.219}, "timestamp": "2026-01-25T20:00:49.106678"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 228.594, "latencies_ms": [228.594], "images_per_second": 4.375, "prompt_tokens": 756, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\n The image shows a person holding up a piece of broccoli. The broccoli is green and appears to be fresh. There are no other objects or people in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.3, "ram_available_mb": 112024.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10481.8, "ram_available_mb": 112024.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.69, 31.69, 31.69], "power_watts_avg": 31.69, "power_watts_peak": 31.69, "energy_joules_est": 7.26, "sample_count": 3, "duration_seconds": 0.229}, "timestamp": "2026-01-25T20:00:49.413618"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 268.457, "latencies_ms": [268.457], "images_per_second": 3.725, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urn of a person wearing a black jacket with fur trim and a red scarf around their neck, smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.8, "ram_available_mb": 112024.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10485.3, "ram_available_mb": 112021.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.69, 31.69, 28.81], "power_watts_avg": 30.73, "power_watts_peak": 31.69, "energy_joules_est": 8.27, "sample_count": 3, "duration_seconds": 0.269}, "timestamp": "2026-01-25T20:00:49.723720"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.054, "latencies_ms": [79.054], "images_per_second": 12.65, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Hat", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10485.3, "ram_available_mb": 112021.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10484.4, "ram_available_mb": 112021.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [28.81], "power_watts_avg": 28.81, "power_watts_peak": 28.81, "energy_joules_est": 2.29, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T20:00:49.828960"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 55.122, "latencies_ms": [55.122], "images_per_second": 18.141, "prompt_tokens": 763, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10484.4, "ram_available_mb": 112021.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10484.4, "ram_available_mb": 112021.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [28.81], "power_watts_avg": 28.81, "power_watts_peak": 28.81, "energy_joules_est": 1.6, "sample_count": 1, "duration_seconds": 0.055}, "timestamp": "2026-01-25T20:00:49.934543"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 48.536, "latencies_ms": [48.536], "images_per_second": 20.603, "prompt_tokens": 757, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.4, "ram_available_mb": 112021.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10488.8, "ram_available_mb": 112017.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [28.81], "power_watts_avg": 28.81, "power_watts_peak": 28.81, "energy_joules_est": 1.41, "sample_count": 1, "duration_seconds": 0.049}, "timestamp": "2026-01-25T20:00:50.040068"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 52.902, "latencies_ms": [52.902], "images_per_second": 18.903, "prompt_tokens": 756, "response_tokens_est": 2, "n_tiles": 1, "output_text": "!!!", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.8, "ram_available_mb": 112017.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 10490.6, "ram_available_mb": 112015.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [29.96], "power_watts_avg": 29.96, "power_watts_peak": 29.96, "energy_joules_est": 1.59, "sample_count": 1, "duration_seconds": 0.053}, "timestamp": "2026-01-25T20:00:50.144542"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 291.957, "latencies_ms": [291.957], "images_per_second": 3.425, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA man in a white shirt and black shorts is playing tennis on a blue court, holding his racket out to hit a yellow ball.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10490.6, "ram_available_mb": 112015.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10488.1, "ram_available_mb": 112018.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [29.96, 29.96, 29.96], "power_watts_avg": 29.96, "power_watts_peak": 29.96, "energy_joules_est": 8.76, "sample_count": 3, "duration_seconds": 0.293}, "timestamp": "2026-01-25T20:00:50.454995"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 74.514, "latencies_ms": [74.514], "images_per_second": 13.42, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Chair 1 - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.1, "ram_available_mb": 112018.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10483.4, "ram_available_mb": 112022.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [29.96], "power_watts_avg": 29.96, "power_watts_peak": 29.96, "energy_joules_est": 2.24, "sample_count": 1, "duration_seconds": 0.075}, "timestamp": "2026-01-25T20:00:50.559195"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 142.86, "latencies_ms": [142.86], "images_per_second": 7.0, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n 1. A man holding a tennis racket and ball on a tennis court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.4, "ram_available_mb": 112022.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10483.6, "ram_available_mb": 112022.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [32.27, 32.27], "power_watts_avg": 32.27, "power_watts_peak": 32.27, "energy_joules_est": 4.62, "sample_count": 2, "duration_seconds": 0.143}, "timestamp": "2026-01-25T20:00:50.764846"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 52.707, "latencies_ms": [52.707], "images_per_second": 18.973, "prompt_tokens": 757, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.6, "ram_available_mb": 112022.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10484.7, "ram_available_mb": 112021.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [32.27], "power_watts_avg": 32.27, "power_watts_peak": 32.27, "energy_joules_est": 1.71, "sample_count": 1, "duration_seconds": 0.053}, "timestamp": "2026-01-25T20:00:50.869610"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 115.886, "latencies_ms": [115.886], "images_per_second": 8.629, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The tennis player is wearing a white shirt and black shorts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.7, "ram_available_mb": 112021.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10485.3, "ram_available_mb": 112021.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [32.27, 32.27], "power_watts_avg": 32.27, "power_watts_peak": 32.27, "energy_joules_est": 3.75, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T20:00:51.074893"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 256.486, "latencies_ms": [256.486], "images_per_second": 3.899, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urn with a painting of two people on it, placed in front of a mirror and surrounded by white lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10485.3, "ram_available_mb": 112021.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10479.1, "ram_available_mb": 112027.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [29.71, 29.71, 29.71], "power_watts_avg": 29.71, "power_watts_peak": 29.71, "energy_joules_est": 7.64, "sample_count": 3, "duration_seconds": 0.257}, "timestamp": "2026-01-25T20:00:51.385942"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 62.707, "latencies_ms": [62.707], "images_per_second": 15.947, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Vase", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.1, "ram_available_mb": 112027.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10480.9, "ram_available_mb": 112025.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [29.71], "power_watts_avg": 29.71, "power_watts_peak": 29.71, "energy_joules_est": 1.88, "sample_count": 1, "duration_seconds": 0.063}, "timestamp": "2026-01-25T20:00:51.492861"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 129.921, "latencies_ms": [129.921], "images_per_second": 7.697, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n 1. A red vase with a picture of two people on it.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10480.9, "ram_available_mb": 112025.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10485.0, "ram_available_mb": 112021.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [29.71, 29.11], "power_watts_avg": 29.41, "power_watts_peak": 29.71, "energy_joules_est": 3.83, "sample_count": 2, "duration_seconds": 0.13}, "timestamp": "2026-01-25T20:00:51.698414"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 230.41, "latencies_ms": [230.41], "images_per_second": 4.34, "prompt_tokens": 757, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\nA red glass vase with a painting of two people on its side sits on a wooden table, surrounded by white lights. A candle is also present in front of the vase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10485.0, "ram_available_mb": 112021.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [29.11, 29.11, 29.11], "power_watts_avg": 29.11, "power_watts_peak": 29.11, "energy_joules_est": 6.73, "sample_count": 3, "duration_seconds": 0.231}, "timestamp": "2026-01-25T20:00:52.004931"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 80.093, "latencies_ms": [80.093], "images_per_second": 12.485, "prompt_tokens": 756, "response_tokens_est": 8, "n_tiles": 1, "output_text": "urn with a painting on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10476.8, "ram_available_mb": 112029.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 20.0}, "power_stats": {"power_watts_samples": [29.11], "power_watts_avg": 29.11, "power_watts_peak": 29.11, "energy_joules_est": 2.34, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T20:00:52.109948"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 227.997, "latencies_ms": [227.997], "images_per_second": 4.386, "prompt_tokens": 744, "response_tokens_est": 19, "n_tiles": 1, "output_text": "urn of water on a table, with a man in a gray sweater leaning over it.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 10476.8, "ram_available_mb": 112029.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10486.2, "ram_available_mb": 112020.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 20.0}, "power_stats": {"power_watts_samples": [31.66, 31.66, 31.66], "power_watts_avg": 31.67, "power_watts_peak": 31.66, "energy_joules_est": 7.24, "sample_count": 3, "duration_seconds": 0.229}, "timestamp": "2026-01-25T20:00:52.417139"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 121.205, "latencies_ms": [121.205], "images_per_second": 8.251, "prompt_tokens": 759, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n Chair 1 0.2\n Couch 2 0.3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10486.2, "ram_available_mb": 112020.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10490.1, "ram_available_mb": 112016.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 20.0}, "power_stats": {"power_watts_samples": [31.66, 31.66], "power_watts_avg": 31.66, "power_watts_peak": 31.66, "energy_joules_est": 3.86, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:00:52.622372"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 131.368, "latencies_ms": [131.368], "images_per_second": 7.612, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A man in a grey shirt is standing next to a camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10490.1, "ram_available_mb": 112016.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10489.4, "ram_available_mb": 112016.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 20.0}, "power_stats": {"power_watts_samples": [32.91, 32.91], "power_watts_avg": 32.91, "power_watts_peak": 32.91, "energy_joules_est": 4.33, "sample_count": 2, "duration_seconds": 0.132}, "timestamp": "2026-01-25T20:00:52.828311"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 281.568, "latencies_ms": [281.568], "images_per_second": 3.552, "prompt_tokens": 757, "response_tokens_est": 45, "n_tiles": 1, "output_text": "\nIn a living room, two men are standing in front of a couch. One man has his head tilted back with his arms crossed over his chest, while the other man is holding a camera to take a picture.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10489.4, "ram_available_mb": 112016.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10485.4, "ram_available_mb": 112020.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.91, 32.91, 31.35], "power_watts_avg": 32.39, "power_watts_peak": 32.91, "energy_joules_est": 9.13, "sample_count": 3, "duration_seconds": 0.282}, "timestamp": "2026-01-25T20:00:53.134678"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 89.336, "latencies_ms": [89.336], "images_per_second": 11.194, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "urn of water on the floor.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10485.4, "ram_available_mb": 112020.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10484.4, "ram_available_mb": 112021.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.35], "power_watts_avg": 31.35, "power_watts_peak": 31.35, "energy_joules_est": 2.81, "sample_count": 1, "duration_seconds": 0.09}, "timestamp": "2026-01-25T20:00:53.240343"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 283.81, "latencies_ms": [283.81], "images_per_second": 3.523, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA woman with short hair and a black hat is captured in a moment of joy, smiling brightly while holding a cigarette between her fingers.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10484.4, "ram_available_mb": 112021.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10482.2, "ram_available_mb": 112024.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.35, 31.35, 31.35], "power_watts_avg": 31.35, "power_watts_peak": 31.35, "energy_joules_est": 8.9, "sample_count": 3, "duration_seconds": 0.284}, "timestamp": "2026-01-25T20:00:53.549961"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 74.512, "latencies_ms": [74.512], "images_per_second": 13.421, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n Hat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.2, "ram_available_mb": 112024.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10481.0, "ram_available_mb": 112025.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.29], "power_watts_avg": 34.29, "power_watts_peak": 34.29, "energy_joules_est": 2.57, "sample_count": 1, "duration_seconds": 0.075}, "timestamp": "2026-01-25T20:00:53.670100"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 129.801, "latencies_ms": [129.801], "images_per_second": 7.704, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nThe woman is wearing a hat and holding a cigarette in her mouth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.0, "ram_available_mb": 112025.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10481.2, "ram_available_mb": 112025.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.29, 34.29], "power_watts_avg": 34.29, "power_watts_peak": 34.29, "energy_joules_est": 4.46, "sample_count": 2, "duration_seconds": 0.13}, "timestamp": "2026-01-25T20:00:53.875739"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 44.232, "latencies_ms": [44.232], "images_per_second": 22.608, "prompt_tokens": 757, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10481.2, "ram_available_mb": 112025.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10480.9, "ram_available_mb": 112025.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.29], "power_watts_avg": 34.29, "power_watts_peak": 34.29, "energy_joules_est": 1.53, "sample_count": 1, "duration_seconds": 0.045}, "timestamp": "2026-01-25T20:00:53.980475"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 122.186, "latencies_ms": [122.186], "images_per_second": 8.184, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n The woman is wearing a hat and has a cigarette in her mouth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.9, "ram_available_mb": 112025.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10482.7, "ram_available_mb": 112023.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [34.29, 31.66], "power_watts_avg": 32.98, "power_watts_peak": 34.29, "energy_joules_est": 4.04, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:00:54.186445"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 288.053, "latencies_ms": [288.053], "images_per_second": 3.472, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nTwo zebras are grazing in a grassy field, with one zebra closer to the foreground and the other slightly further away.", "error": null, "sys_before": {"cpu_percent": 3.7, "ram_used_mb": 10482.7, "ram_available_mb": 112023.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [31.66, 31.66, 31.66], "power_watts_avg": 31.67, "power_watts_peak": 31.66, "energy_joules_est": 9.14, "sample_count": 3, "duration_seconds": 0.289}, "timestamp": "2026-01-25T20:00:54.501492"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 102.327, "latencies_ms": [102.327], "images_per_second": 9.773, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Zebra in foreground", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.8, "ram_available_mb": 112033.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [31.66, 30.25], "power_watts_avg": 30.96, "power_watts_peak": 31.66, "energy_joules_est": 3.17, "sample_count": 2, "duration_seconds": 0.103}, "timestamp": "2026-01-25T20:00:54.707048"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 124.416, "latencies_ms": [124.416], "images_per_second": 8.038, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Zebra in front of tree on left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10465.5, "ram_available_mb": 112040.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [30.25, 30.25], "power_watts_avg": 30.25, "power_watts_peak": 30.25, "energy_joules_est": 3.78, "sample_count": 2, "duration_seconds": 0.125}, "timestamp": "2026-01-25T20:00:54.911934"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 170.215, "latencies_ms": [170.215], "images_per_second": 5.875, "prompt_tokens": 757, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\n Two zebras are grazing in a grassy field, with one zebra standing near a large rock.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.5, "ram_available_mb": 112040.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [30.25, 30.25], "power_watts_avg": 30.25, "power_watts_peak": 30.25, "energy_joules_est": 5.16, "sample_count": 2, "duration_seconds": 0.171}, "timestamp": "2026-01-25T20:00:55.118470"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 715.673, "latencies_ms": [715.673], "images_per_second": 1.397, "prompt_tokens": 756, "response_tokens_est": 128, "n_tiles": 1, "output_text": "\n The image features two zebras grazing in a grassy field. One zebra is standing on the left side of the frame and the other one is positioned more towards the center. Both zebras have distinct black and white stripes that stand out against their surroundings. The background includes trees, rocks, and a stone wall, creating an interesting contrast between the natural elements in the foreground and the man-made structures in the background.\nThe lighting appears to be coming from the top left corner of the image, casting shadows on the zebras as they graze. This creates a sense of depth and dimension within the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.4, "ram_available_mb": 112041.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10457.0, "ram_available_mb": 112049.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [30.84, 30.84, 30.84, 30.84, 30.84, 30.28, 30.28, 30.28], "power_watts_avg": 30.63, "power_watts_peak": 30.84, "energy_joules_est": 21.94, "sample_count": 8, "duration_seconds": 0.716}, "timestamp": "2026-01-25T20:00:55.924015"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 228.133, "latencies_ms": [228.133], "images_per_second": 4.383, "prompt_tokens": 744, "response_tokens_est": 18, "n_tiles": 1, "output_text": "urn with a chain on it, saying \"olde\" and \"1935\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.0, "ram_available_mb": 112049.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 36.0}, "power_stats": {"power_watts_samples": [30.28, 37.45, 37.45], "power_watts_avg": 35.06, "power_watts_peak": 37.45, "energy_joules_est": 8.02, "sample_count": 3, "duration_seconds": 0.229}, "timestamp": "2026-01-25T20:00:56.236213"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 510.735, "latencies_ms": [510.735], "images_per_second": 1.958, "prompt_tokens": 759, "response_tokens_est": 92, "n_tiles": 1, "output_text": "\n 1. Pink flowers 2. Green leaves 3. Gray stone wall 4. Brown fire hydrant 5. Black chain 6. Brown brick wall 7. Gray stone wall 8. Brown brick wall 9. Brown brick wall 10. Brown brick wall 11. Brown brick wall 12. Brown brick wall 13. Brown brick wall 14. Brown brick wall 15. Brown brick wall 16. Brown brick wall 17. Brown brick wall 18. Brown brick wall 19.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10471.6, "ram_available_mb": 112034.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 36.0}, "power_stats": {"power_watts_samples": [37.45, 37.45, 37.45, 38.3, 38.3, 38.3], "power_watts_avg": 37.88, "power_watts_peak": 38.3, "energy_joules_est": 19.38, "sample_count": 6, "duration_seconds": 0.512}, "timestamp": "2026-01-25T20:00:56.844431"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 79.462, "latencies_ms": [79.462], "images_per_second": 12.585, "prompt_tokens": 763, "response_tokens_est": 6, "n_tiles": 1, "output_text": "urn is on ground.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10471.6, "ram_available_mb": 112034.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10474.6, "ram_available_mb": 112031.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 36.0}, "power_stats": {"power_watts_samples": [38.3], "power_watts_avg": 38.3, "power_watts_peak": 38.3, "energy_joules_est": 3.07, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T20:00:56.949868"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 145.587, "latencies_ms": [145.587], "images_per_second": 6.869, "prompt_tokens": 757, "response_tokens_est": 19, "n_tiles": 1, "output_text": "urn with a chain on top of it, saying \"old dog\" in white letters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.6, "ram_available_mb": 112031.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10478.0, "ram_available_mb": 112028.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [38.3, 34.7], "power_watts_avg": 36.5, "power_watts_peak": 38.3, "energy_joules_est": 5.32, "sample_count": 2, "duration_seconds": 0.146}, "timestamp": "2026-01-25T20:00:57.155190"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 244.204, "latencies_ms": [244.204], "images_per_second": 4.095, "prompt_tokens": 756, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nThe fire hydrant is painted in a faded orange color and has rust on it. The surrounding area appears to be dirty with leaves scattered around the base of the hydrant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.0, "ram_available_mb": 112028.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10478.0, "ram_available_mb": 112028.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [34.7, 34.7, 34.7], "power_watts_avg": 34.7, "power_watts_peak": 34.7, "energy_joules_est": 8.48, "sample_count": 3, "duration_seconds": 0.244}, "timestamp": "2026-01-25T20:00:57.460880"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 235.244, "latencies_ms": [235.244], "images_per_second": 4.251, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nTwo brown bears are walking on a dirt road, with one bear slightly ahead of the other.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10478.0, "ram_available_mb": 112028.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10478.5, "ram_available_mb": 112027.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [34.7, 30.73, 30.73], "power_watts_avg": 32.05, "power_watts_peak": 34.7, "energy_joules_est": 7.56, "sample_count": 3, "duration_seconds": 0.236}, "timestamp": "2026-01-25T20:00:57.775663"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 70.701, "latencies_ms": [70.701], "images_per_second": 14.144, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Bear", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.5, "ram_available_mb": 112027.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10478.5, "ram_available_mb": 112027.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [30.73], "power_watts_avg": 30.73, "power_watts_peak": 30.73, "energy_joules_est": 2.18, "sample_count": 1, "duration_seconds": 0.071}, "timestamp": "2026-01-25T20:00:57.881280"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 102.875, "latencies_ms": [102.875], "images_per_second": 9.721, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nA brown bear is walking towards a camera.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10478.5, "ram_available_mb": 112027.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10478.9, "ram_available_mb": 112027.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [30.73, 30.73], "power_watts_avg": 30.73, "power_watts_peak": 30.73, "energy_joules_est": 3.17, "sample_count": 2, "duration_seconds": 0.103}, "timestamp": "2026-01-25T20:00:58.087017"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 123.123, "latencies_ms": [123.123], "images_per_second": 8.122, "prompt_tokens": 757, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n Two brown bears are walking on a dirt road in a field.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10478.9, "ram_available_mb": 112027.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10480.4, "ram_available_mb": 112025.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [32.99, 32.99], "power_watts_avg": 32.99, "power_watts_peak": 32.99, "energy_joules_est": 4.08, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T20:00:58.292091"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 86.819, "latencies_ms": [86.819], "images_per_second": 11.518, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The bear is brown and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.4, "ram_available_mb": 112025.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10480.4, "ram_available_mb": 112025.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [32.99], "power_watts_avg": 32.99, "power_watts_peak": 32.99, "energy_joules_est": 2.89, "sample_count": 1, "duration_seconds": 0.088}, "timestamp": "2026-01-25T20:00:58.396856"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 262.947, "latencies_ms": [262.947], "images_per_second": 3.803, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urn of water on a dirt ground, with a young boy in white shirt and tie kneeling next to it.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10480.4, "ram_available_mb": 112025.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10480.1, "ram_available_mb": 112026.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [32.99, 32.99, 31.39], "power_watts_avg": 32.46, "power_watts_peak": 32.99, "energy_joules_est": 8.54, "sample_count": 3, "duration_seconds": 0.263}, "timestamp": "2026-01-25T20:00:58.709781"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 71.676, "latencies_ms": [71.676], "images_per_second": 13.952, "prompt_tokens": 759, "response_tokens_est": 4, "n_tiles": 1, "output_text": "\n Bucket 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.1, "ram_available_mb": 112026.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10479.9, "ram_available_mb": 112026.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [31.39], "power_watts_avg": 31.39, "power_watts_peak": 31.39, "energy_joules_est": 2.27, "sample_count": 1, "duration_seconds": 0.072}, "timestamp": "2026-01-25T20:00:58.815386"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 149.325, "latencies_ms": [149.325], "images_per_second": 6.697, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nA young boy in a white shirt and tie is crouching down next to a bucket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.9, "ram_available_mb": 112026.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10480.6, "ram_available_mb": 112025.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [31.39, 31.39], "power_watts_avg": 31.39, "power_watts_peak": 31.39, "energy_joules_est": 4.71, "sample_count": 2, "duration_seconds": 0.15}, "timestamp": "2026-01-25T20:00:59.021478"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 185.084, "latencies_ms": [185.084], "images_per_second": 5.403, "prompt_tokens": 757, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA young child in a white shirt, tie with fruit patterns, and khaki pants kneels on the ground next to a bucket.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10480.6, "ram_available_mb": 112025.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10480.6, "ram_available_mb": 112025.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [31.39, 29.72], "power_watts_avg": 30.55, "power_watts_peak": 31.39, "energy_joules_est": 5.67, "sample_count": 2, "duration_seconds": 0.186}, "timestamp": "2026-01-25T20:00:59.227240"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 148.645, "latencies_ms": [148.645], "images_per_second": 6.727, "prompt_tokens": 756, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n The image is a black and white photo of a young boy wearing a tie with fruit designs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.6, "ram_available_mb": 112025.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10480.6, "ram_available_mb": 112025.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [29.72, 29.72], "power_watts_avg": 29.72, "power_watts_peak": 29.72, "energy_joules_est": 4.43, "sample_count": 2, "duration_seconds": 0.149}, "timestamp": "2026-01-25T20:00:59.432452"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 291.363, "latencies_ms": [291.363], "images_per_second": 3.432, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "urns of flowers and beer bottles are scattered on a table in a desert-like setting, with a stuffed bear sitting atop it.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.6, "ram_available_mb": 112025.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10482.4, "ram_available_mb": 112023.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [29.72, 32.94, 32.94], "power_watts_avg": 31.87, "power_watts_peak": 32.94, "energy_joules_est": 9.3, "sample_count": 3, "duration_seconds": 0.292}, "timestamp": "2026-01-25T20:00:59.740689"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 83.788, "latencies_ms": [83.788], "images_per_second": 11.935, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Teddy bear: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.4, "ram_available_mb": 112023.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10483.7, "ram_available_mb": 112022.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [32.94], "power_watts_avg": 32.94, "power_watts_peak": 32.94, "energy_joules_est": 2.77, "sample_count": 1, "duration_seconds": 0.084}, "timestamp": "2026-01-25T20:00:59.846103"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 138.174, "latencies_ms": [138.174], "images_per_second": 7.237, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n A bear on a chair is next to a table with beer bottles and flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.7, "ram_available_mb": 112022.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10479.8, "ram_available_mb": 112026.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 69.0}, "power_stats": {"power_watts_samples": [32.94, 32.94], "power_watts_avg": 32.94, "power_watts_peak": 32.94, "energy_joules_est": 4.56, "sample_count": 2, "duration_seconds": 0.138}, "timestamp": "2026-01-25T20:01:00.050982"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 283.608, "latencies_ms": [283.608], "images_per_second": 3.526, "prompt_tokens": 757, "response_tokens_est": 46, "n_tiles": 1, "output_text": "\nIn a desert-like environment, there are two teddy bears sitting on an ottoman. The ottoman also holds several beer bottles and cups scattered around it. A chair can be seen nearby as well.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.8, "ram_available_mb": 112026.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10475.7, "ram_available_mb": 112030.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 69.0}, "power_stats": {"power_watts_samples": [35.13, 35.13, 35.13], "power_watts_avg": 35.13, "power_watts_peak": 35.13, "energy_joules_est": 9.98, "sample_count": 3, "duration_seconds": 0.284}, "timestamp": "2026-01-25T20:01:00.356498"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 127.618, "latencies_ms": [127.618], "images_per_second": 7.836, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A brown wooden table with a green and white striped cloth on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.7, "ram_available_mb": 112030.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10478.4, "ram_available_mb": 112027.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 69.0}, "power_stats": {"power_watts_samples": [35.13, 35.13], "power_watts_avg": 35.13, "power_watts_peak": 35.13, "energy_joules_est": 4.49, "sample_count": 2, "duration_seconds": 0.128}, "timestamp": "2026-01-25T20:01:00.563606"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 270.772, "latencies_ms": [270.772], "images_per_second": 3.693, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\n A group of boats are docked at a pier, with some in the water and others on land.", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 10478.4, "ram_available_mb": 112027.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10475.2, "ram_available_mb": 112031.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 69.0}, "power_stats": {"power_watts_samples": [35.46, 35.46, 35.46], "power_watts_avg": 35.46, "power_watts_peak": 35.46, "energy_joules_est": 9.61, "sample_count": 3, "duration_seconds": 0.271}, "timestamp": "2026-01-25T20:01:00.878861"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 86.757, "latencies_ms": [86.757], "images_per_second": 11.527, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Boat: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 69.0}, "power_stats": {"power_watts_samples": [35.46], "power_watts_avg": 35.46, "power_watts_peak": 35.46, "energy_joules_est": 3.09, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T20:01:00.984250"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 111.691, "latencies_ms": [111.691], "images_per_second": 8.953, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Boat on right side of image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10476.5, "ram_available_mb": 112029.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [35.46, 33.09], "power_watts_avg": 34.28, "power_watts_peak": 35.46, "energy_joules_est": 3.83, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T20:01:01.190004"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 117.577, "latencies_ms": [117.577], "images_per_second": 8.505, "prompt_tokens": 757, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A harbor with several boats docked, including a green boat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.5, "ram_available_mb": 112029.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10475.7, "ram_available_mb": 112030.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [33.09, 33.09], "power_watts_avg": 33.09, "power_watts_peak": 33.09, "energy_joules_est": 3.91, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:01:01.396568"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 130.236, "latencies_ms": [130.236], "images_per_second": 7.678, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A boat docked at a pier with another boat in the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.7, "ram_available_mb": 112030.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10473.3, "ram_available_mb": 112033.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [33.09, 33.09], "power_watts_avg": 33.09, "power_watts_peak": 33.09, "energy_joules_est": 4.33, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T20:01:01.603390"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 231.224, "latencies_ms": [231.224], "images_per_second": 4.325, "prompt_tokens": 744, "response_tokens_est": 18, "n_tiles": 1, "output_text": "urn of hot sauce on a table, with a woman in black clothing eating it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.3, "ram_available_mb": 112033.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10473.8, "ram_available_mb": 112032.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [29.57, 29.57, 29.57], "power_watts_avg": 29.57, "power_watts_peak": 29.57, "energy_joules_est": 6.86, "sample_count": 3, "duration_seconds": 0.232}, "timestamp": "2026-01-25T20:01:01.911965"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 75.453, "latencies_ms": [75.453], "images_per_second": 13.253, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Hot dog", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.8, "ram_available_mb": 112032.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10478.4, "ram_available_mb": 112027.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [29.57], "power_watts_avg": 29.57, "power_watts_peak": 29.57, "energy_joules_est": 2.24, "sample_count": 1, "duration_seconds": 0.076}, "timestamp": "2026-01-25T20:01:02.017569"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 127.538, "latencies_ms": [127.538], "images_per_second": 7.841, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe woman is eating a hot dog in front of her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.4, "ram_available_mb": 112027.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10478.9, "ram_available_mb": 112027.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [29.57, 28.89], "power_watts_avg": 29.23, "power_watts_peak": 29.57, "energy_joules_est": 3.73, "sample_count": 2, "duration_seconds": 0.128}, "timestamp": "2026-01-25T20:01:02.224102"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 142.067, "latencies_ms": [142.067], "images_per_second": 7.039, "prompt_tokens": 757, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nA woman with short hair is eating a hot dog in front of a dark background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.9, "ram_available_mb": 112027.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10481.3, "ram_available_mb": 112025.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [28.89, 28.89], "power_watts_avg": 28.89, "power_watts_peak": 28.89, "energy_joules_est": 4.12, "sample_count": 2, "duration_seconds": 0.143}, "timestamp": "2026-01-25T20:01:02.430751"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 199.02, "latencies_ms": [199.02], "images_per_second": 5.025, "prompt_tokens": 756, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\n The image shows a woman with short hair eating a hot dog. She is wearing a black scarf around her neck and has brown eyes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.3, "ram_available_mb": 112025.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10481.0, "ram_available_mb": 112025.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [28.89, 29.49], "power_watts_avg": 29.19, "power_watts_peak": 29.49, "energy_joules_est": 5.82, "sample_count": 2, "duration_seconds": 0.199}, "timestamp": "2026-01-25T20:01:02.635299"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 241.71, "latencies_ms": [241.71], "images_per_second": 4.137, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urns of water on a table, and a man in a suit holding a glass of wine.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.0, "ram_available_mb": 112025.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10478.6, "ram_available_mb": 112027.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [29.49, 29.49, 29.49], "power_watts_avg": 29.49, "power_watts_peak": 29.49, "energy_joules_est": 7.14, "sample_count": 3, "duration_seconds": 0.242}, "timestamp": "2026-01-25T20:01:02.947827"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 180.693, "latencies_ms": [180.693], "images_per_second": 5.534, "prompt_tokens": 759, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\n 1. Doorway  2. Mirror  3. Glass  4. Jacket  5. Tie  6. Glass", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10478.6, "ram_available_mb": 112027.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10477.4, "ram_available_mb": 112028.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 14.0}, "power_stats": {"power_watts_samples": [29.49, 32.36], "power_watts_avg": 30.92, "power_watts_peak": 32.36, "energy_joules_est": 5.6, "sample_count": 2, "duration_seconds": 0.181}, "timestamp": "2026-01-25T20:01:03.153913"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 143.402, "latencies_ms": [143.402], "images_per_second": 6.973, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nThe man is wearing a suit and tie while holding a glass of wine.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.4, "ram_available_mb": 112028.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10483.5, "ram_available_mb": 112022.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 14.0}, "power_stats": {"power_watts_samples": [32.36, 32.36], "power_watts_avg": 32.36, "power_watts_peak": 32.36, "energy_joules_est": 4.65, "sample_count": 2, "duration_seconds": 0.144}, "timestamp": "2026-01-25T20:01:03.359323"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 281.317, "latencies_ms": [281.317], "images_per_second": 3.555, "prompt_tokens": 757, "response_tokens_est": 45, "n_tiles": 1, "output_text": "\nIn this image, a man in a suit and tie stands next to a woman wearing a green dress. The man holds a glass of wine while they both stand against a white wall with a door visible behind them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.5, "ram_available_mb": 112022.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10485.2, "ram_available_mb": 112021.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 14.0}, "power_stats": {"power_watts_samples": [32.36, 32.36, 35.7], "power_watts_avg": 33.47, "power_watts_peak": 35.7, "energy_joules_est": 9.43, "sample_count": 3, "duration_seconds": 0.282}, "timestamp": "2026-01-25T20:01:03.665680"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 332.169, "latencies_ms": [332.169], "images_per_second": 3.011, "prompt_tokens": 756, "response_tokens_est": 55, "n_tiles": 1, "output_text": "\n The image shows a man and woman dressed in formal attire standing next to each other. The man is wearing a black suit with a tie, while the woman is wearing a green dress. They are both holding glasses of wine, which they appear to be enjoying together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10485.2, "ram_available_mb": 112021.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10486.2, "ram_available_mb": 112020.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [35.7, 35.7, 35.7, 35.7], "power_watts_avg": 35.7, "power_watts_peak": 35.7, "energy_joules_est": 11.88, "sample_count": 4, "duration_seconds": 0.333}, "timestamp": "2026-01-25T20:01:04.071872"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 258.276, "latencies_ms": [258.276], "images_per_second": 3.872, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urns, vases, and other knick-knacks are displayed on a blue shelf in an antique store.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10486.2, "ram_available_mb": 112020.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10488.3, "ram_available_mb": 112018.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [34.88, 34.88, 34.88], "power_watts_avg": 34.88, "power_watts_peak": 34.88, "energy_joules_est": 9.02, "sample_count": 3, "duration_seconds": 0.259}, "timestamp": "2026-01-25T20:01:04.381620"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.154, "latencies_ms": [79.154], "images_per_second": 12.634, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n Chair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.3, "ram_available_mb": 112018.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10486.1, "ram_available_mb": 112020.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [34.88], "power_watts_avg": 34.88, "power_watts_peak": 34.88, "energy_joules_est": 2.79, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T20:01:04.487452"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 102.323, "latencies_ms": [102.323], "images_per_second": 9.773, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urns and cups on a blue shelf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10486.1, "ram_available_mb": 112020.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10484.2, "ram_available_mb": 112022.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [34.88, 35.92], "power_watts_avg": 35.4, "power_watts_peak": 35.92, "energy_joules_est": 3.63, "sample_count": 2, "duration_seconds": 0.103}, "timestamp": "2026-01-25T20:01:04.694377"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 173.28, "latencies_ms": [173.28], "images_per_second": 5.771, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urns, vases, cups, and a book are displayed on a blue shelf in an antique store.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10484.2, "ram_available_mb": 112022.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10488.0, "ram_available_mb": 112018.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [35.92, 35.92], "power_watts_avg": 35.92, "power_watts_peak": 35.92, "energy_joules_est": 6.23, "sample_count": 2, "duration_seconds": 0.174}, "timestamp": "2026-01-25T20:01:04.900191"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 257.377, "latencies_ms": [257.377], "images_per_second": 3.885, "prompt_tokens": 756, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\nThe blue shelf is made of wood and has a metal vase on it. The shelf also holds several other items including a book, a candle holder with candles, and some cups.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10487.6, "ram_available_mb": 112018.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10487.6, "ram_available_mb": 112018.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [35.92, 35.92, 30.25], "power_watts_avg": 34.03, "power_watts_peak": 35.92, "energy_joules_est": 8.77, "sample_count": 3, "duration_seconds": 0.258}, "timestamp": "2026-01-25T20:01:05.206800"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 259.355, "latencies_ms": [259.355], "images_per_second": 3.856, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA white plate holds four pieces of toast with a layer of butter on each, arranged in a circular pattern.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10487.6, "ram_available_mb": 112018.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10490.1, "ram_available_mb": 112016.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [30.25, 30.25, 30.25], "power_watts_avg": 30.25, "power_watts_peak": 30.25, "energy_joules_est": 7.86, "sample_count": 3, "duration_seconds": 0.26}, "timestamp": "2026-01-25T20:01:05.516265"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 73.427, "latencies_ms": [73.427], "images_per_second": 13.619, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Sandwich 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10490.1, "ram_available_mb": 112016.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10494.8, "ram_available_mb": 112011.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [30.25], "power_watts_avg": 30.25, "power_watts_peak": 30.25, "energy_joules_est": 2.24, "sample_count": 1, "duration_seconds": 0.074}, "timestamp": "2026-01-25T20:01:05.622000"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 137.452, "latencies_ms": [137.452], "images_per_second": 7.275, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n 1. A plate of toast with butter and jam on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10494.8, "ram_available_mb": 112011.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10495.5, "ram_available_mb": 112010.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [33.34, 33.34], "power_watts_avg": 33.34, "power_watts_peak": 33.34, "energy_joules_est": 4.6, "sample_count": 2, "duration_seconds": 0.138}, "timestamp": "2026-01-25T20:01:05.828570"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 218.149, "latencies_ms": [218.149], "images_per_second": 4.584, "prompt_tokens": 757, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA white plate holds four pieces of toast with butter on them, arranged in a circle. The plate rests on a wooden table, which also has a cell phone nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10495.5, "ram_available_mb": 112010.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10491.6, "ram_available_mb": 112014.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.34, 33.34, 34.95], "power_watts_avg": 33.88, "power_watts_peak": 34.95, "energy_joules_est": 7.4, "sample_count": 3, "duration_seconds": 0.218}, "timestamp": "2026-01-25T20:01:06.132813"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 122.145, "latencies_ms": [122.145], "images_per_second": 8.187, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n The plate is white and has a black cell phone on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10491.6, "ram_available_mb": 112014.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10491.1, "ram_available_mb": 112015.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.95, 34.95], "power_watts_avg": 34.95, "power_watts_peak": 34.95, "energy_joules_est": 4.28, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:01:06.337956"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 257.963, "latencies_ms": [257.963], "images_per_second": 3.877, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urn of light colored liquid sits on a table, with a man in a suit adjusting his tie nearby.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10491.1, "ram_available_mb": 112015.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10493.3, "ram_available_mb": 112013.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.95, 34.95, 30.9], "power_watts_avg": 33.6, "power_watts_peak": 34.95, "energy_joules_est": 8.68, "sample_count": 3, "duration_seconds": 0.258}, "timestamp": "2026-01-25T20:01:06.646217"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 69.667, "latencies_ms": [69.667], "images_per_second": 14.354, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Glasses", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10493.3, "ram_available_mb": 112013.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10493.5, "ram_available_mb": 112012.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.9], "power_watts_avg": 30.9, "power_watts_peak": 30.9, "energy_joules_est": 2.16, "sample_count": 1, "duration_seconds": 0.07}, "timestamp": "2026-01-25T20:01:06.750665"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 90.777, "latencies_ms": [90.777], "images_per_second": 11.016, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nThe man is wearing a suit and tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10493.5, "ram_available_mb": 112012.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10493.5, "ram_available_mb": 112012.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.9], "power_watts_avg": 30.9, "power_watts_peak": 30.9, "energy_joules_est": 2.81, "sample_count": 1, "duration_seconds": 0.091}, "timestamp": "2026-01-25T20:01:06.855840"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 56.014, "latencies_ms": [56.014], "images_per_second": 17.853, "prompt_tokens": 757, "response_tokens_est": 2, "n_tiles": 1, "output_text": "urn", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10493.5, "ram_available_mb": 112012.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10493.5, "ram_available_mb": 112012.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.9], "power_watts_avg": 30.9, "power_watts_peak": 30.9, "energy_joules_est": 1.74, "sample_count": 1, "duration_seconds": 0.056}, "timestamp": "2026-01-25T20:01:06.959845"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 208.715, "latencies_ms": [208.715], "images_per_second": 4.791, "prompt_tokens": 756, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\n The man is wearing a black suit and tie with colorful lights on it. He has glasses on his face and is looking up at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10493.5, "ram_available_mb": 112012.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10490.6, "ram_available_mb": 112015.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 8.0}, "power_stats": {"power_watts_samples": [30.9, 29.47, 29.47], "power_watts_avg": 29.95, "power_watts_peak": 30.9, "energy_joules_est": 6.26, "sample_count": 3, "duration_seconds": 0.209}, "timestamp": "2026-01-25T20:01:07.265456"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 241.156, "latencies_ms": [241.156], "images_per_second": 4.147, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA man walks across a street at night, with a building and traffic lights in the background.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10490.6, "ram_available_mb": 112015.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10497.5, "ram_available_mb": 112008.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 8.0}, "power_stats": {"power_watts_samples": [29.47, 29.47, 29.47], "power_watts_avg": 29.47, "power_watts_peak": 29.47, "energy_joules_est": 7.13, "sample_count": 3, "duration_seconds": 0.242}, "timestamp": "2026-01-25T20:01:07.576445"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 81.526, "latencies_ms": [81.526], "images_per_second": 12.266, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Traffic light", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 10497.5, "ram_available_mb": 112008.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10497.2, "ram_available_mb": 112009.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 8.0}, "power_stats": {"power_watts_samples": [32.8], "power_watts_avg": 32.8, "power_watts_peak": 32.8, "energy_joules_est": 2.68, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T20:01:07.679905"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 131.876, "latencies_ms": [131.876], "images_per_second": 7.583, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nA man walking across a street in front of a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10497.2, "ram_available_mb": 112009.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10505.0, "ram_available_mb": 112001.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 8.0}, "power_stats": {"power_watts_samples": [32.8, 32.8], "power_watts_avg": 32.8, "power_watts_peak": 32.8, "energy_joules_est": 4.33, "sample_count": 2, "duration_seconds": 0.132}, "timestamp": "2026-01-25T20:01:07.884343"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 189.166, "latencies_ms": [189.166], "images_per_second": 5.286, "prompt_tokens": 757, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA man walks across a street at night, holding a camera. The building he passes by has a sign that says \"Tacos\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10505.0, "ram_available_mb": 112001.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10502.2, "ram_available_mb": 112004.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [32.8, 32.8], "power_watts_avg": 32.8, "power_watts_peak": 32.8, "energy_joules_est": 6.22, "sample_count": 2, "duration_seconds": 0.19}, "timestamp": "2026-01-25T20:01:08.089355"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 111.993, "latencies_ms": [111.993], "images_per_second": 8.929, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The man is walking across the street at night.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10502.2, "ram_available_mb": 112004.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 10503.2, "ram_available_mb": 112003.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [32.64, 32.64], "power_watts_avg": 32.64, "power_watts_peak": 32.64, "energy_joules_est": 3.68, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T20:01:08.293512"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 292.438, "latencies_ms": [292.438], "images_per_second": 3.42, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA young girl in a purple bikini is surfing on a wave, with two other people nearby and another person further out in the water.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 10503.2, "ram_available_mb": 112003.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10500.5, "ram_available_mb": 112005.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [32.64, 32.64, 32.64], "power_watts_avg": 32.64, "power_watts_peak": 32.64, "energy_joules_est": 9.55, "sample_count": 3, "duration_seconds": 0.293}, "timestamp": "2026-01-25T20:01:08.602119"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 91.74, "latencies_ms": [91.74], "images_per_second": 10.9, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Girl surfing - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10500.5, "ram_available_mb": 112005.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10497.3, "ram_available_mb": 112009.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [30.71], "power_watts_avg": 30.71, "power_watts_peak": 30.71, "energy_joules_est": 2.84, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T20:01:08.707444"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 130.667, "latencies_ms": [130.667], "images_per_second": 7.653, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nThe girl is surfing on a wave in front of two other people.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10497.3, "ram_available_mb": 112009.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10498.7, "ram_available_mb": 112007.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [30.71, 30.71], "power_watts_avg": 30.71, "power_watts_peak": 30.71, "energy_joules_est": 4.02, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T20:01:08.911859"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 239.494, "latencies_ms": [239.494], "images_per_second": 4.175, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nIn the image, a young girl in a bikini is surfing on a wave while other people are also riding surfboards. The ocean water appears to be calm at the moment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10498.7, "ram_available_mb": 112007.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10500.1, "ram_available_mb": 112006.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [30.71, 30.71, 32.43], "power_watts_avg": 31.28, "power_watts_peak": 32.43, "energy_joules_est": 7.5, "sample_count": 3, "duration_seconds": 0.24}, "timestamp": "2026-01-25T20:01:09.217011"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 345.0, "latencies_ms": [345.0], "images_per_second": 2.899, "prompt_tokens": 756, "response_tokens_est": 59, "n_tiles": 1, "output_text": "\n The image shows a young girl surfing on a wave in the ocean. She is wearing a blue bikini and has her arms outstretched as she rides the wave. There are other people present in the water, but they do not appear to be directly involved with the main action of surfing.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10500.1, "ram_available_mb": 112006.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 10499.6, "ram_available_mb": 112006.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [32.43, 32.43, 32.43, 32.43], "power_watts_avg": 32.43, "power_watts_peak": 32.43, "energy_joules_est": 11.21, "sample_count": 4, "duration_seconds": 0.346}, "timestamp": "2026-01-25T20:01:09.623491"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 145.878, "latencies_ms": [145.878], "images_per_second": 6.855, "prompt_tokens": 744, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10499.6, "ram_available_mb": 112006.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10498.3, "ram_available_mb": 112008.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [32.82, 32.82], "power_watts_avg": 32.82, "power_watts_peak": 32.82, "energy_joules_est": 4.8, "sample_count": 2, "duration_seconds": 0.146}, "timestamp": "2026-01-25T20:01:09.833872"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 80.669, "latencies_ms": [80.669], "images_per_second": 12.396, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Elephant : 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10498.3, "ram_available_mb": 112008.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10501.2, "ram_available_mb": 112005.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [32.82], "power_watts_avg": 32.82, "power_watts_peak": 32.82, "energy_joules_est": 2.67, "sample_count": 1, "duration_seconds": 0.081}, "timestamp": "2026-01-25T20:01:09.938310"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 119.011, "latencies_ms": [119.011], "images_per_second": 8.403, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nA man is feeding an elephant while another person watches.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10501.2, "ram_available_mb": 112005.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10495.8, "ram_available_mb": 112010.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [32.82, 34.38], "power_watts_avg": 33.6, "power_watts_peak": 34.38, "energy_joules_est": 4.01, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T20:01:10.143887"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 166.376, "latencies_ms": [166.376], "images_per_second": 6.01, "prompt_tokens": 757, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA man in a white shirt feeds an elephant through his hand, while another person watches from behind a fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10495.8, "ram_available_mb": 112010.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10499.5, "ram_available_mb": 112006.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [34.38, 34.38], "power_watts_avg": 34.38, "power_watts_peak": 34.38, "energy_joules_est": 5.73, "sample_count": 2, "duration_seconds": 0.167}, "timestamp": "2026-01-25T20:01:10.349995"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 91.371, "latencies_ms": [91.371], "images_per_second": 10.944, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The elephant is gray and brown.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10499.5, "ram_available_mb": 112006.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10497.7, "ram_available_mb": 112008.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [34.38], "power_watts_avg": 34.38, "power_watts_peak": 34.38, "energy_joules_est": 3.15, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T20:01:10.454184"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 250.458, "latencies_ms": [250.458], "images_per_second": 3.993, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urn of flowers sits on a bed, with a dog sitting next to it and looking at the camera.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10497.7, "ram_available_mb": 112008.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10493.8, "ram_available_mb": 112012.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [34.38, 30.09, 30.09], "power_watts_avg": 31.52, "power_watts_peak": 34.38, "energy_joules_est": 7.9, "sample_count": 3, "duration_seconds": 0.251}, "timestamp": "2026-01-25T20:01:10.763020"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.843, "latencies_ms": [79.843], "images_per_second": 12.525, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bag: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10493.8, "ram_available_mb": 112012.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10488.7, "ram_available_mb": 112017.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [30.09], "power_watts_avg": 30.09, "power_watts_peak": 30.09, "energy_joules_est": 2.41, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T20:01:10.867181"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 133.593, "latencies_ms": [133.593], "images_per_second": 7.485, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A brown dog is sitting on a bed with clothes and bags.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.7, "ram_available_mb": 112017.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10487.9, "ram_available_mb": 112018.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_watts_samples": [30.09, 30.09], "power_watts_avg": 30.09, "power_watts_peak": 30.09, "energy_joules_est": 4.03, "sample_count": 2, "duration_seconds": 0.134}, "timestamp": "2026-01-25T20:01:11.072186"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 235.259, "latencies_ms": [235.259], "images_per_second": 4.251, "prompt_tokens": 757, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA brown dog sits on a bed, surrounded by various items including clothes. The dog appears to be looking at the camera with an expression of curiosity or amusement.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10487.9, "ram_available_mb": 112018.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10487.3, "ram_available_mb": 112019.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_watts_samples": [32.75, 32.75, 32.75], "power_watts_avg": 32.75, "power_watts_peak": 32.75, "energy_joules_est": 7.73, "sample_count": 3, "duration_seconds": 0.236}, "timestamp": "2026-01-25T20:01:11.378903"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 326.785, "latencies_ms": [326.785], "images_per_second": 3.06, "prompt_tokens": 756, "response_tokens_est": 53, "n_tiles": 1, "output_text": "\n A brown dog is sitting on a bed with clothes and bags. The dog's fur color contrasts with the white sheets of the bed. There are two windows in the room that allow natural light to enter, creating an inviting atmosphere for the dog to rest.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10487.3, "ram_available_mb": 112019.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10491.8, "ram_available_mb": 112014.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_watts_samples": [32.75, 32.75, 32.79, 32.79], "power_watts_avg": 32.77, "power_watts_peak": 32.79, "energy_joules_est": 10.73, "sample_count": 4, "duration_seconds": 0.327}, "timestamp": "2026-01-25T20:01:11.785731"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 264.01, "latencies_ms": [264.01], "images_per_second": 3.788, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urn of water on a table, with a man in a blue shirt and tie sitting at it, writing something down.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 10491.8, "ram_available_mb": 112014.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10489.9, "ram_available_mb": 112016.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.79, 32.79, 32.79], "power_watts_avg": 32.79, "power_watts_peak": 32.79, "energy_joules_est": 8.68, "sample_count": 3, "duration_seconds": 0.265}, "timestamp": "2026-01-25T20:01:12.096386"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 193.076, "latencies_ms": [193.076], "images_per_second": 5.179, "prompt_tokens": 759, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\n Laptop, 2; Tie, 1; Glasses, 1; Paper, 1; Pen, 1; Notebook, 1", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10489.9, "ram_available_mb": 112016.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10484.9, "ram_available_mb": 112021.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.62, 34.62], "power_watts_avg": 34.62, "power_watts_peak": 34.62, "energy_joules_est": 6.69, "sample_count": 2, "duration_seconds": 0.193}, "timestamp": "2026-01-25T20:01:12.300809"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 150.907, "latencies_ms": [150.907], "images_per_second": 6.627, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n 1. Man with glasses and a tie sitting at a desk in front of his laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.9, "ram_available_mb": 112021.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10482.4, "ram_available_mb": 112023.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.62, 34.62], "power_watts_avg": 34.62, "power_watts_peak": 34.62, "energy_joules_est": 5.23, "sample_count": 2, "duration_seconds": 0.151}, "timestamp": "2026-01-25T20:01:12.506031"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 261.565, "latencies_ms": [261.565], "images_per_second": 3.823, "prompt_tokens": 757, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nA man in a blue tie, wearing glasses, sits at his desk with a laptop computer. He has a pen and paper nearby, suggesting he might be working on a project or taking notes during a meeting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.4, "ram_available_mb": 112023.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10481.2, "ram_available_mb": 112025.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.62, 36.25, 36.25], "power_watts_avg": 35.71, "power_watts_peak": 36.25, "energy_joules_est": 9.35, "sample_count": 3, "duration_seconds": 0.262}, "timestamp": "2026-01-25T20:01:12.812490"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 110.732, "latencies_ms": [110.732], "images_per_second": 9.031, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The man is wearing a blue tie and glasses.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.2, "ram_available_mb": 112025.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10479.7, "ram_available_mb": 112026.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [36.25, 36.25], "power_watts_avg": 36.25, "power_watts_peak": 36.25, "energy_joules_est": 4.04, "sample_count": 2, "duration_seconds": 0.111}, "timestamp": "2026-01-25T20:01:13.019564"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 281.201, "latencies_ms": [281.201], "images_per_second": 3.556, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA large commercial airplane is flying through a clear blue sky, with its tail lights on and moving from left to right.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 10479.7, "ram_available_mb": 112026.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 16.0}, "power_stats": {"power_watts_samples": [36.25, 31.12, 31.12], "power_watts_avg": 32.83, "power_watts_peak": 36.25, "energy_joules_est": 9.26, "sample_count": 3, "duration_seconds": 0.282}, "timestamp": "2026-01-25T20:01:13.328970"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.248, "latencies_ms": [82.248], "images_per_second": 12.158, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Moon", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 16.0}, "power_stats": {"power_watts_samples": [31.12], "power_watts_avg": 31.12, "power_watts_peak": 31.12, "energy_joules_est": 2.58, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:01:13.434256"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 112.176, "latencies_ms": [112.176], "images_per_second": 8.915, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe airplane is flying in front of a full moon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10479.8, "ram_available_mb": 112026.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 16.0}, "power_stats": {"power_watts_samples": [31.12, 33.38], "power_watts_avg": 32.25, "power_watts_peak": 33.38, "energy_joules_est": 3.63, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T20:01:13.639582"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 54.916, "latencies_ms": [54.916], "images_per_second": 18.21, "prompt_tokens": 757, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.8, "ram_available_mb": 112026.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10475.8, "ram_available_mb": 112030.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 16.0}, "power_stats": {"power_watts_samples": [33.38], "power_watts_avg": 33.38, "power_watts_peak": 33.38, "energy_joules_est": 1.85, "sample_count": 1, "duration_seconds": 0.055}, "timestamp": "2026-01-25T20:01:13.744598"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 120.613, "latencies_ms": [120.613], "images_per_second": 8.291, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n A plane flying in the sky with a full moon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.8, "ram_available_mb": 112030.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10478.0, "ram_available_mb": 112028.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 16.0}, "power_stats": {"power_watts_samples": [33.38, 33.38], "power_watts_avg": 33.38, "power_watts_peak": 33.38, "energy_joules_est": 4.04, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T20:01:13.951390"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 314.946, "latencies_ms": [314.946], "images_per_second": 3.175, "prompt_tokens": 744, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA young man in a blue and pink tie-dye shirt is performing an impressive skateboard trick, mid-air with his board flipping beneath him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.0, "ram_available_mb": 112028.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10481.3, "ram_available_mb": 112025.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 1.0}, "power_stats": {"power_watts_samples": [33.38, 27.77, 27.77, 27.77], "power_watts_avg": 29.17, "power_watts_peak": 33.38, "energy_joules_est": 9.2, "sample_count": 4, "duration_seconds": 0.315}, "timestamp": "2026-01-25T20:01:14.361240"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 112.132, "latencies_ms": [112.132], "images_per_second": 8.918, "prompt_tokens": 759, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Skateboarder in mid-air", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.3, "ram_available_mb": 112025.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10482.6, "ram_available_mb": 112023.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 1.0}, "power_stats": {"power_watts_samples": [27.77, 27.77], "power_watts_avg": 27.77, "power_watts_peak": 27.77, "energy_joules_est": 3.12, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T20:01:14.566255"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 126.315, "latencies_ms": [126.315], "images_per_second": 7.917, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n 1. Skateboarder in midair with skateboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.6, "ram_available_mb": 112023.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10479.1, "ram_available_mb": 112027.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 1.0}, "power_stats": {"power_watts_samples": [29.38, 29.38], "power_watts_avg": 29.38, "power_watts_peak": 29.38, "energy_joules_est": 3.73, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:01:14.772092"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 409.211, "latencies_ms": [409.211], "images_per_second": 2.444, "prompt_tokens": 757, "response_tokens_est": 71, "n_tiles": 1, "output_text": "\nA young man in a tie-dye shirt and black pants is performing an impressive skateboard trick on a concrete ramp. The skateboard has colorful designs, adding to the visual appeal of his performance. In the background, there are palm trees swaying gently under a cloudy sky, creating a relaxed atmosphere for this exciting display of skill.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.1, "ram_available_mb": 112027.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10479.0, "ram_available_mb": 112027.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [29.38, 29.38, 29.38, 33.8, 33.8], "power_watts_avg": 31.15, "power_watts_peak": 33.8, "energy_joules_est": 12.76, "sample_count": 5, "duration_seconds": 0.409}, "timestamp": "2026-01-25T20:01:15.278448"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 143.16, "latencies_ms": [143.16], "images_per_second": 6.985, "prompt_tokens": 756, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n The skateboarder is wearing a blue and pink tie-dye shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.0, "ram_available_mb": 112027.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10472.8, "ram_available_mb": 112033.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.8, 33.8], "power_watts_avg": 33.8, "power_watts_peak": 33.8, "energy_joules_est": 4.85, "sample_count": 2, "duration_seconds": 0.144}, "timestamp": "2026-01-25T20:01:15.484746"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 267.972, "latencies_ms": [267.972], "images_per_second": 3.732, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA sheep with a white face and brown wool stands in front of a wire fence, looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10472.8, "ram_available_mb": 112033.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.8, 29.17, 29.17], "power_watts_avg": 30.72, "power_watts_peak": 33.8, "energy_joules_est": 8.24, "sample_count": 3, "duration_seconds": 0.268}, "timestamp": "2026-01-25T20:01:15.796895"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 92.616, "latencies_ms": [92.616], "images_per_second": 10.797, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Sheep: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [29.17], "power_watts_avg": 29.17, "power_watts_peak": 29.17, "energy_joules_est": 2.71, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:01:15.901355"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 110.227, "latencies_ms": [110.227], "images_per_second": 9.072, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nA sheep is standing in front of a fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10476.2, "ram_available_mb": 112030.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [29.17, 29.17], "power_watts_avg": 29.17, "power_watts_peak": 29.17, "energy_joules_est": 3.23, "sample_count": 2, "duration_seconds": 0.111}, "timestamp": "2026-01-25T20:01:16.107049"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 242.111, "latencies_ms": [242.111], "images_per_second": 4.13, "prompt_tokens": 757, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nIn a lush green field, two sheep are standing next to each other. One of them has its head through a wire fence while looking at the camera with curiosity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.2, "ram_available_mb": 112030.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [33.61, 33.61, 33.61], "power_watts_avg": 33.61, "power_watts_peak": 33.61, "energy_joules_est": 8.15, "sample_count": 3, "duration_seconds": 0.242}, "timestamp": "2026-01-25T20:01:16.413624"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 208.771, "latencies_ms": [208.771], "images_per_second": 4.79, "prompt_tokens": 756, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\n A sheep with a white face and brown body is standing in front of a wire fence. The sheep's wool appears to be dirty or soiled.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10472.2, "ram_available_mb": 112034.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [33.61, 33.61, 32.77], "power_watts_avg": 33.33, "power_watts_peak": 33.61, "energy_joules_est": 6.99, "sample_count": 3, "duration_seconds": 0.21}, "timestamp": "2026-01-25T20:01:16.719598"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 256.421, "latencies_ms": [256.421], "images_per_second": 3.9, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA silver and black Walkman device is displayed on a table, with its buttons and controls facing upwards.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.2, "ram_available_mb": 112034.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10473.7, "ram_available_mb": 112032.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [32.77, 32.77, 32.77], "power_watts_avg": 32.77, "power_watts_peak": 32.77, "energy_joules_est": 8.41, "sample_count": 3, "duration_seconds": 0.257}, "timestamp": "2026-01-25T20:01:17.028764"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 85.238, "latencies_ms": [85.238], "images_per_second": 11.732, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\nAT&T: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.7, "ram_available_mb": 112032.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [33.05], "power_watts_avg": 33.05, "power_watts_peak": 33.05, "energy_joules_est": 2.84, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T20:01:17.133192"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 81.247, "latencies_ms": [81.247], "images_per_second": 12.308, "prompt_tokens": 763, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\nThe camera is on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10472.6, "ram_available_mb": 112033.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [33.05], "power_watts_avg": 33.05, "power_watts_peak": 33.05, "energy_joules_est": 2.69, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T20:01:17.237463"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 253.315, "latencies_ms": [253.315], "images_per_second": 3.948, "prompt_tokens": 757, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\nThe image shows a close-up view of an AT&T cell phone with its buttons and display facing upwards. The phone appears to be turned on, as indicated by the illuminated screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.6, "ram_available_mb": 112033.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [33.05, 33.05, 33.05], "power_watts_avg": 33.05, "power_watts_peak": 33.05, "energy_joules_est": 8.38, "sample_count": 3, "duration_seconds": 0.254}, "timestamp": "2026-01-25T20:01:17.544154"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 260.827, "latencies_ms": [260.827], "images_per_second": 3.834, "prompt_tokens": 756, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nThe image shows a silver and black cellphone with an orange logo on the bottom right corner. The phone is turned off, and it appears to be resting on a table or countertop in front of a window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10475.5, "ram_available_mb": 112030.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [34.71, 34.71, 34.71], "power_watts_avg": 34.71, "power_watts_peak": 34.71, "energy_joules_est": 9.07, "sample_count": 3, "duration_seconds": 0.261}, "timestamp": "2026-01-25T20:01:17.849694"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 281.023, "latencies_ms": [281.023], "images_per_second": 3.558, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "urn of water on a countertop, with a woman in a black dress standing next to it and holding a glass of wine.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.5, "ram_available_mb": 112030.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10476.6, "ram_available_mb": 112029.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [34.71, 34.71, 35.1], "power_watts_avg": 34.84, "power_watts_peak": 35.1, "energy_joules_est": 9.81, "sample_count": 3, "duration_seconds": 0.282}, "timestamp": "2026-01-25T20:01:18.159516"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 91.556, "latencies_ms": [91.556], "images_per_second": 10.922, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Refrigerator door", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.6, "ram_available_mb": 112029.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10474.9, "ram_available_mb": 112031.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [35.1], "power_watts_avg": 35.1, "power_watts_peak": 35.1, "energy_joules_est": 3.23, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T20:01:18.264925"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 139.332, "latencies_ms": [139.332], "images_per_second": 7.177, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nA woman in a black dress stands next to an open refrigerator with her arm outstretched.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.9, "ram_available_mb": 112031.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10477.3, "ram_available_mb": 112029.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [35.1, 35.1], "power_watts_avg": 35.1, "power_watts_peak": 35.1, "energy_joules_est": 4.9, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T20:01:18.471523"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 169.872, "latencies_ms": [169.872], "images_per_second": 5.887, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA woman in a black dress stands in front of an open refrigerator, holding a glass of orange juice.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.3, "ram_available_mb": 112029.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10474.1, "ram_available_mb": 112032.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [35.1, 34.45], "power_watts_avg": 34.77, "power_watts_peak": 35.1, "energy_joules_est": 5.93, "sample_count": 2, "duration_seconds": 0.171}, "timestamp": "2026-01-25T20:01:18.677786"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 157.36, "latencies_ms": [157.36], "images_per_second": 6.355, "prompt_tokens": 756, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n A woman in a black dress stands next to an open refrigerator with her arm outstretched.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.1, "ram_available_mb": 112032.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10476.3, "ram_available_mb": 112030.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [34.45, 34.45], "power_watts_avg": 34.45, "power_watts_peak": 34.45, "energy_joules_est": 5.43, "sample_count": 2, "duration_seconds": 0.158}, "timestamp": "2026-01-25T20:01:18.883476"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 276.015, "latencies_ms": [276.015], "images_per_second": 3.623, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA yellow school bus is reflected in a round mirror, driving down a street with other cars and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.3, "ram_available_mb": 112030.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10472.8, "ram_available_mb": 112033.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [34.45, 34.45, 32.58], "power_watts_avg": 33.82, "power_watts_peak": 34.45, "energy_joules_est": 9.35, "sample_count": 3, "duration_seconds": 0.277}, "timestamp": "2026-01-25T20:01:19.193870"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 81.431, "latencies_ms": [81.431], "images_per_second": 12.28, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bus: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.8, "ram_available_mb": 112033.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [32.58], "power_watts_avg": 32.58, "power_watts_peak": 32.58, "energy_joules_est": 2.66, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T20:01:19.298974"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 112.596, "latencies_ms": [112.596], "images_per_second": 8.881, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nA reflection of a yellow school bus in a mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [32.58, 32.58], "power_watts_avg": 32.58, "power_watts_peak": 32.58, "energy_joules_est": 3.67, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T20:01:19.505159"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 243.082, "latencies_ms": [243.082], "images_per_second": 4.114, "prompt_tokens": 757, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\nA yellow school bus is driving down a street, with its side view mirror reflecting the surrounding area. The bus is in motion, and there are other vehicles on the road as well.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [32.58, 32.43, 32.43], "power_watts_avg": 32.48, "power_watts_peak": 32.58, "energy_joules_est": 7.92, "sample_count": 3, "duration_seconds": 0.244}, "timestamp": "2026-01-25T20:01:19.811204"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 146.143, "latencies_ms": [146.143], "images_per_second": 6.843, "prompt_tokens": 756, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n The reflection in the side mirror shows a yellow school bus driving down the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10465.2, "ram_available_mb": 112041.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [32.43, 32.43], "power_watts_avg": 32.43, "power_watts_peak": 32.43, "energy_joules_est": 4.75, "sample_count": 2, "duration_seconds": 0.146}, "timestamp": "2026-01-25T20:01:20.016791"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 338.132, "latencies_ms": [338.132], "images_per_second": 2.957, "prompt_tokens": 744, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\nA gray cat sits on a wooden table, while a brown dog stands behind it with its tail raised high in the air. The scene is set against a window backdrop of trees and grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.2, "ram_available_mb": 112041.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [32.43, 31.43, 31.43, 31.43], "power_watts_avg": 31.68, "power_watts_peak": 32.43, "energy_joules_est": 10.73, "sample_count": 4, "duration_seconds": 0.339}, "timestamp": "2026-01-25T20:01:20.426646"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 101.891, "latencies_ms": [101.891], "images_per_second": 9.814, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Dog and cat in the window sill", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [31.43, 35.03], "power_watts_avg": 33.23, "power_watts_peak": 35.03, "energy_joules_est": 3.4, "sample_count": 2, "duration_seconds": 0.102}, "timestamp": "2026-01-25T20:01:20.631617"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 123.626, "latencies_ms": [123.626], "images_per_second": 8.089, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A cat sitting on a table next to two potted plants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10475.3, "ram_available_mb": 112031.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [35.03, 35.03], "power_watts_avg": 35.03, "power_watts_peak": 35.03, "energy_joules_est": 4.35, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T20:01:20.837095"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 252.696, "latencies_ms": [252.696], "images_per_second": 3.957, "prompt_tokens": 757, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\n A gray cat sits on a wooden table, while a brown dog stands behind it. The cat appears to be looking at something outside of the frame, possibly observing another animal or an interesting object.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.3, "ram_available_mb": 112031.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10475.5, "ram_available_mb": 112030.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.03, 35.03, 32.79], "power_watts_avg": 34.28, "power_watts_peak": 35.03, "energy_joules_est": 8.67, "sample_count": 3, "duration_seconds": 0.253}, "timestamp": "2026-01-25T20:01:21.143785"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 296.78, "latencies_ms": [296.78], "images_per_second": 3.369, "prompt_tokens": 756, "response_tokens_est": 49, "n_tiles": 1, "output_text": "\n A dog and a cat are sitting on the floor in front of a window. The dog is brown with black spots while the cat has gray fur. There are two potted plants near them - one green plant and another yellow plant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.5, "ram_available_mb": 112030.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10475.5, "ram_available_mb": 112030.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.79, 32.79, 32.79], "power_watts_avg": 32.79, "power_watts_peak": 32.79, "energy_joules_est": 9.74, "sample_count": 3, "duration_seconds": 0.297}, "timestamp": "2026-01-25T20:01:21.448775"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 307.335, "latencies_ms": [307.335], "images_per_second": 3.254, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA female soccer player in a blue uniform is about to kick a white and blue ball, while another player stands nearby wearing a yellow shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.5, "ram_available_mb": 112030.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10481.7, "ram_available_mb": 112024.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.79, 30.2, 30.2, 30.2], "power_watts_avg": 30.85, "power_watts_peak": 32.79, "energy_joules_est": 9.49, "sample_count": 4, "duration_seconds": 0.308}, "timestamp": "2026-01-25T20:01:21.861186"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 74.881, "latencies_ms": [74.881], "images_per_second": 13.355, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Ball", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.7, "ram_available_mb": 112024.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10482.9, "ram_available_mb": 112023.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [30.2], "power_watts_avg": 30.2, "power_watts_peak": 30.2, "energy_joules_est": 2.28, "sample_count": 1, "duration_seconds": 0.075}, "timestamp": "2026-01-25T20:01:21.967284"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 139.817, "latencies_ms": [139.817], "images_per_second": 7.152, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nThe woman in a blue shirt is running with a soccer ball while another woman watches.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.9, "ram_available_mb": 112023.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10490.7, "ram_available_mb": 112015.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [30.2, 34.7], "power_watts_avg": 32.45, "power_watts_peak": 34.7, "energy_joules_est": 4.54, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T20:01:22.174151"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 168.856, "latencies_ms": [168.856], "images_per_second": 5.922, "prompt_tokens": 757, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA female soccer player in a blue uniform is running with a white ball, while another player in yellow stands nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10490.6, "ram_available_mb": 112015.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10490.6, "ram_available_mb": 112015.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [34.7, 34.7], "power_watts_avg": 34.7, "power_watts_peak": 34.7, "energy_joules_est": 5.88, "sample_count": 2, "duration_seconds": 0.169}, "timestamp": "2026-01-25T20:01:22.379635"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 55.034, "latencies_ms": [55.034], "images_per_second": 18.171, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10490.6, "ram_available_mb": 112015.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10493.1, "ram_available_mb": 112013.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [34.7], "power_watts_avg": 34.7, "power_watts_peak": 34.7, "energy_joules_est": 1.94, "sample_count": 1, "duration_seconds": 0.056}, "timestamp": "2026-01-25T20:01:22.485263"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 300.84, "latencies_ms": [300.84], "images_per_second": 3.324, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nTwo giraffes stand in a fenced-in area, with one reaching up to eat leaves from a tree and the other standing nearby.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10493.1, "ram_available_mb": 112013.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10483.5, "ram_available_mb": 112022.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [34.7, 31.07, 31.07, 31.07], "power_watts_avg": 31.98, "power_watts_peak": 34.7, "energy_joules_est": 9.63, "sample_count": 4, "duration_seconds": 0.301}, "timestamp": "2026-01-25T20:01:22.897345"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 424.139, "latencies_ms": [424.139], "images_per_second": 2.358, "prompt_tokens": 759, "response_tokens_est": 75, "n_tiles": 1, "output_text": "\n 1. Giraffe standing tall and looking at the camera 2. Giraffe bending down to eat 3. Giraffe standing tall and looking at the camera 4. Giraffe bending down to eat 5. Giraffe standing tall and looking at the camera 6. Giraffe standing tall and looking at the camera 7. Giraffe standing tall and looking at the camera 8.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10483.3, "ram_available_mb": 112023.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10484.2, "ram_available_mb": 112022.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [31.07, 31.07, 32.78, 32.78, 32.78], "power_watts_avg": 32.1, "power_watts_peak": 32.78, "energy_joules_est": 13.64, "sample_count": 5, "duration_seconds": 0.425}, "timestamp": "2026-01-25T20:01:23.404420"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 141.27, "latencies_ms": [141.27], "images_per_second": 7.079, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n 1. Giraffe standing in front of a fence with another giraffe behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.2, "ram_available_mb": 112022.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10487.9, "ram_available_mb": 112018.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [32.78, 32.78], "power_watts_avg": 32.78, "power_watts_peak": 32.78, "energy_joules_est": 4.65, "sample_count": 2, "duration_seconds": 0.142}, "timestamp": "2026-01-25T20:01:23.609625"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 270.372, "latencies_ms": [270.372], "images_per_second": 3.699, "prompt_tokens": 757, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\nTwo giraffes are standing in a fenced-in area, with one of them bending down to eat. The other giraffe stands nearby, possibly keeping watch or waiting for its turn to eat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10487.9, "ram_available_mb": 112018.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10486.7, "ram_available_mb": 112019.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [37.07, 37.07, 37.07], "power_watts_avg": 37.07, "power_watts_peak": 37.07, "energy_joules_est": 10.04, "sample_count": 3, "duration_seconds": 0.271}, "timestamp": "2026-01-25T20:01:23.914834"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 92.302, "latencies_ms": [92.302], "images_per_second": 10.834, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n The giraffe is brown and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10486.7, "ram_available_mb": 112019.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [37.07], "power_watts_avg": 37.07, "power_watts_peak": 37.07, "energy_joules_est": 3.43, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:01:24.019553"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 258.465, "latencies_ms": [258.465], "images_per_second": 3.869, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n A suitcase and two bags are sitting on a carpeted floor, with a curtain in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [37.07, 33.29, 33.29], "power_watts_avg": 34.55, "power_watts_peak": 37.07, "energy_joules_est": 8.95, "sample_count": 3, "duration_seconds": 0.259}, "timestamp": "2026-01-25T20:01:24.328707"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 90.882, "latencies_ms": [90.882], "images_per_second": 11.003, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Suitcase", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10472.8, "ram_available_mb": 112033.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [33.29], "power_watts_avg": 33.29, "power_watts_peak": 33.29, "energy_joules_est": 3.04, "sample_count": 1, "duration_seconds": 0.091}, "timestamp": "2026-01-25T20:01:24.433321"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 154.834, "latencies_ms": [154.834], "images_per_second": 6.459, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n 1. A black suitcase with a gray handle and wheels in front of a white curtain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.8, "ram_available_mb": 112033.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10474.6, "ram_available_mb": 112031.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [33.29, 33.51], "power_watts_avg": 33.4, "power_watts_peak": 33.51, "energy_joules_est": 5.18, "sample_count": 2, "duration_seconds": 0.155}, "timestamp": "2026-01-25T20:01:24.638171"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 162.482, "latencies_ms": [162.482], "images_per_second": 6.155, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA black and white photograph of a hotel room with three pieces of luggage, including a gray suitcase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.6, "ram_available_mb": 112031.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10468.7, "ram_available_mb": 112037.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [33.51, 33.51], "power_watts_avg": 33.51, "power_watts_peak": 33.51, "energy_joules_est": 5.46, "sample_count": 2, "duration_seconds": 0.163}, "timestamp": "2026-01-25T20:01:24.843174"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 131.881, "latencies_ms": [131.881], "images_per_second": 7.583, "prompt_tokens": 756, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n A black and white photo of a suitcase on the floor next to two bags.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.7, "ram_available_mb": 112037.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10476.8, "ram_available_mb": 112029.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [33.51, 33.51], "power_watts_avg": 33.51, "power_watts_peak": 33.51, "energy_joules_est": 4.43, "sample_count": 2, "duration_seconds": 0.132}, "timestamp": "2026-01-25T20:01:25.046647"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 300.612, "latencies_ms": [300.612], "images_per_second": 3.327, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA man in a blue shirt and orange bandana is walking behind two horses, with another person riding on the back of the first horse.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10476.8, "ram_available_mb": 112029.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10477.6, "ram_available_mb": 112028.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [32.35, 32.35, 32.35, 32.35], "power_watts_avg": 32.35, "power_watts_peak": 32.35, "energy_joules_est": 9.76, "sample_count": 4, "duration_seconds": 0.302}, "timestamp": "2026-01-25T20:01:25.458997"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.05, "latencies_ms": [78.05], "images_per_second": 12.812, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Tree trunk", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.6, "ram_available_mb": 112028.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10479.7, "ram_available_mb": 112026.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [32.35], "power_watts_avg": 32.35, "power_watts_peak": 32.35, "energy_joules_est": 2.53, "sample_count": 1, "duration_seconds": 0.078}, "timestamp": "2026-01-25T20:01:25.564368"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 121.052, "latencies_ms": [121.052], "images_per_second": 8.261, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nA man in a blue shirt is walking behind two horses.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.7, "ram_available_mb": 112026.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10480.5, "ram_available_mb": 112025.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [32.32, 32.32], "power_watts_avg": 32.32, "power_watts_peak": 32.32, "energy_joules_est": 3.93, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:01:25.770283"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 256.43, "latencies_ms": [256.43], "images_per_second": 3.9, "prompt_tokens": 757, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\nA man in a blue shirt and orange bandana rides on horseback, with two more horses following behind him. They are walking through a forested area, surrounded by trees and rocks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.5, "ram_available_mb": 112025.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10482.2, "ram_available_mb": 112024.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.32, 32.32, 32.32], "power_watts_avg": 32.32, "power_watts_peak": 32.32, "energy_joules_est": 8.31, "sample_count": 3, "duration_seconds": 0.257}, "timestamp": "2026-01-25T20:01:26.075206"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 253.568, "latencies_ms": [253.568], "images_per_second": 3.944, "prompt_tokens": 756, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\n The image shows a man wearing an orange bandana and blue shirt riding on the back of a horse in a wooded area. There are other people nearby who appear to be enjoying their time as well.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.2, "ram_available_mb": 112024.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10480.5, "ram_available_mb": 112025.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.5, 32.5, 32.5], "power_watts_avg": 32.5, "power_watts_peak": 32.5, "energy_joules_est": 8.25, "sample_count": 3, "duration_seconds": 0.254}, "timestamp": "2026-01-25T20:01:26.380665"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 281.345, "latencies_ms": [281.345], "images_per_second": 3.554, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA man in a gray sweater and black pants rides a horse, holding the reins with both hands while standing on its back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.5, "ram_available_mb": 112025.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10483.4, "ram_available_mb": 112022.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.5, 32.5, 34.38], "power_watts_avg": 33.13, "power_watts_peak": 34.38, "energy_joules_est": 9.34, "sample_count": 3, "duration_seconds": 0.282}, "timestamp": "2026-01-25T20:01:26.689344"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 266.138, "latencies_ms": [266.138], "images_per_second": 3.757, "prompt_tokens": 759, "response_tokens_est": 42, "n_tiles": 1, "output_text": "\n 1. Horse's head 2. Man's face 3. Man's arm 4. Man's leg 5. Man's hand 6. Man's foot 7. Horse's tail 8. Horse's ears", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.7, "ram_available_mb": 112022.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.38, 34.38, 34.38], "power_watts_avg": 34.38, "power_watts_peak": 34.38, "energy_joules_est": 9.16, "sample_count": 3, "duration_seconds": 0.266}, "timestamp": "2026-01-25T20:01:26.994947"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 105.126, "latencies_ms": [105.126], "images_per_second": 9.512, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Horse is moving quickly through a city street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10469.7, "ram_available_mb": 112036.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [34.38, 36.32], "power_watts_avg": 35.35, "power_watts_peak": 36.32, "energy_joules_est": 3.73, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T20:01:27.200614"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 274.118, "latencies_ms": [274.118], "images_per_second": 3.648, "prompt_tokens": 757, "response_tokens_est": 45, "n_tiles": 1, "output_text": "\nA man in a suit stands on top of a horse, holding its reins. The horse appears to be galloping or running at high speed. In the background, there are buildings visible behind the horse and rider.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.7, "ram_available_mb": 112036.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10470.4, "ram_available_mb": 112035.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [36.32, 36.32, 36.32], "power_watts_avg": 36.32, "power_watts_peak": 36.32, "energy_joules_est": 9.97, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T20:01:27.506780"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 141.779, "latencies_ms": [141.779], "images_per_second": 7.053, "prompt_tokens": 756, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n The image is a black and white photo of a man riding on the back of a horse.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10470.4, "ram_available_mb": 112035.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10471.2, "ram_available_mb": 112035.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [36.32, 33.56], "power_watts_avg": 34.94, "power_watts_peak": 36.32, "energy_joules_est": 4.97, "sample_count": 2, "duration_seconds": 0.142}, "timestamp": "2026-01-25T20:01:27.711913"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 272.87, "latencies_ms": [272.87], "images_per_second": 3.665, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nTwo geese are swimming in a pond, with their heads above the water and necks extended forward.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10471.2, "ram_available_mb": 112035.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10464.6, "ram_available_mb": 112041.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [33.56, 33.56, 33.56], "power_watts_avg": 33.56, "power_watts_peak": 33.56, "energy_joules_est": 9.17, "sample_count": 3, "duration_seconds": 0.273}, "timestamp": "2026-01-25T20:01:28.025333"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 98.225, "latencies_ms": [98.225], "images_per_second": 10.181, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Duck swimming in water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.8, "ram_available_mb": 112041.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [33.56], "power_watts_avg": 33.56, "power_watts_peak": 33.56, "energy_joules_est": 3.31, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T20:01:28.130236"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 98.751, "latencies_ms": [98.751], "images_per_second": 10.126, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\nTwo geese swimming in a lake.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [34.49], "power_watts_avg": 34.49, "power_watts_peak": 34.49, "energy_joules_est": 3.41, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T20:01:28.234422"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 275.629, "latencies_ms": [275.629], "images_per_second": 3.628, "prompt_tokens": 757, "response_tokens_est": 46, "n_tiles": 1, "output_text": "\nA group of ducks are swimming in a pond, with some ducks floating on the water's surface. The pond is surrounded by trees and bushes, creating a peaceful and natural environment for the birds to enjoy their time together.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10447.0, "ram_available_mb": 112059.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [34.49, 34.49, 34.49], "power_watts_avg": 34.49, "power_watts_peak": 34.49, "energy_joules_est": 9.51, "sample_count": 3, "duration_seconds": 0.276}, "timestamp": "2026-01-25T20:01:28.539172"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 306.275, "latencies_ms": [306.275], "images_per_second": 3.265, "prompt_tokens": 756, "response_tokens_est": 51, "n_tiles": 1, "output_text": "\n The image features a group of ducks swimming in a body of water. There are two large white ducks and one smaller duck near the center of the scene. The ducks appear to be enjoying their time together as they swim leisurely across the lake.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.2, "ram_available_mb": 112059.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10444.0, "ram_available_mb": 112062.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 77.0}, "power_stats": {"power_watts_samples": [35.13, 35.13, 35.13, 35.13], "power_watts_avg": 35.13, "power_watts_peak": 35.13, "energy_joules_est": 10.78, "sample_count": 4, "duration_seconds": 0.307}, "timestamp": "2026-01-25T20:01:28.945335"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 295.154, "latencies_ms": [295.154], "images_per_second": 3.388, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA cat is sitting on top of a black Mercedes-Benz car parked in front of a house with a green fence and trees nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10444.0, "ram_available_mb": 112062.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10446.6, "ram_available_mb": 112059.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 5.0}, "power_stats": {"power_watts_samples": [35.13, 32.73, 32.73], "power_watts_avg": 33.53, "power_watts_peak": 35.13, "energy_joules_est": 9.92, "sample_count": 3, "duration_seconds": 0.296}, "timestamp": "2026-01-25T20:01:29.253888"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 98.89, "latencies_ms": [98.89], "images_per_second": 10.112, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Cat sitting on car hood", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10446.6, "ram_available_mb": 112059.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 5.0}, "power_stats": {"power_watts_samples": [32.73], "power_watts_avg": 32.73, "power_watts_peak": 32.73, "energy_joules_est": 3.24, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T20:01:29.358185"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 103.5, "latencies_ms": [103.5], "images_per_second": 9.662, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nA cat is sitting on top of a car.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 10448.2, "ram_available_mb": 112058.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 5.0}, "power_stats": {"power_watts_samples": [32.73, 32.73], "power_watts_avg": 32.73, "power_watts_peak": 32.73, "energy_joules_est": 3.39, "sample_count": 2, "duration_seconds": 0.104}, "timestamp": "2026-01-25T20:01:29.563834"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 164.791, "latencies_ms": [164.791], "images_per_second": 6.068, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA cat is sitting on top of a black Mercedes-Benz car parked in front of a house.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.2, "ram_available_mb": 112058.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10445.9, "ram_available_mb": 112060.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 5.0}, "power_stats": {"power_watts_samples": [34.59, 34.59], "power_watts_avg": 34.59, "power_watts_peak": 34.59, "energy_joules_est": 5.71, "sample_count": 2, "duration_seconds": 0.165}, "timestamp": "2026-01-25T20:01:29.769077"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 114.887, "latencies_ms": [114.887], "images_per_second": 8.704, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n A cat is sitting on the hood of a car.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.9, "ram_available_mb": 112060.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10449.7, "ram_available_mb": 112056.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 5.0}, "power_stats": {"power_watts_samples": [34.59, 34.59], "power_watts_avg": 34.59, "power_watts_peak": 34.59, "energy_joules_est": 3.99, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T20:01:29.975237"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 305.302, "latencies_ms": [305.302], "images_per_second": 3.275, "prompt_tokens": 744, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA snowboarder in a brown jacket and yellow pants is captured mid-air, performing an impressive trick on their snowboard against a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.7, "ram_available_mb": 112056.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10450.2, "ram_available_mb": 112056.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [34.59, 30.86, 30.86, 30.86], "power_watts_avg": 31.79, "power_watts_peak": 34.59, "energy_joules_est": 9.72, "sample_count": 4, "duration_seconds": 0.306}, "timestamp": "2026-01-25T20:01:30.384866"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 84.937, "latencies_ms": [84.937], "images_per_second": 11.773, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Snowboarder", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.2, "ram_available_mb": 112056.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10453.6, "ram_available_mb": 112052.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.86], "power_watts_avg": 30.86, "power_watts_peak": 30.86, "energy_joules_est": 2.63, "sample_count": 1, "duration_seconds": 0.085}, "timestamp": "2026-01-25T20:01:30.489768"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 153.867, "latencies_ms": [153.867], "images_per_second": 6.499, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nThe snowboarder is in midair and appears to be flying through the air.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.8, "ram_available_mb": 112052.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10456.2, "ram_available_mb": 112050.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.86, 30.55], "power_watts_avg": 30.7, "power_watts_peak": 30.86, "energy_joules_est": 4.74, "sample_count": 2, "duration_seconds": 0.154}, "timestamp": "2026-01-25T20:01:30.695504"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 193.209, "latencies_ms": [193.209], "images_per_second": 5.176, "prompt_tokens": 757, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA snowboarder in a brown jacket and yellow pants is performing an aerial trick on their snowboard, with their arms outstretched.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.2, "ram_available_mb": 112050.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10457.5, "ram_available_mb": 112048.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.55, 30.55], "power_watts_avg": 30.55, "power_watts_peak": 30.55, "energy_joules_est": 5.91, "sample_count": 2, "duration_seconds": 0.194}, "timestamp": "2026-01-25T20:01:30.899631"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 372.447, "latencies_ms": [372.447], "images_per_second": 2.685, "prompt_tokens": 756, "response_tokens_est": 62, "n_tiles": 1, "output_text": "\n The image shows a snowboarder in the air performing an impressive trick. The snowboarder is wearing a brown jacket and yellow pants while holding onto their snowboard with both hands. The background of the photo features a clear blue sky, which contrasts nicely with the white snow on the ground below.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.5, "ram_available_mb": 112048.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [30.55, 30.55, 34.57, 34.57], "power_watts_avg": 32.56, "power_watts_peak": 34.57, "energy_joules_est": 12.13, "sample_count": 4, "duration_seconds": 0.373}, "timestamp": "2026-01-25T20:01:31.305276"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 274.428, "latencies_ms": [274.428], "images_per_second": 3.644, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "urn of water on a chain hangs from the ceiling in an old bathroom, with a white bathtub and toilet below it.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10473.5, "ram_available_mb": 112032.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [34.57, 34.57, 34.57], "power_watts_avg": 34.57, "power_watts_peak": 34.57, "energy_joules_est": 9.49, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T20:01:31.615755"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 254.352, "latencies_ms": [254.352], "images_per_second": 3.932, "prompt_tokens": 759, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\n 1. Toilet bowl  2. Bathtub  3. Toilet paper holder  4. Pipe  5. Pipe  6. Pipe  7. Pipe  8. Pipe", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10473.5, "ram_available_mb": 112032.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [38.21, 38.21, 38.21], "power_watts_avg": 38.21, "power_watts_peak": 38.21, "energy_joules_est": 9.73, "sample_count": 3, "duration_seconds": 0.255}, "timestamp": "2026-01-25T20:01:31.922217"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 84.428, "latencies_ms": [84.428], "images_per_second": 11.844, "prompt_tokens": 763, "response_tokens_est": 6, "n_tiles": 1, "output_text": "urn with towel on it", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10478.5, "ram_available_mb": 112027.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [38.21], "power_watts_avg": 38.21, "power_watts_peak": 38.21, "energy_joules_est": 3.25, "sample_count": 1, "duration_seconds": 0.085}, "timestamp": "2026-01-25T20:01:32.028118"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 130.33, "latencies_ms": [130.33], "images_per_second": 7.673, "prompt_tokens": 757, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA small bathroom with a white bathtub, toilet, and sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.5, "ram_available_mb": 112027.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10482.2, "ram_available_mb": 112024.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [37.39, 37.39], "power_watts_avg": 37.39, "power_watts_peak": 37.39, "energy_joules_est": 4.88, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T20:01:32.236072"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 121.581, "latencies_ms": [121.581], "images_per_second": 8.225, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The bathroom is white and has a brown wooden door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.2, "ram_available_mb": 112024.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10482.0, "ram_available_mb": 112024.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [37.39, 37.39], "power_watts_avg": 37.39, "power_watts_peak": 37.39, "energy_joules_est": 4.57, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:01:32.442815"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 277.144, "latencies_ms": [277.144], "images_per_second": 3.608, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA statue of two children flying a kite is displayed on top of a building, with the sky visible in the background.", "error": null, "sys_before": {"cpu_percent": 8.7, "ram_used_mb": 10482.0, "ram_available_mb": 112024.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10486.6, "ram_available_mb": 112019.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [37.39, 29.04, 29.04], "power_watts_avg": 31.82, "power_watts_peak": 37.39, "energy_joules_est": 8.83, "sample_count": 3, "duration_seconds": 0.277}, "timestamp": "2026-01-25T20:01:32.755747"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 693.787, "latencies_ms": [693.787], "images_per_second": 1.441, "prompt_tokens": 759, "response_tokens_est": 128, "n_tiles": 1, "output_text": "\n 1. Kite 2. Kite 3. Kite 4. Kite 5. Kite 6. Kite 7. Kite 8. Kite 9. Kite 10. Kite 11. Kite 12. Kite 13. Kite 14. Kite 15. Kite 16. Kite 17. Kite 18. Kite 19. Kite 20. Kite 21. Kite 22. Kite 23. Kite 24. Kite 25. Kite 26. Kite 27. Kite 28. Kite 29. Kite 30. Kite 31. Kite 32. K", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10486.6, "ram_available_mb": 112019.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10484.6, "ram_available_mb": 112021.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [29.04, 29.04, 29.04, 34.43, 34.43, 34.43, 34.43], "power_watts_avg": 32.12, "power_watts_peak": 34.43, "energy_joules_est": 22.3, "sample_count": 7, "duration_seconds": 0.694}, "timestamp": "2026-01-25T20:01:33.463138"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 142.031, "latencies_ms": [142.031], "images_per_second": 7.041, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nA statue of two children flying a kite is located in front of a tall building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.6, "ram_available_mb": 112021.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10490.0, "ram_available_mb": 112016.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.43, 41.47], "power_watts_avg": 37.95, "power_watts_peak": 41.47, "energy_joules_est": 5.4, "sample_count": 2, "duration_seconds": 0.142}, "timestamp": "2026-01-25T20:01:33.668008"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 136.668, "latencies_ms": [136.668], "images_per_second": 7.317, "prompt_tokens": 757, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA statue of two children holding a kite in front of a tall building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10489.8, "ram_available_mb": 112016.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10490.0, "ram_available_mb": 112016.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [41.47, 41.47], "power_watts_avg": 41.47, "power_watts_peak": 41.47, "energy_joules_est": 5.68, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T20:01:33.873920"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 633.409, "latencies_ms": [633.409], "images_per_second": 1.579, "prompt_tokens": 756, "response_tokens_est": 113, "n_tiles": 1, "output_text": "\n The image features a statue of two children flying a kite. One child is holding the kite string while the other stands beside him. They are positioned in front of a tall building with a glass facade that reflects the surrounding environment. The sky above them appears to be cloudy and gray, creating an atmospheric setting for their kite-flying activity.\n \nThe statue itself is made from metal or stone, giving it a sturdy appearance. It's located on top of a platform, which provides a vantage point for viewers to appreciate the scene below.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10490.0, "ram_available_mb": 112016.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10492.9, "ram_available_mb": 112013.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_watts_samples": [41.47, 41.47, 34.98, 34.98, 34.98, 34.98, 34.98], "power_watts_avg": 36.83, "power_watts_peak": 41.47, "energy_joules_est": 23.35, "sample_count": 7, "duration_seconds": 0.634}, "timestamp": "2026-01-25T20:01:34.582775"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 383.037, "latencies_ms": [383.037], "images_per_second": 2.611, "prompt_tokens": 744, "response_tokens_est": 46, "n_tiles": 1, "output_text": "\nA table is covered with a variety of fresh produce, including red radishes, green broccoli, and orange carrots. The vegetables are arranged in an appealing manner, showcasing their vibrant colors against the backdrop of the wooden table.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10492.9, "ram_available_mb": 112013.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10492.5, "ram_available_mb": 112013.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_watts_samples": [35.76, 35.76, 35.76, 35.76], "power_watts_avg": 35.76, "power_watts_peak": 35.76, "energy_joules_est": 13.72, "sample_count": 4, "duration_seconds": 0.384}, "timestamp": "2026-01-25T20:01:34.997001"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.39, "latencies_ms": [82.39], "images_per_second": 12.137, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Broccoli: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10492.5, "ram_available_mb": 112013.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10484.5, "ram_available_mb": 112021.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [35.76], "power_watts_avg": 35.76, "power_watts_peak": 35.76, "energy_joules_est": 2.95, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:01:35.102387"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 139.868, "latencies_ms": [139.868], "images_per_second": 7.15, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n 1. Broccoli and carrots in a basket on the left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.5, "ram_available_mb": 112021.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10483.8, "ram_available_mb": 112022.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [40.39, 40.39], "power_watts_avg": 40.39, "power_watts_peak": 40.39, "energy_joules_est": 5.66, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T20:01:35.308507"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 271.429, "latencies_ms": [271.429], "images_per_second": 3.684, "prompt_tokens": 757, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\nA table covered with a white cloth displays an assortment of fresh fruits, vegetables, and flowers. The produce includes strawberries, broccoli, carrots, radishes, green beans, potatoes, and zucchini.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.8, "ram_available_mb": 112022.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10483.8, "ram_available_mb": 112022.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [40.39, 40.39, 40.39], "power_watts_avg": 40.39, "power_watts_peak": 40.39, "energy_joules_est": 10.98, "sample_count": 3, "duration_seconds": 0.272}, "timestamp": "2026-01-25T20:01:35.615021"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 413.656, "latencies_ms": [413.656], "images_per_second": 2.417, "prompt_tokens": 756, "response_tokens_est": 71, "n_tiles": 1, "output_text": "\n The image shows a variety of fresh vegetables and fruits on display. There are several bunches of broccoli with green leaves, some carrots in the foreground, and strawberries in the background. A few red radishes can also be seen among the produce. The colors of the vegetables range from green to orange, creating an appealing visual contrast against each other.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.8, "ram_available_mb": 112022.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.36, 35.36, 35.36, 35.36, 35.36], "power_watts_avg": 35.36, "power_watts_peak": 35.36, "energy_joules_est": 14.64, "sample_count": 5, "duration_seconds": 0.414}, "timestamp": "2026-01-25T20:01:36.122873"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 256.133, "latencies_ms": [256.133], "images_per_second": 3.904, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nThree men are sitting on a couch, each holding a Wii remote and engaged in playing a video game together.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10483.3, "ram_available_mb": 112023.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.11, 35.11, 35.11], "power_watts_avg": 35.11, "power_watts_peak": 35.11, "energy_joules_est": 9.01, "sample_count": 3, "duration_seconds": 0.257}, "timestamp": "2026-01-25T20:01:36.432532"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.329, "latencies_ms": [82.329], "images_per_second": 12.146, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bed: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.3, "ram_available_mb": 112023.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10484.0, "ram_available_mb": 112022.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.11], "power_watts_avg": 35.11, "power_watts_peak": 35.11, "energy_joules_est": 2.91, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:01:36.538589"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 137.998, "latencies_ms": [137.998], "images_per_second": 7.246, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n - A man in a black shirt is holding a wii controller.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.0, "ram_available_mb": 112022.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10477.2, "ram_available_mb": 112029.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.42, 36.42], "power_watts_avg": 36.42, "power_watts_peak": 36.42, "energy_joules_est": 5.06, "sample_count": 2, "duration_seconds": 0.139}, "timestamp": "2026-01-25T20:01:36.744441"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 216.995, "latencies_ms": [216.995], "images_per_second": 4.608, "prompt_tokens": 757, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nTwo men are sitting on a couch, playing video games. One man holds a Wii remote in his hand while another man has one in his other hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.2, "ram_available_mb": 112029.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10479.1, "ram_available_mb": 112027.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [36.42, 36.42, 36.42], "power_watts_avg": 36.42, "power_watts_peak": 36.42, "energy_joules_est": 7.91, "sample_count": 3, "duration_seconds": 0.217}, "timestamp": "2026-01-25T20:01:37.050124"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 119.447, "latencies_ms": [119.447], "images_per_second": 8.372, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. The man is wearing a black shirt and glasses.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10479.1, "ram_available_mb": 112027.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10481.0, "ram_available_mb": 112025.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [31.73, 31.73], "power_watts_avg": 31.73, "power_watts_peak": 31.73, "energy_joules_est": 3.79, "sample_count": 2, "duration_seconds": 0.12}, "timestamp": "2026-01-25T20:01:37.256667"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 266.818, "latencies_ms": [266.818], "images_per_second": 3.748, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA group of cows are resting in a lush green field, with some lying down and others standing near trees.", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 10481.0, "ram_available_mb": 112025.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10473.6, "ram_available_mb": 112032.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [31.73, 31.73, 31.73], "power_watts_avg": 31.73, "power_watts_peak": 31.73, "energy_joules_est": 8.49, "sample_count": 3, "duration_seconds": 0.267}, "timestamp": "2026-01-25T20:01:37.571469"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 102.355, "latencies_ms": [102.355], "images_per_second": 9.77, "prompt_tokens": 759, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Tree trunk and roots - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.6, "ram_available_mb": 112032.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10476.0, "ram_available_mb": 112030.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [32.31, 32.31], "power_watts_avg": 32.31, "power_watts_peak": 32.31, "energy_joules_est": 3.33, "sample_count": 2, "duration_seconds": 0.103}, "timestamp": "2026-01-25T20:01:37.777957"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 114.632, "latencies_ms": [114.632], "images_per_second": 8.724, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n A tree trunk and a cow in the background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 10476.0, "ram_available_mb": 112030.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [32.31, 32.31], "power_watts_avg": 32.31, "power_watts_peak": 32.31, "energy_joules_est": 3.72, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T20:01:37.984548"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 313.796, "latencies_ms": [313.796], "images_per_second": 3.187, "prompt_tokens": 757, "response_tokens_est": 53, "n_tiles": 1, "output_text": "\nIn a lush green field, two white cows are lying down under a large tree. One cow is closer to the viewer while the other one is further away in the background. The scene depicts a peaceful moment of rest for these animals amidst their natural surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10476.0, "ram_available_mb": 112030.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [32.31, 28.52, 28.52, 28.52], "power_watts_avg": 29.47, "power_watts_peak": 32.31, "energy_joules_est": 9.27, "sample_count": 4, "duration_seconds": 0.315}, "timestamp": "2026-01-25T20:01:38.391206"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 264.79, "latencies_ms": [264.79], "images_per_second": 3.777, "prompt_tokens": 756, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\n The image features a group of cows and sheep in the field. There are two white cows lying down under a tree, while three black and white sheep are resting on the grassy area next to them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.0, "ram_available_mb": 112030.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [28.52, 28.52, 27.87], "power_watts_avg": 28.3, "power_watts_peak": 28.52, "energy_joules_est": 7.5, "sample_count": 3, "duration_seconds": 0.265}, "timestamp": "2026-01-25T20:01:38.698328"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 273.552, "latencies_ms": [273.552], "images_per_second": 3.656, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urns of water are on display in front of a brick building, with a group of people posing for a photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [27.87, 27.87, 27.87], "power_watts_avg": 27.87, "power_watts_peak": 27.87, "energy_joules_est": 7.63, "sample_count": 3, "duration_seconds": 0.274}, "timestamp": "2026-01-25T20:01:39.007409"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 219.841, "latencies_ms": [219.841], "images_per_second": 4.549, "prompt_tokens": 759, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\n 1. Person 2.3  2.4  3.5  4.6  5.7  6.75  7.81  8.9", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10475.8, "ram_available_mb": 112030.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [27.87, 37.08, 37.08], "power_watts_avg": 34.01, "power_watts_peak": 37.08, "energy_joules_est": 7.49, "sample_count": 3, "duration_seconds": 0.22}, "timestamp": "2026-01-25T20:01:39.314681"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 110.758, "latencies_ms": [110.758], "images_per_second": 9.029, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Person in front row on left side of photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.8, "ram_available_mb": 112030.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [37.08, 37.08], "power_watts_avg": 37.08, "power_watts_peak": 37.08, "energy_joules_est": 4.13, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T20:01:39.519617"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 96.738, "latencies_ms": [96.738], "images_per_second": 10.337, "prompt_tokens": 757, "response_tokens_est": 11, "n_tiles": 1, "output_text": "urns of water are on a table.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10470.4, "ram_available_mb": 112035.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [37.08], "power_watts_avg": 37.08, "power_watts_peak": 37.08, "energy_joules_est": 3.61, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T20:01:39.623905"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 48.138, "latencies_ms": [48.138], "images_per_second": 20.774, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.4, "ram_available_mb": 112035.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [34.66], "power_watts_avg": 34.66, "power_watts_peak": 34.66, "energy_joules_est": 1.68, "sample_count": 1, "duration_seconds": 0.048}, "timestamp": "2026-01-25T20:01:39.729456"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 325.952, "latencies_ms": [325.952], "images_per_second": 3.068, "prompt_tokens": 744, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA large, rainbow-colored kite soars high in a clear blue sky with fluffy white clouds. The kite is flying over a grassy field and houses below.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.66, 34.66, 34.66, 33.49], "power_watts_avg": 34.37, "power_watts_peak": 34.66, "energy_joules_est": 11.23, "sample_count": 4, "duration_seconds": 0.327}, "timestamp": "2026-01-25T20:01:40.138399"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 692.368, "latencies_ms": [692.368], "images_per_second": 1.444, "prompt_tokens": 759, "response_tokens_est": 128, "n_tiles": 1, "output_text": "\n 1. House  2. Tree 3. Car 4. Car 5. Car 6. Car 7. Car 8. Car 9. Car 10. Car 11. Car 12. Car 13. Car 14. Car 15. Car 16. Car 17. Car 18. Car 19. Car 20. Car 21. Car 22. Car 23. Car 24. Car 25. Car 26. Car 27. Car 28. Car 29. Car 30. Car 31. Car 32. Car 33. Car 34. Car 35. Car 36. Car 37. Car 38. Car 39. Car 40. Car 41. Car 42. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.49, 33.49, 33.49, 33.49, 37.01, 37.01, 37.01], "power_watts_avg": 35.0, "power_watts_peak": 37.01, "energy_joules_est": 24.24, "sample_count": 7, "duration_seconds": 0.693}, "timestamp": "2026-01-25T20:01:40.845721"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 122.443, "latencies_ms": [122.443], "images_per_second": 8.167, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nA colorful kite is flying in the sky above a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [37.01, 37.01], "power_watts_avg": 37.01, "power_watts_peak": 37.01, "energy_joules_est": 4.54, "sample_count": 2, "duration_seconds": 0.123}, "timestamp": "2026-01-25T20:01:41.050987"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 257.121, "latencies_ms": [257.121], "images_per_second": 3.889, "prompt_tokens": 757, "response_tokens_est": 42, "n_tiles": 1, "output_text": "\nA rainbow-colored kite soars high in a clear blue sky, with fluffy white clouds below. The kite's long tail trails behind it as it flies over a park filled with trees.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [36.38, 36.38, 36.38], "power_watts_avg": 36.38, "power_watts_peak": 36.38, "energy_joules_est": 9.36, "sample_count": 3, "duration_seconds": 0.257}, "timestamp": "2026-01-25T20:01:41.356353"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 114.849, "latencies_ms": [114.849], "images_per_second": 8.707, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A colorful kite with a long tail flying in the sky.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [36.38, 36.38], "power_watts_avg": 36.38, "power_watts_peak": 36.38, "energy_joules_est": 4.19, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T20:01:41.563006"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 242.398, "latencies_ms": [242.398], "images_per_second": 4.125, "prompt_tokens": 744, "response_tokens_est": 19, "n_tiles": 1, "output_text": "iced pizza with cheese and sauce on top of a cardboard box, ready to be eaten.", "error": null, "sys_before": {"cpu_percent": 6.1, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10471.6, "ram_available_mb": 112034.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [33.91, 33.91, 33.91], "power_watts_avg": 33.91, "power_watts_peak": 33.91, "energy_joules_est": 8.24, "sample_count": 3, "duration_seconds": 0.243}, "timestamp": "2026-01-25T20:01:41.881960"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 89.869, "latencies_ms": [89.869], "images_per_second": 11.127, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Pizza crust", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.6, "ram_available_mb": 112034.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10472.2, "ram_available_mb": 112034.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [33.91], "power_watts_avg": 33.91, "power_watts_peak": 33.91, "energy_joules_est": 3.08, "sample_count": 1, "duration_seconds": 0.091}, "timestamp": "2026-01-25T20:01:41.988252"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 119.085, "latencies_ms": [119.085], "images_per_second": 8.397, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Pizza in a box on top of table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.2, "ram_available_mb": 112034.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [33.91, 31.72], "power_watts_avg": 32.82, "power_watts_peak": 33.91, "energy_joules_est": 3.91, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T20:01:42.193205"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 140.323, "latencies_ms": [140.323], "images_per_second": 7.126, "prompt_tokens": 757, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA large pepperoni pizza sits in a cardboard box on top of a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10478.9, "ram_available_mb": 112027.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [31.72, 31.72], "power_watts_avg": 31.72, "power_watts_peak": 31.72, "energy_joules_est": 4.48, "sample_count": 2, "duration_seconds": 0.141}, "timestamp": "2026-01-25T20:01:42.399259"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 86.913, "latencies_ms": [86.913], "images_per_second": 11.506, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n The pizza is in a cardboard box.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.9, "ram_available_mb": 112027.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10479.3, "ram_available_mb": 112027.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [31.72], "power_watts_avg": 31.72, "power_watts_peak": 31.72, "energy_joules_est": 2.78, "sample_count": 1, "duration_seconds": 0.088}, "timestamp": "2026-01-25T20:01:42.505091"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 272.925, "latencies_ms": [272.925], "images_per_second": 3.664, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nTwo women are sitting on a small refrigerator, with one woman smoking and the other holding a cup of coffee.", "error": null, "sys_before": {"cpu_percent": 3.4, "ram_used_mb": 10479.3, "ram_available_mb": 112027.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [31.72, 29.0, 29.0], "power_watts_avg": 29.91, "power_watts_peak": 31.72, "energy_joules_est": 8.18, "sample_count": 3, "duration_seconds": 0.274}, "timestamp": "2026-01-25T20:01:42.822289"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 113.153, "latencies_ms": [113.153], "images_per_second": 8.838, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Door of refrigerator - 0.5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [29.0, 29.0], "power_watts_avg": 29.0, "power_watts_peak": 29.0, "energy_joules_est": 3.29, "sample_count": 2, "duration_seconds": 0.114}, "timestamp": "2026-01-25T20:01:43.028940"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 109.518, "latencies_ms": [109.518], "images_per_second": 9.131, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\nTwo women are sitting on a refrigerator.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10460.3, "ram_available_mb": 112046.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [30.5, 30.5], "power_watts_avg": 30.5, "power_watts_peak": 30.5, "energy_joules_est": 3.35, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T20:01:43.236919"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 196.057, "latencies_ms": [196.057], "images_per_second": 5.101, "prompt_tokens": 757, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nTwo women are sitting on a small refrigerator outside, with one of them smoking. The refrigerator has an open door revealing its interior.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.3, "ram_available_mb": 112046.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [30.5, 30.5], "power_watts_avg": 30.5, "power_watts_peak": 30.5, "energy_joules_est": 5.99, "sample_count": 2, "duration_seconds": 0.196}, "timestamp": "2026-01-25T20:01:43.442769"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 138.154, "latencies_ms": [138.154], "images_per_second": 7.238, "prompt_tokens": 756, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n The woman sitting in the open refrigerator is wearing a black jacket and blue jeans.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [30.5, 31.59], "power_watts_avg": 31.04, "power_watts_peak": 31.59, "energy_joules_est": 4.3, "sample_count": 2, "duration_seconds": 0.138}, "timestamp": "2026-01-25T20:01:43.647615"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 322.716, "latencies_ms": [322.716], "images_per_second": 3.099, "prompt_tokens": 744, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA man in a green polo shirt and straw hat holds a tray of hot dogs on a silver foil pan, with some of them appearing to be burnt.", "error": null, "sys_before": {"cpu_percent": 3.7, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10462.3, "ram_available_mb": 112044.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [31.59, 31.59, 31.59, 31.59], "power_watts_avg": 31.59, "power_watts_peak": 31.59, "energy_joules_est": 10.21, "sample_count": 4, "duration_seconds": 0.323}, "timestamp": "2026-01-25T20:01:44.064179"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 63.283, "latencies_ms": [63.283], "images_per_second": 15.802, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n Hat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.3, "ram_available_mb": 112044.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10469.5, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [32.93], "power_watts_avg": 32.93, "power_watts_peak": 32.93, "energy_joules_est": 2.09, "sample_count": 1, "duration_seconds": 0.063}, "timestamp": "2026-01-25T20:01:44.169905"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 99.445, "latencies_ms": [99.445], "images_per_second": 10.056, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Hat on man's head.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10469.5, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10473.3, "ram_available_mb": 112033.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [32.93], "power_watts_avg": 32.93, "power_watts_peak": 32.93, "energy_joules_est": 3.29, "sample_count": 1, "duration_seconds": 0.1}, "timestamp": "2026-01-25T20:01:44.275532"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 213.819, "latencies_ms": [213.819], "images_per_second": 4.677, "prompt_tokens": 757, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA man in a green polo shirt holds up a tray of hot dogs on a silver foil pan, likely at an outdoor event or gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.3, "ram_available_mb": 112033.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [32.93, 32.93, 32.93], "power_watts_avg": 32.93, "power_watts_peak": 32.93, "energy_joules_est": 7.05, "sample_count": 3, "duration_seconds": 0.214}, "timestamp": "2026-01-25T20:01:44.581442"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 122.921, "latencies_ms": [122.921], "images_per_second": 8.135, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n The man is wearing a hat and holding a tray of hot dogs.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10475.8, "ram_available_mb": 112030.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [33.21, 33.21], "power_watts_avg": 33.21, "power_watts_peak": 33.21, "energy_joules_est": 4.09, "sample_count": 2, "duration_seconds": 0.123}, "timestamp": "2026-01-25T20:01:44.787166"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 252.722, "latencies_ms": [252.722], "images_per_second": 3.957, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urns of flowers sit on a bookshelf in a living room, with a laptop and chair nearby.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10475.8, "ram_available_mb": 112030.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [33.21, 33.21, 33.21], "power_watts_avg": 33.21, "power_watts_peak": 33.21, "energy_joules_est": 8.41, "sample_count": 3, "duration_seconds": 0.253}, "timestamp": "2026-01-25T20:01:45.099685"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 84.557, "latencies_ms": [84.557], "images_per_second": 11.826, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bookshelf", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10472.8, "ram_available_mb": 112033.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.64], "power_watts_avg": 31.64, "power_watts_peak": 31.64, "energy_joules_est": 2.69, "sample_count": 1, "duration_seconds": 0.085}, "timestamp": "2026-01-25T20:01:45.206615"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 120.043, "latencies_ms": [120.043], "images_per_second": 8.33, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A bookshelf filled with books is located next to a couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.8, "ram_available_mb": 112033.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.64, 31.64], "power_watts_avg": 31.64, "power_watts_peak": 31.64, "energy_joules_est": 3.82, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T20:01:45.413961"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 311.74, "latencies_ms": [311.74], "images_per_second": 3.208, "prompt_tokens": 757, "response_tokens_est": 53, "n_tiles": 1, "output_text": "\nThe image shows a living room with a couch, bookshelf filled with books, and a desk. A laptop computer sits on top of the desk. The room appears to be in need of cleaning or reorganizing due to the clutter around the furniture.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.64, 31.64, 30.84, 30.84], "power_watts_avg": 31.24, "power_watts_peak": 31.64, "energy_joules_est": 9.75, "sample_count": 4, "duration_seconds": 0.312}, "timestamp": "2026-01-25T20:01:45.821678"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 376.603, "latencies_ms": [376.603], "images_per_second": 2.655, "prompt_tokens": 756, "response_tokens_est": 66, "n_tiles": 1, "output_text": "\nThe room is mostly white with a gray carpet. The walls are painted beige and there's a window on the left side of the room. A bookshelf filled with various items stands against one wall, and a chair sits in front of it. There is also a laptop computer sitting on top of the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10467.3, "ram_available_mb": 112039.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.84, 30.84, 30.84, 30.98], "power_watts_avg": 30.88, "power_watts_peak": 30.98, "energy_joules_est": 11.64, "sample_count": 4, "duration_seconds": 0.377}, "timestamp": "2026-01-25T20:01:46.230095"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 223.642, "latencies_ms": [223.642], "images_per_second": 4.471, "prompt_tokens": 744, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nTwo elephants stand in a field, facing each other with their trunks intertwined.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10467.3, "ram_available_mb": 112039.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10467.3, "ram_available_mb": 112039.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.98, 30.98, 30.98], "power_watts_avg": 30.98, "power_watts_peak": 30.98, "energy_joules_est": 6.95, "sample_count": 3, "duration_seconds": 0.224}, "timestamp": "2026-01-25T20:01:46.545176"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.041, "latencies_ms": [82.041], "images_per_second": 12.189, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Elephant: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.3, "ram_available_mb": 112039.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [37.91], "power_watts_avg": 37.91, "power_watts_peak": 37.91, "energy_joules_est": 3.13, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:01:46.650639"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 118.02, "latencies_ms": [118.02], "images_per_second": 8.473, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe elephant on the left is facing away from the camera.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10468.7, "ram_available_mb": 112037.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [37.91, 37.91], "power_watts_avg": 37.91, "power_watts_peak": 37.91, "energy_joules_est": 4.48, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:01:46.856455"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 224.949, "latencies_ms": [224.949], "images_per_second": 4.445, "prompt_tokens": 757, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nTwo large elephants are standing in a field, facing each other. The elephant on the left has its trunk raised while the one on the right has its trunk lowered.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.7, "ram_available_mb": 112037.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [37.91, 37.91, 34.22], "power_watts_avg": 36.68, "power_watts_peak": 37.91, "energy_joules_est": 8.27, "sample_count": 3, "duration_seconds": 0.226}, "timestamp": "2026-01-25T20:01:47.164630"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 448.042, "latencies_ms": [448.042], "images_per_second": 2.232, "prompt_tokens": 756, "response_tokens_est": 79, "n_tiles": 1, "output_text": "\n The image shows two elephants standing in a field with tall grass and trees. One elephant is facing the camera while the other one has its back turned to us. They are both brown in color, which makes them blend well with their surroundings. The sky above them appears hazy, suggesting that it might be early morning or late afternoon when the sun's rays are not as strong.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.22, 34.22, 34.22, 34.22, 31.59], "power_watts_avg": 33.69, "power_watts_peak": 34.22, "energy_joules_est": 15.13, "sample_count": 5, "duration_seconds": 0.449}, "timestamp": "2026-01-25T20:01:47.672818"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 292.97, "latencies_ms": [292.97], "images_per_second": 3.413, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA man in a pink hat and shirtless, holding a white frisbee, stands on a grassy field with trees in the background.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10469.4, "ram_available_mb": 112036.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.59, 31.59, 31.59], "power_watts_avg": 31.59, "power_watts_peak": 31.59, "energy_joules_est": 9.26, "sample_count": 3, "duration_seconds": 0.293}, "timestamp": "2026-01-25T20:01:47.981790"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 127.781, "latencies_ms": [127.781], "images_per_second": 7.826, "prompt_tokens": 759, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Shirtless man holding frisbee and water bottle", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.4, "ram_available_mb": 112036.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10469.4, "ram_available_mb": 112036.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [31.59, 37.61], "power_watts_avg": 34.6, "power_watts_peak": 37.61, "energy_joules_est": 4.43, "sample_count": 2, "duration_seconds": 0.128}, "timestamp": "2026-01-25T20:01:48.187310"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 141.745, "latencies_ms": [141.745], "images_per_second": 7.055, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nThe man is holding a white frisbee in his hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.4, "ram_available_mb": 112036.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [37.61, 37.61], "power_watts_avg": 37.61, "power_watts_peak": 37.61, "energy_joules_est": 5.35, "sample_count": 2, "duration_seconds": 0.142}, "timestamp": "2026-01-25T20:01:48.393519"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 231.358, "latencies_ms": [231.358], "images_per_second": 4.322, "prompt_tokens": 757, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nIn a lush green field, two people are playing frisbee. One person is holding a white frisbee in their hand while standing on the grassy field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10466.8, "ram_available_mb": 112039.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [37.61, 37.61, 33.65], "power_watts_avg": 36.29, "power_watts_peak": 37.61, "energy_joules_est": 8.41, "sample_count": 3, "duration_seconds": 0.232}, "timestamp": "2026-01-25T20:01:48.700454"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 113.758, "latencies_ms": [113.758], "images_per_second": 8.791, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The man is wearing a pink hat and shirtless.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.8, "ram_available_mb": 112039.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [33.65, 33.65], "power_watts_avg": 33.65, "power_watts_peak": 33.65, "energy_joules_est": 3.84, "sample_count": 2, "duration_seconds": 0.114}, "timestamp": "2026-01-25T20:01:48.906781"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 255.772, "latencies_ms": [255.772], "images_per_second": 3.91, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "urn of gold and a white horse are on a table, with a chocolate skateboard cake in front of them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [33.65, 33.65, 30.15], "power_watts_avg": 32.48, "power_watts_peak": 33.65, "energy_joules_est": 8.32, "sample_count": 3, "duration_seconds": 0.256}, "timestamp": "2026-01-25T20:01:49.219163"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 91.693, "latencies_ms": [91.693], "images_per_second": 10.906, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Cake: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [30.15], "power_watts_avg": 30.15, "power_watts_peak": 30.15, "energy_joules_est": 2.78, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T20:01:49.324948"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 85.89, "latencies_ms": [85.89], "images_per_second": 11.643, "prompt_tokens": 763, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urns of water on table", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [30.15], "power_watts_avg": 30.15, "power_watts_peak": 30.15, "energy_joules_est": 2.6, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T20:01:49.429584"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 242.875, "latencies_ms": [242.875], "images_per_second": 4.117, "prompt_tokens": 757, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\n A young boy in a blue shirt is cutting into a cake with a knife on a dining table. The cake has toy cars on top, making it unique and fun for the celebration.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [30.15, 31.67, 31.67], "power_watts_avg": 31.16, "power_watts_peak": 31.67, "energy_joules_est": 7.58, "sample_count": 3, "duration_seconds": 0.243}, "timestamp": "2026-01-25T20:01:49.736500"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 205.367, "latencies_ms": [205.367], "images_per_second": 4.869, "prompt_tokens": 756, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nThe cake is on a table with a boy in blue shirt sitting at the table. The cake has chocolate frosting and toy cars made of chocolate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10474.0, "ram_available_mb": 112032.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [31.67, 31.67, 31.67], "power_watts_avg": 31.67, "power_watts_peak": 31.67, "energy_joules_est": 6.52, "sample_count": 3, "duration_seconds": 0.206}, "timestamp": "2026-01-25T20:01:50.043102"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 271.661, "latencies_ms": [271.661], "images_per_second": 3.681, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA zebra stands next to a fence, its head tilted down as it leans over to eat from a trough on the ground.", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 10474.0, "ram_available_mb": 112032.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10476.0, "ram_available_mb": 112030.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [32.23, 32.23, 32.23], "power_watts_avg": 32.23, "power_watts_peak": 32.23, "energy_joules_est": 8.77, "sample_count": 3, "duration_seconds": 0.272}, "timestamp": "2026-01-25T20:01:50.355209"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 84.178, "latencies_ms": [84.178], "images_per_second": 11.88, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Zebra face", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.0, "ram_available_mb": 112030.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10477.5, "ram_available_mb": 112028.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [32.23], "power_watts_avg": 32.23, "power_watts_peak": 32.23, "energy_joules_est": 2.73, "sample_count": 1, "duration_seconds": 0.085}, "timestamp": "2026-01-25T20:01:50.460862"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 122.108, "latencies_ms": [122.108], "images_per_second": 8.189, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe zebra is looking at a tree in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.5, "ram_available_mb": 112028.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10478.2, "ram_available_mb": 112028.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [32.23, 34.42], "power_watts_avg": 33.32, "power_watts_peak": 34.42, "energy_joules_est": 4.1, "sample_count": 2, "duration_seconds": 0.123}, "timestamp": "2026-01-25T20:01:50.668399"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 342.538, "latencies_ms": [342.538], "images_per_second": 2.919, "prompt_tokens": 757, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\nIn this image, there are two zebras in a zoo enclosure. One zebra is standing next to another zebra that is eating from a trough of food. The zebras have distinct black and white stripes on their bodies, which makes them easily recognizable.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 10478.2, "ram_available_mb": 112028.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10476.9, "ram_available_mb": 112029.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.42, 34.42, 34.42, 34.42], "power_watts_avg": 34.42, "power_watts_peak": 34.42, "energy_joules_est": 11.82, "sample_count": 4, "duration_seconds": 0.343}, "timestamp": "2026-01-25T20:01:51.076006"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 139.328, "latencies_ms": [139.328], "images_per_second": 7.177, "prompt_tokens": 756, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n The zebra is standing in a zoo and has black and white stripes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.9, "ram_available_mb": 112029.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.84, 34.84], "power_watts_avg": 34.84, "power_watts_peak": 34.84, "energy_joules_est": 4.87, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T20:01:51.283374"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 532.891, "latencies_ms": [532.891], "images_per_second": 1.877, "prompt_tokens": 744, "response_tokens_est": 77, "n_tiles": 1, "output_text": "\nThe image shows a train station with two trains on parallel tracks, one of which is pulling into the station and another waiting for passengers to board or disembark. The platform has a sign that reads \"La Spezia Centrale\", indicating the name of the station. A person can be seen near the edge of the platform, possibly waiting for their train or just passing by.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10479.3, "ram_available_mb": 112027.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.84, 34.84, 34.84, 34.87, 34.87, 34.87], "power_watts_avg": 34.85, "power_watts_peak": 34.87, "energy_joules_est": 18.59, "sample_count": 6, "duration_seconds": 0.533}, "timestamp": "2026-01-25T20:01:51.896861"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 307.469, "latencies_ms": [307.469], "images_per_second": 3.252, "prompt_tokens": 759, "response_tokens_est": 49, "n_tiles": 1, "output_text": "\n 1. Train station platform 2. Train station platform 3. Train station platform 4. Train station platform 5. Train station platform 6. Train station platform 7. Train station platform 8. Train station platform 9. Train station platform 10.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.3, "ram_available_mb": 112027.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10479.5, "ram_available_mb": 112026.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [34.87, 34.87, 35.62, 35.62], "power_watts_avg": 35.25, "power_watts_peak": 35.62, "energy_joules_est": 10.87, "sample_count": 4, "duration_seconds": 0.308}, "timestamp": "2026-01-25T20:01:52.303998"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 193.868, "latencies_ms": [193.868], "images_per_second": 5.158, "prompt_tokens": 763, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\n 1. A sign that says \"La Spezia Centrale\" is on a pole in front of a train station.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10479.5, "ram_available_mb": 112026.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10480.0, "ram_available_mb": 112026.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [35.62, 35.62], "power_watts_avg": 35.62, "power_watts_peak": 35.62, "energy_joules_est": 6.92, "sample_count": 2, "duration_seconds": 0.194}, "timestamp": "2026-01-25T20:01:52.509947"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 167.275, "latencies_ms": [167.275], "images_per_second": 5.978, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA train station with a sign that says \"La Spezia Centrale\" in black letters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.0, "ram_available_mb": 112026.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10476.1, "ram_available_mb": 112030.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [35.62, 34.21], "power_watts_avg": 34.92, "power_watts_peak": 35.62, "energy_joules_est": 5.85, "sample_count": 2, "duration_seconds": 0.167}, "timestamp": "2026-01-25T20:01:52.715908"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 378.81, "latencies_ms": [378.81], "images_per_second": 2.64, "prompt_tokens": 756, "response_tokens_est": 63, "n_tiles": 1, "output_text": "\n The image is a black and white photograph of an old train station with two trains on the tracks. One train appears to be pulling into the station while the other one is parked at the platform. There are people present in the scene as well, some standing near the platform and others closer to the trains.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.1, "ram_available_mb": 112030.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.21, 34.21, 34.21, 34.21], "power_watts_avg": 34.21, "power_watts_peak": 34.21, "energy_joules_est": 12.98, "sample_count": 4, "duration_seconds": 0.379}, "timestamp": "2026-01-25T20:01:53.124394"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 256.086, "latencies_ms": [256.086], "images_per_second": 3.905, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "urn of water with a surfer in a black wetsuit sitting on top, facing away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.96, 33.96, 33.96], "power_watts_avg": 33.96, "power_watts_peak": 33.96, "energy_joules_est": 8.72, "sample_count": 3, "duration_seconds": 0.257}, "timestamp": "2026-01-25T20:01:53.438157"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 178.579, "latencies_ms": [178.579], "images_per_second": 5.6, "prompt_tokens": 759, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\n 1. Surfer in black wetsuit  2. Red surfboard 3. Sunset 4. Sky 5. Clouds", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10463.4, "ram_available_mb": 112042.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.96, 37.26], "power_watts_avg": 35.61, "power_watts_peak": 37.26, "energy_joules_est": 6.37, "sample_count": 2, "duration_seconds": 0.179}, "timestamp": "2026-01-25T20:01:53.643454"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 156.387, "latencies_ms": [156.387], "images_per_second": 6.394, "prompt_tokens": 763, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA woman in a wetsuit is sitting on her surfboard while looking out at the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.6, "ram_available_mb": 112042.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.26, 37.26], "power_watts_avg": 37.26, "power_watts_peak": 37.26, "energy_joules_est": 5.85, "sample_count": 2, "duration_seconds": 0.157}, "timestamp": "2026-01-25T20:01:53.849258"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 161.468, "latencies_ms": [161.468], "images_per_second": 6.193, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA woman in a wetsuit sits on her surfboard, looking out at the ocean during sunset.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [37.26, 37.26], "power_watts_avg": 37.26, "power_watts_peak": 37.26, "energy_joules_est": 6.04, "sample_count": 2, "duration_seconds": 0.162}, "timestamp": "2026-01-25T20:01:54.053588"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 130.522, "latencies_ms": [130.522], "images_per_second": 7.662, "prompt_tokens": 756, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n The woman is wearing a black wetsuit and has her hair pulled back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10476.3, "ram_available_mb": 112030.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.02, 36.02], "power_watts_avg": 36.02, "power_watts_peak": 36.02, "energy_joules_est": 4.71, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T20:01:54.259204"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 274.616, "latencies_ms": [274.616], "images_per_second": 3.641, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA man and a woman are seated at a table in an airplane, enjoying their meal of sushi rolls with chopsticks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.3, "ram_available_mb": 112030.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10480.7, "ram_available_mb": 112025.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.02, 36.02, 36.02], "power_watts_avg": 36.02, "power_watts_peak": 36.02, "energy_joules_est": 9.91, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T20:01:54.569923"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 126.705, "latencies_ms": [126.705], "images_per_second": 7.892, "prompt_tokens": 759, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n Tray 1: Food, utensils and drink; 2", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10480.7, "ram_available_mb": 112025.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10480.7, "ram_available_mb": 112025.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.28, 35.28], "power_watts_avg": 35.28, "power_watts_peak": 35.28, "energy_joules_est": 4.48, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:01:54.774963"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 165.075, "latencies_ms": [165.075], "images_per_second": 6.058, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n 1. Woman in pink shirt and pearls holding chopsticks over a tray of food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.7, "ram_available_mb": 112025.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10488.3, "ram_available_mb": 112018.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.28, 35.28], "power_watts_avg": 35.28, "power_watts_peak": 35.28, "energy_joules_est": 5.85, "sample_count": 2, "duration_seconds": 0.166}, "timestamp": "2026-01-25T20:01:54.980807"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 317.491, "latencies_ms": [317.491], "images_per_second": 3.15, "prompt_tokens": 757, "response_tokens_est": 51, "n_tiles": 1, "output_text": "\nA man and a woman are seated at a table in an airplane, enjoying their meal. The man holds chopsticks while the woman has a plate of food in front of her. They appear to be having a pleasant dining experience during their flight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.8, "ram_available_mb": 112017.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10487.3, "ram_available_mb": 112019.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [35.28, 30.91, 30.91, 30.91], "power_watts_avg": 32.0, "power_watts_peak": 35.28, "energy_joules_est": 10.18, "sample_count": 4, "duration_seconds": 0.318}, "timestamp": "2026-01-25T20:01:55.386763"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 53.473, "latencies_ms": [53.473], "images_per_second": 18.701, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10487.3, "ram_available_mb": 112019.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 10490.8, "ram_available_mb": 112015.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [30.91], "power_watts_avg": 30.91, "power_watts_peak": 30.91, "energy_joules_est": 1.66, "sample_count": 1, "duration_seconds": 0.054}, "timestamp": "2026-01-25T20:01:55.492793"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 279.074, "latencies_ms": [279.074], "images_per_second": 3.583, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nTwo men in suits and ties are walking down a city street at night, with one man making a peace sign gesture.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10489.1, "ram_available_mb": 112017.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10486.9, "ram_available_mb": 112019.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [30.91, 27.42, 27.42], "power_watts_avg": 28.58, "power_watts_peak": 30.91, "energy_joules_est": 7.99, "sample_count": 3, "duration_seconds": 0.28}, "timestamp": "2026-01-25T20:01:55.802126"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 220.427, "latencies_ms": [220.427], "images_per_second": 4.537, "prompt_tokens": 759, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\n 1. Building  2. Building  3. Building  4. Building  5. Building  6. Building  7. Building  8. Building", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10486.8, "ram_available_mb": 112019.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10487.7, "ram_available_mb": 112018.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [27.42, 27.42, 27.42], "power_watts_avg": 27.42, "power_watts_peak": 27.42, "energy_joules_est": 6.06, "sample_count": 3, "duration_seconds": 0.221}, "timestamp": "2026-01-25T20:01:56.108272"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 161.372, "latencies_ms": [161.372], "images_per_second": 6.197, "prompt_tokens": 763, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nThe man in a pink shirt is walking across the street while another man wearing a suit and tie walks behind him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10487.7, "ram_available_mb": 112018.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10490.6, "ram_available_mb": 112015.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.18, 36.18], "power_watts_avg": 36.18, "power_watts_peak": 36.18, "energy_joules_est": 5.85, "sample_count": 2, "duration_seconds": 0.162}, "timestamp": "2026-01-25T20:01:56.313127"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 116.284, "latencies_ms": [116.284], "images_per_second": 8.6, "prompt_tokens": 757, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nTwo men in suits are walking down a city street at night.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10490.6, "ram_available_mb": 112015.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10486.0, "ram_available_mb": 112020.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.18, 36.18], "power_watts_avg": 36.18, "power_watts_peak": 36.18, "energy_joules_est": 4.22, "sample_count": 2, "duration_seconds": 0.117}, "timestamp": "2026-01-25T20:01:56.519766"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 99.791, "latencies_ms": [99.791], "images_per_second": 10.021, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "urns of light and dark colors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10486.0, "ram_available_mb": 112020.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 10486.0, "ram_available_mb": 112020.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.18], "power_watts_avg": 36.18, "power_watts_peak": 36.18, "energy_joules_est": 3.62, "sample_count": 1, "duration_seconds": 0.1}, "timestamp": "2026-01-25T20:01:56.624614"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 308.867, "latencies_ms": [308.867], "images_per_second": 3.238, "prompt_tokens": 744, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA man in a gray shirt is wiping his hands on a white towel, while another person holds a wine glass and a bottle of wine at a bar.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10486.4, "ram_available_mb": 112019.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10478.9, "ram_available_mb": 112027.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [33.94, 33.94, 33.94, 33.94], "power_watts_avg": 33.94, "power_watts_peak": 33.94, "energy_joules_est": 10.49, "sample_count": 4, "duration_seconds": 0.309}, "timestamp": "2026-01-25T20:01:57.036333"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 84.522, "latencies_ms": [84.522], "images_per_second": 11.831, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Glasses: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.9, "ram_available_mb": 112027.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10480.1, "ram_available_mb": 112026.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [31.84], "power_watts_avg": 31.84, "power_watts_peak": 31.84, "energy_joules_est": 2.7, "sample_count": 1, "duration_seconds": 0.085}, "timestamp": "2026-01-25T20:01:57.139473"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 110.29, "latencies_ms": [110.29], "images_per_second": 9.067, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n A man is holding a wine glass in front of him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.1, "ram_available_mb": 112026.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10480.5, "ram_available_mb": 112025.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [31.84, 31.84], "power_watts_avg": 31.84, "power_watts_peak": 31.84, "energy_joules_est": 3.52, "sample_count": 2, "duration_seconds": 0.111}, "timestamp": "2026-01-25T20:01:57.344801"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 362.262, "latencies_ms": [362.262], "images_per_second": 2.76, "prompt_tokens": 757, "response_tokens_est": 60, "n_tiles": 1, "output_text": "\nIn a wine bar, a man in a gray shirt is pouring red wine into a glass. The table has two glasses of wine on it, one being filled by the bartender. There are also several bottles of wine behind them, indicating that this is an establishment serving various types of wines.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.5, "ram_available_mb": 112025.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10479.9, "ram_available_mb": 112026.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [31.84, 31.84, 32.6, 32.6], "power_watts_avg": 32.22, "power_watts_peak": 32.6, "energy_joules_est": 11.68, "sample_count": 4, "duration_seconds": 0.363}, "timestamp": "2026-01-25T20:01:57.751264"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 95.971, "latencies_ms": [95.971], "images_per_second": 10.42, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "urns of wine on a shelf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.9, "ram_available_mb": 112026.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10473.7, "ram_available_mb": 112032.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [32.6], "power_watts_avg": 32.6, "power_watts_peak": 32.6, "energy_joules_est": 3.14, "sample_count": 1, "duration_seconds": 0.096}, "timestamp": "2026-01-25T20:01:57.856674"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 298.937, "latencies_ms": [298.937], "images_per_second": 3.345, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA tennis player in white is captured mid-swing, with a blue and white racket poised to strike an incoming ball on a grass court.", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 10473.7, "ram_available_mb": 112032.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10475.9, "ram_available_mb": 112030.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [32.6, 32.6, 33.52], "power_watts_avg": 32.9, "power_watts_peak": 33.52, "energy_joules_est": 9.85, "sample_count": 3, "duration_seconds": 0.299}, "timestamp": "2026-01-25T20:01:58.167716"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 105.837, "latencies_ms": [105.837], "images_per_second": 9.448, "prompt_tokens": 759, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Grass  2. Ball  3. Racket", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.9, "ram_available_mb": 112030.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10476.8, "ram_available_mb": 112029.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [33.52, 33.52], "power_watts_avg": 33.52, "power_watts_peak": 33.52, "energy_joules_est": 3.56, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T20:01:58.373545"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 112.725, "latencies_ms": [112.725], "images_per_second": 8.871, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe man is swinging a tennis racket at a ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.8, "ram_available_mb": 112029.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10476.8, "ram_available_mb": 112029.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [33.52, 33.52], "power_watts_avg": 33.52, "power_watts_peak": 33.52, "energy_joules_est": 3.79, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T20:01:58.581299"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 174.918, "latencies_ms": [174.918], "images_per_second": 5.717, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA man in white is playing tennis on a grass court, swinging his racket to hit a yellow ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.8, "ram_available_mb": 112029.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [34.94, 34.94], "power_watts_avg": 34.94, "power_watts_peak": 34.94, "energy_joules_est": 6.12, "sample_count": 2, "duration_seconds": 0.175}, "timestamp": "2026-01-25T20:01:58.788137"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 56.908, "latencies_ms": [56.908], "images_per_second": 17.572, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10475.8, "ram_available_mb": 112030.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 85.0}, "power_stats": {"power_watts_samples": [34.94], "power_watts_avg": 34.94, "power_watts_peak": 34.94, "energy_joules_est": 2.01, "sample_count": 1, "duration_seconds": 0.058}, "timestamp": "2026-01-25T20:01:58.893819"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 265.812, "latencies_ms": [265.812], "images_per_second": 3.762, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA white and brown cat is sitting on a wooden shelf, looking at a television screen displaying an actor's face.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.8, "ram_available_mb": 112030.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [34.94, 34.94, 30.53], "power_watts_avg": 33.47, "power_watts_peak": 34.94, "energy_joules_est": 8.92, "sample_count": 3, "duration_seconds": 0.266}, "timestamp": "2026-01-25T20:01:59.208092"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 90.52, "latencies_ms": [90.52], "images_per_second": 11.047, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Dvd player", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [30.53], "power_watts_avg": 30.53, "power_watts_peak": 30.53, "energy_joules_est": 2.77, "sample_count": 1, "duration_seconds": 0.091}, "timestamp": "2026-01-25T20:01:59.312715"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 116.654, "latencies_ms": [116.654], "images_per_second": 8.572, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. A cat on a shelf watching television.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [30.53, 30.53], "power_watts_avg": 30.53, "power_watts_peak": 30.53, "energy_joules_est": 3.57, "sample_count": 2, "duration_seconds": 0.117}, "timestamp": "2026-01-25T20:01:59.517683"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 225.121, "latencies_ms": [225.121], "images_per_second": 4.442, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA white cat with orange patches is sitting on a wooden entertainment center, looking at a television screen. The TV is turned on, displaying an image of a man wearing glasses.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [30.53, 29.73, 29.73], "power_watts_avg": 30.0, "power_watts_peak": 30.53, "energy_joules_est": 6.76, "sample_count": 3, "duration_seconds": 0.226}, "timestamp": "2026-01-25T20:01:59.823661"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 123.602, "latencies_ms": [123.602], "images_per_second": 8.09, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n A cat is sitting on top of a wooden entertainment center.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10475.4, "ram_available_mb": 112030.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [29.73, 29.73], "power_watts_avg": 29.73, "power_watts_peak": 29.73, "energy_joules_est": 3.69, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T20:02:00.029872"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 364.562, "latencies_ms": [364.562], "images_per_second": 2.743, "prompt_tokens": 744, "response_tokens_est": 46, "n_tiles": 1, "output_text": "\nA blue sign with a bicycle and pedestrian symbol is attached to a pole, accompanied by a white sign in Chinese characters that reads \"\u5927\u962a\u5e02\", indicating the location of an important city street or intersection.", "error": null, "sys_before": {"cpu_percent": 3.0, "ram_used_mb": 10475.4, "ram_available_mb": 112030.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10473.3, "ram_available_mb": 112033.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [29.94, 29.94, 29.94, 29.94], "power_watts_avg": 29.94, "power_watts_peak": 29.94, "energy_joules_est": 10.92, "sample_count": 4, "duration_seconds": 0.365}, "timestamp": "2026-01-25T20:02:00.447777"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 67.792, "latencies_ms": [67.792], "images_per_second": 14.751, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Tree", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10473.3, "ram_available_mb": 112033.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [29.94], "power_watts_avg": 29.94, "power_watts_peak": 29.94, "energy_joules_est": 2.04, "sample_count": 1, "duration_seconds": 0.068}, "timestamp": "2026-01-25T20:02:00.552857"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 111.909, "latencies_ms": [111.909], "images_per_second": 8.936, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nA blue sign with a bicycle and pedestrian crossing on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10477.4, "ram_available_mb": 112028.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [34.79, 34.79], "power_watts_avg": 34.79, "power_watts_peak": 34.79, "energy_joules_est": 3.92, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T20:02:00.758771"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 227.212, "latencies_ms": [227.212], "images_per_second": 4.401, "prompt_tokens": 757, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA blue sign with a bicycle symbol on it, along with a pedestrian crossing sign, is attached to a pole. The signs are in English and Chinese characters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.4, "ram_available_mb": 112028.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10473.9, "ram_available_mb": 112032.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_watts_samples": [34.79, 34.79, 34.79], "power_watts_avg": 34.79, "power_watts_peak": 34.79, "energy_joules_est": 7.92, "sample_count": 3, "duration_seconds": 0.228}, "timestamp": "2026-01-25T20:02:01.065657"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 112.01, "latencies_ms": [112.01], "images_per_second": 8.928, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n A blue sign with a bicycle and pedestrian crossing on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.9, "ram_available_mb": 112032.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10474.9, "ram_available_mb": 112031.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_watts_samples": [34.38, 34.38], "power_watts_avg": 34.38, "power_watts_peak": 34.38, "energy_joules_est": 3.88, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T20:02:01.272327"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 295.101, "latencies_ms": [295.101], "images_per_second": 3.389, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "urn of water on a table in front of a young girl with black hair and gold bracelets, who is eating pizza at a restaurant.\n", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 10474.9, "ram_available_mb": 112031.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10466.5, "ram_available_mb": 112039.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_watts_samples": [34.38, 34.38, 34.38], "power_watts_avg": 34.38, "power_watts_peak": 34.38, "energy_joules_est": 10.16, "sample_count": 3, "duration_seconds": 0.296}, "timestamp": "2026-01-25T20:02:01.586106"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 241.188, "latencies_ms": [241.188], "images_per_second": 4.146, "prompt_tokens": 759, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\n Chair 1: 2, Chair 2: 3, Chair 3: 4, Chair 4: 5, Chair 5: 6, Chair 6: 7, Chair 7: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_watts_samples": [31.66, 31.66, 31.66], "power_watts_avg": 31.66, "power_watts_peak": 31.66, "energy_joules_est": 7.66, "sample_count": 3, "duration_seconds": 0.242}, "timestamp": "2026-01-25T20:02:01.892341"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 96.396, "latencies_ms": [96.396], "images_per_second": 10.374, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Girl eating pizza at table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 74.0}, "power_stats": {"power_watts_samples": [31.66], "power_watts_avg": 31.66, "power_watts_peak": 31.66, "energy_joules_est": 3.06, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T20:02:01.997033"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 54.118, "latencies_ms": [54.118], "images_per_second": 18.478, "prompt_tokens": 757, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [31.66], "power_watts_avg": 31.66, "power_watts_peak": 31.66, "energy_joules_est": 1.72, "sample_count": 1, "duration_seconds": 0.054}, "timestamp": "2026-01-25T20:02:02.101334"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 90.329, "latencies_ms": [90.329], "images_per_second": 11.071, "prompt_tokens": 756, "response_tokens_est": 8, "n_tiles": 1, "output_text": "urns of water on table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10473.8, "ram_available_mb": 112032.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [32.64], "power_watts_avg": 32.64, "power_watts_peak": 32.64, "energy_joules_est": 2.96, "sample_count": 1, "duration_seconds": 0.091}, "timestamp": "2026-01-25T20:02:02.206083"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 296.708, "latencies_ms": [296.708], "images_per_second": 3.37, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\n A table is set with plates of food, including a bowl of broccoli and cauliflower salad, rice, breads, and other dishes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.8, "ram_available_mb": 112032.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [32.64, 32.64, 32.64], "power_watts_avg": 32.64, "power_watts_peak": 32.64, "energy_joules_est": 9.71, "sample_count": 3, "duration_seconds": 0.298}, "timestamp": "2026-01-25T20:02:02.515017"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 106.333, "latencies_ms": [106.333], "images_per_second": 9.404, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Salad with broccoli and carrots - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [32.64, 34.02], "power_watts_avg": 33.33, "power_watts_peak": 34.02, "energy_joules_est": 3.55, "sample_count": 2, "duration_seconds": 0.107}, "timestamp": "2026-01-25T20:02:02.718797"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 132.899, "latencies_ms": [132.899], "images_per_second": 7.525, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A plate of food is on a table next to another plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10472.0, "ram_available_mb": 112034.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [34.02, 34.02], "power_watts_avg": 34.02, "power_watts_peak": 34.02, "energy_joules_est": 4.54, "sample_count": 2, "duration_seconds": 0.133}, "timestamp": "2026-01-25T20:02:02.924250"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 130.144, "latencies_ms": [130.144], "images_per_second": 7.684, "prompt_tokens": 757, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nA table with plates of food, including a bowl of broccoli and rice.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.0, "ram_available_mb": 112034.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [34.02, 34.02], "power_watts_avg": 34.02, "power_watts_peak": 34.02, "energy_joules_est": 4.44, "sample_count": 2, "duration_seconds": 0.13}, "timestamp": "2026-01-25T20:02:03.129517"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 105.504, "latencies_ms": [105.504], "images_per_second": 9.478, "prompt_tokens": 756, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n A silver pan with food on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10469.7, "ram_available_mb": 112036.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [31.09, 31.09], "power_watts_avg": 31.09, "power_watts_peak": 31.09, "energy_joules_est": 3.29, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T20:02:03.334764"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 255.659, "latencies_ms": [255.659], "images_per_second": 3.911, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA green bus is driving down a city street, passing by several cars and a truck on its route.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10469.7, "ram_available_mb": 112036.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [31.09, 31.09, 28.86], "power_watts_avg": 30.35, "power_watts_peak": 31.09, "energy_joules_est": 7.77, "sample_count": 3, "duration_seconds": 0.256}, "timestamp": "2026-01-25T20:02:03.648502"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 80.315, "latencies_ms": [80.315], "images_per_second": 12.451, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bus: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10463.9, "ram_available_mb": 112042.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [28.86], "power_watts_avg": 28.86, "power_watts_peak": 28.86, "energy_joules_est": 2.33, "sample_count": 1, "duration_seconds": 0.081}, "timestamp": "2026-01-25T20:02:03.753226"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 117.377, "latencies_ms": [117.377], "images_per_second": 8.52, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Bus is in front of cars and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.9, "ram_available_mb": 112042.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10452.4, "ram_available_mb": 112053.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [28.86, 28.86], "power_watts_avg": 28.86, "power_watts_peak": 28.86, "energy_joules_est": 3.4, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:02:03.959060"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 154.613, "latencies_ms": [154.613], "images_per_second": 6.468, "prompt_tokens": 757, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA city street with a red car, white van, and green bus driving through an intersection.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.4, "ram_available_mb": 112053.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10453.2, "ram_available_mb": 112053.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 4.0}, "power_stats": {"power_watts_samples": [28.86, 29.29], "power_watts_avg": 29.07, "power_watts_peak": 29.29, "energy_joules_est": 4.51, "sample_count": 2, "duration_seconds": 0.155}, "timestamp": "2026-01-25T20:02:04.167087"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 406.723, "latencies_ms": [406.723], "images_per_second": 2.459, "prompt_tokens": 756, "response_tokens_est": 71, "n_tiles": 1, "output_text": "\n The image shows a busy city street with cars and buses. A red car is driving down the road in front of a bus that has a green sign on it. There are also other vehicles present, including a white van and another bus further back on the street. The scene takes place during daylight hours under clear skies, creating an urban atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.2, "ram_available_mb": 112053.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 4.0}, "power_stats": {"power_watts_samples": [29.29, 29.29, 29.29, 29.29, 31.54], "power_watts_avg": 29.74, "power_watts_peak": 31.54, "energy_joules_est": 12.12, "sample_count": 5, "duration_seconds": 0.407}, "timestamp": "2026-01-25T20:02:04.674392"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 255.267, "latencies_ms": [255.267], "images_per_second": 3.917, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA black Toshiba laptop sits on a table, with a phone and a small device nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 4.0}, "power_stats": {"power_watts_samples": [31.54, 31.54, 31.54], "power_watts_avg": 31.54, "power_watts_peak": 31.54, "energy_joules_est": 8.07, "sample_count": 3, "duration_seconds": 0.256}, "timestamp": "2026-01-25T20:02:04.985204"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 416.401, "latencies_ms": [416.401], "images_per_second": 2.402, "prompt_tokens": 759, "response_tokens_est": 73, "n_tiles": 1, "output_text": "\n 1. Laptop screen 2. Laptop keyboard 3. Cell phone 4. Camera 5. Microphone 6. Remote control 7. Laptop speaker 8. Laptop power button 9. Laptop mouse 10. Laptop trackpad 11. Laptop speaker 12. Laptop power button 13. Laptop speaker 14. Laptop power button 15.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [31.54, 35.87, 35.87, 35.87, 35.87], "power_watts_avg": 35.0, "power_watts_peak": 35.87, "energy_joules_est": 14.59, "sample_count": 5, "duration_seconds": 0.417}, "timestamp": "2026-01-25T20:02:05.492063"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 125.704, "latencies_ms": [125.704], "images_per_second": 7.955, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. A black laptop with a silver camera on top of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10457.0, "ram_available_mb": 112049.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [35.87, 35.06], "power_watts_avg": 35.46, "power_watts_peak": 35.87, "energy_joules_est": 4.48, "sample_count": 2, "duration_seconds": 0.126}, "timestamp": "2026-01-25T20:02:05.698588"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 254.429, "latencies_ms": [254.429], "images_per_second": 3.93, "prompt_tokens": 757, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\nIn a room, there is an open laptop on a table with a cell phone nearby. A person can be seen in the background, possibly using one of these devices or engaging in another activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.0, "ram_available_mb": 112049.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10455.2, "ram_available_mb": 112051.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [35.06, 35.06, 35.06], "power_watts_avg": 35.06, "power_watts_peak": 35.06, "energy_joules_est": 8.93, "sample_count": 3, "duration_seconds": 0.255}, "timestamp": "2026-01-25T20:02:06.005305"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 121.698, "latencies_ms": [121.698], "images_per_second": 8.217, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The laptop screen is lit up and displays a blue background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.2, "ram_available_mb": 112051.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10454.4, "ram_available_mb": 112051.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [35.06, 33.07], "power_watts_avg": 34.06, "power_watts_peak": 35.06, "energy_joules_est": 4.16, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:02:06.211134"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 334.672, "latencies_ms": [334.672], "images_per_second": 2.988, "prompt_tokens": 744, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nA computer desk with a monitor, keyboard, and mouse is situated in front of a window overlooking an outdoor view. The desk also holds several books and papers scattered across its surface.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 10454.4, "ram_available_mb": 112051.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10465.5, "ram_available_mb": 112040.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [33.07, 33.07, 33.07, 33.07], "power_watts_avg": 33.07, "power_watts_peak": 33.07, "energy_joules_est": 11.07, "sample_count": 4, "duration_seconds": 0.335}, "timestamp": "2026-01-25T20:02:06.620618"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.838, "latencies_ms": [78.838], "images_per_second": 12.684, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Computer monitor: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [35.24], "power_watts_avg": 35.24, "power_watts_peak": 35.24, "energy_joules_est": 2.79, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T20:02:06.725325"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 128.329, "latencies_ms": [128.329], "images_per_second": 7.792, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n A computer monitor is on a desk next to two laptops.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [35.24, 35.24], "power_watts_avg": 35.24, "power_watts_peak": 35.24, "energy_joules_est": 4.53, "sample_count": 2, "duration_seconds": 0.129}, "timestamp": "2026-01-25T20:02:06.931173"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 142.719, "latencies_ms": [142.719], "images_per_second": 7.007, "prompt_tokens": 757, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nA computer desk with a monitor, keyboard, mouse, and several books.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [35.24, 31.86], "power_watts_avg": 33.55, "power_watts_peak": 35.24, "energy_joules_est": 4.81, "sample_count": 2, "duration_seconds": 0.143}, "timestamp": "2026-01-25T20:02:07.135941"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 264.718, "latencies_ms": [264.718], "images_per_second": 3.778, "prompt_tokens": 756, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\n The image shows a desk with a computer monitor and keyboard. There are several books on the desk as well. A window is visible in the background, allowing natural light to enter the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10468.3, "ram_available_mb": 112038.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [31.86, 31.86, 31.86], "power_watts_avg": 31.86, "power_watts_peak": 31.86, "energy_joules_est": 8.44, "sample_count": 3, "duration_seconds": 0.265}, "timestamp": "2026-01-25T20:02:07.441842"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 245.612, "latencies_ms": [245.612], "images_per_second": 4.071, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA skateboarder is performing a trick in mid-air, with spectators watching from below.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10468.3, "ram_available_mb": 112038.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10461.8, "ram_available_mb": 112044.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [31.86, 28.92, 28.92], "power_watts_avg": 29.9, "power_watts_peak": 31.86, "energy_joules_est": 7.35, "sample_count": 3, "duration_seconds": 0.246}, "timestamp": "2026-01-25T20:02:07.755890"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.968, "latencies_ms": [82.968], "images_per_second": 12.053, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n Skateboarder: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.8, "ram_available_mb": 112044.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [28.92], "power_watts_avg": 28.92, "power_watts_peak": 28.92, "energy_joules_est": 2.41, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:02:07.860888"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 102.732, "latencies_ms": [102.732], "images_per_second": 9.734, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Skateboarder in mid air", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [28.92, 28.92], "power_watts_avg": 28.92, "power_watts_peak": 28.92, "energy_joules_est": 2.98, "sample_count": 2, "duration_seconds": 0.103}, "timestamp": "2026-01-25T20:02:08.066449"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 274.758, "latencies_ms": [274.758], "images_per_second": 3.64, "prompt_tokens": 757, "response_tokens_est": 46, "n_tiles": 1, "output_text": "\nIn a large stadium, a skateboarder in black tights and a helmet performs an impressive trick on his skateboard. A group of people are watching him from behind a railing, capturing the moment with their cameras.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10472.4, "ram_available_mb": 112033.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [33.05, 33.05, 33.05], "power_watts_avg": 33.05, "power_watts_peak": 33.05, "energy_joules_est": 9.1, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T20:02:08.372850"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 338.431, "latencies_ms": [338.431], "images_per_second": 2.955, "prompt_tokens": 756, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\n The image shows a skateboarder performing an aerial trick in front of a crowd. The skateboarder is wearing a black shirt and gray pants while executing the stunt on his skateboard. The audience appears to be watching intently from behind a railing or bleacher seats.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.4, "ram_available_mb": 112033.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [33.05, 33.05, 34.27, 34.27], "power_watts_avg": 33.66, "power_watts_peak": 34.27, "energy_joules_est": 11.4, "sample_count": 4, "duration_seconds": 0.339}, "timestamp": "2026-01-25T20:02:08.778915"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 278.87, "latencies_ms": [278.87], "images_per_second": 3.586, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA red fire hydrant with a smiley face drawn on it stands in front of a tree and sidewalk, surrounded by buildings.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10478.7, "ram_available_mb": 112027.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.27, 34.27, 34.27], "power_watts_avg": 34.27, "power_watts_peak": 34.27, "energy_joules_est": 9.57, "sample_count": 3, "duration_seconds": 0.279}, "timestamp": "2026-01-25T20:02:09.091518"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 86.167, "latencies_ms": [86.167], "images_per_second": 11.605, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Fire hydrant - 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.7, "ram_available_mb": 112027.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10476.0, "ram_available_mb": 112030.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [38.09], "power_watts_avg": 38.09, "power_watts_peak": 38.09, "energy_joules_est": 3.3, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T20:02:09.197033"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 129.306, "latencies_ms": [129.306], "images_per_second": 7.734, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. Fire hydrant with a smiley face drawn on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.0, "ram_available_mb": 112030.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10476.3, "ram_available_mb": 112030.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [38.09, 38.09], "power_watts_avg": 38.09, "power_watts_peak": 38.09, "energy_joules_est": 4.95, "sample_count": 2, "duration_seconds": 0.13}, "timestamp": "2026-01-25T20:02:09.404925"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 181.555, "latencies_ms": [181.555], "images_per_second": 5.508, "prompt_tokens": 757, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA red fire hydrant with a smiley face drawn on it stands in front of a tree, with cars parked nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.3, "ram_available_mb": 112030.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10475.7, "ram_available_mb": 112030.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [38.09, 38.09], "power_watts_avg": 38.09, "power_watts_peak": 38.09, "energy_joules_est": 6.93, "sample_count": 2, "duration_seconds": 0.182}, "timestamp": "2026-01-25T20:02:09.609495"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 111.299, "latencies_ms": [111.299], "images_per_second": 8.985, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The fire hydrant has a red body and black top.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.7, "ram_available_mb": 112030.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10473.8, "ram_available_mb": 112032.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.31, 35.31], "power_watts_avg": 35.31, "power_watts_peak": 35.31, "energy_joules_est": 3.94, "sample_count": 2, "duration_seconds": 0.111}, "timestamp": "2026-01-25T20:02:09.814562"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 327.457, "latencies_ms": [327.457], "images_per_second": 3.054, "prompt_tokens": 744, "response_tokens_est": 36, "n_tiles": 1, "output_text": "urns of luggage, including a green and blue suitcase with \"SAL\" written on it, are stacked on top of each other in front of a green door.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.8, "ram_available_mb": 112032.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10473.8, "ram_available_mb": 112032.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [35.31, 35.31, 35.31, 32.36], "power_watts_avg": 34.57, "power_watts_peak": 35.31, "energy_joules_est": 11.35, "sample_count": 4, "duration_seconds": 0.328}, "timestamp": "2026-01-25T20:02:10.226264"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 679.394, "latencies_ms": [679.394], "images_per_second": 1.472, "prompt_tokens": 759, "response_tokens_est": 128, "n_tiles": 1, "output_text": "\n 1. Green door  2. Brown suitcase 3. Blue suitcase 4. Brown suitcase 5. Brown suitcase 6. Brown suitcase 7. Brown suitcase 8. Brown suitcase 9. Brown suitcase 10. Brown suitcase 11. Brown suitcase 12. Brown suitcase 13. Brown suitcase 14. Brown suitcase 15. Brown suitcase 16. Brown suitcase 17. Brown suitcase 18. Brown suitcase 19. Brown suitcase 20. Brown suitcase 21. Brown suitcase 22. Brown suitcase 23. Brown suitcase 24. Brown suitcase 25. Brown suitcase 26. Brown suitcase 27. Brown suitcase 28. Brown suitcase 29. Brown suitcase 30. Brown suitcase 31. Brown suitcase 32.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.8, "ram_available_mb": 112032.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10477.7, "ram_available_mb": 112028.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [32.36, 32.36, 32.36, 32.36, 36.37, 36.37, 36.37], "power_watts_avg": 34.08, "power_watts_peak": 36.37, "energy_joules_est": 23.16, "sample_count": 7, "duration_seconds": 0.68}, "timestamp": "2026-01-25T20:02:10.934828"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 111.363, "latencies_ms": [111.363], "images_per_second": 8.98, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Green door with white trim and a window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.7, "ram_available_mb": 112028.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10476.0, "ram_available_mb": 112030.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [36.37, 38.19], "power_watts_avg": 37.28, "power_watts_peak": 38.19, "energy_joules_est": 4.16, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T20:02:11.140093"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 247.727, "latencies_ms": [247.727], "images_per_second": 4.037, "prompt_tokens": 757, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\nIn front of a green door, there is a cart filled with various suitcases. The suitcases are stacked on top of each other in different sizes and colors, creating an interesting visual display.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10476.0, "ram_available_mb": 112030.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10476.2, "ram_available_mb": 112030.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [38.19, 38.19, 38.19], "power_watts_avg": 38.19, "power_watts_peak": 38.19, "energy_joules_est": 9.47, "sample_count": 3, "duration_seconds": 0.248}, "timestamp": "2026-01-25T20:02:11.444100"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 100.356, "latencies_ms": [100.356], "images_per_second": 9.965, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n A green door and a white wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.2, "ram_available_mb": 112030.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 11.3, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [38.19], "power_watts_avg": 38.19, "power_watts_peak": 38.19, "energy_joules_est": 3.85, "sample_count": 1, "duration_seconds": 0.101}, "timestamp": "2026-01-25T20:02:11.548495"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 269.624, "latencies_ms": [269.624], "images_per_second": 3.709, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "urn of flowers on a couch, with a young girl in a pink dress holding a remote control and playing a video game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [34.68, 34.68, 34.68], "power_watts_avg": 34.68, "power_watts_peak": 34.68, "energy_joules_est": 9.36, "sample_count": 3, "duration_seconds": 0.27}, "timestamp": "2026-01-25T20:02:11.856087"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.777, "latencies_ms": [76.777], "images_per_second": 13.025, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Couch", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10471.2, "ram_available_mb": 112035.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [34.68], "power_watts_avg": 34.68, "power_watts_peak": 34.68, "energy_joules_est": 2.68, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T20:02:11.962254"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 136.713, "latencies_ms": [136.713], "images_per_second": 7.315, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nA young girl in a pink dress is playing with a video game controller.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10471.2, "ram_available_mb": 112035.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10472.5, "ram_available_mb": 112033.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [34.68, 33.21], "power_watts_avg": 33.95, "power_watts_peak": 34.68, "energy_joules_est": 4.65, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T20:02:12.167964"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 132.293, "latencies_ms": [132.293], "images_per_second": 7.559, "prompt_tokens": 757, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nA young girl in a pink dress stands on a couch, holding a remote control.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.5, "ram_available_mb": 112033.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10473.4, "ram_available_mb": 112032.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [33.21, 33.21], "power_watts_avg": 33.21, "power_watts_peak": 33.21, "energy_joules_est": 4.41, "sample_count": 2, "duration_seconds": 0.133}, "timestamp": "2026-01-25T20:02:12.374917"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 115.616, "latencies_ms": [115.616], "images_per_second": 8.649, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n The girl is wearing a pink dress and holding a white game controller.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.4, "ram_available_mb": 112032.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [33.21, 33.21], "power_watts_avg": 33.21, "power_watts_peak": 33.21, "energy_joules_est": 3.86, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T20:02:12.582223"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 232.484, "latencies_ms": [232.484], "images_per_second": 4.301, "prompt_tokens": 744, "response_tokens_est": 19, "n_tiles": 1, "output_text": "urn of water sits on a desk, with a laptop and keyboard in front of it.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10471.4, "ram_available_mb": 112034.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [32.84, 32.84, 32.84], "power_watts_avg": 32.84, "power_watts_peak": 32.84, "energy_joules_est": 7.65, "sample_count": 3, "duration_seconds": 0.233}, "timestamp": "2026-01-25T20:02:12.895009"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 92.025, "latencies_ms": [92.025], "images_per_second": 10.867, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Laptop: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.6, "ram_available_mb": 112034.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [32.84], "power_watts_avg": 32.84, "power_watts_peak": 32.84, "energy_joules_est": 3.06, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:02:13.001271"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 119.824, "latencies_ms": [119.824], "images_per_second": 8.346, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n A computer on a desk with headphones and speakers.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10474.4, "ram_available_mb": 112031.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [32.84, 31.02], "power_watts_avg": 31.93, "power_watts_peak": 32.84, "energy_joules_est": 3.83, "sample_count": 2, "duration_seconds": 0.12}, "timestamp": "2026-01-25T20:02:13.206092"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 181.125, "latencies_ms": [181.125], "images_per_second": 5.521, "prompt_tokens": 757, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA glass desk with a laptop on top, accompanied by a black chair. A white trash can sits next to the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.4, "ram_available_mb": 112031.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10475.9, "ram_available_mb": 112030.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [31.02, 31.02], "power_watts_avg": 31.02, "power_watts_peak": 31.02, "energy_joules_est": 5.63, "sample_count": 2, "duration_seconds": 0.181}, "timestamp": "2026-01-25T20:02:13.411860"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 97.83, "latencies_ms": [97.83], "images_per_second": 10.222, "prompt_tokens": 756, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n A computer desk with a laptop and mouse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.9, "ram_available_mb": 112030.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 10475.4, "ram_available_mb": 112030.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [31.02], "power_watts_avg": 31.02, "power_watts_peak": 31.02, "energy_joules_est": 3.05, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T20:02:13.516369"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 213.206, "latencies_ms": [213.206], "images_per_second": 4.69, "prompt_tokens": 744, "response_tokens_est": 14, "n_tiles": 1, "output_text": "iced pizza with mushrooms, peppers and sausage on a white plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.4, "ram_available_mb": 112030.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10475.4, "ram_available_mb": 112030.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [31.02, 31.12, 31.12], "power_watts_avg": 31.09, "power_watts_peak": 31.12, "energy_joules_est": 6.63, "sample_count": 3, "duration_seconds": 0.213}, "timestamp": "2026-01-25T20:02:13.829286"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 70.781, "latencies_ms": [70.781], "images_per_second": 14.128, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Pizza crust", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.4, "ram_available_mb": 112030.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10471.7, "ram_available_mb": 112034.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [31.12], "power_watts_avg": 31.12, "power_watts_peak": 31.12, "energy_joules_est": 2.22, "sample_count": 1, "duration_seconds": 0.071}, "timestamp": "2026-01-25T20:02:13.934913"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 121.828, "latencies_ms": [121.828], "images_per_second": 8.208, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nA person is eating a pizza with mushrooms and peppers on top.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.0, "ram_available_mb": 112034.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10471.0, "ram_available_mb": 112035.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [31.12, 31.63], "power_watts_avg": 31.37, "power_watts_peak": 31.63, "energy_joules_est": 3.83, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:02:14.140684"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 324.545, "latencies_ms": [324.545], "images_per_second": 3.081, "prompt_tokens": 757, "response_tokens_est": 54, "n_tiles": 1, "output_text": "\nA person is eating a pizza with mushrooms, peppers, and sausage on top of a white plate. The pizza has been cut into eight slices and is placed on a red and white checkered tablecloth. A fork is being used to eat the pizza.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.0, "ram_available_mb": 112035.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [31.63, 31.63, 31.63, 31.63], "power_watts_avg": 31.63, "power_watts_peak": 31.63, "energy_joules_est": 10.27, "sample_count": 4, "duration_seconds": 0.325}, "timestamp": "2026-01-25T20:02:14.545467"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 97.03, "latencies_ms": [97.03], "images_per_second": 10.306, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n The pizza is on a white plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10472.3, "ram_available_mb": 112034.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [32.65], "power_watts_avg": 32.65, "power_watts_peak": 32.65, "energy_joules_est": 3.18, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T20:02:14.649604"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 290.421, "latencies_ms": [290.421], "images_per_second": 3.443, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA white and red bus labeled \"Metro Transit System\" is parked on a city street, with people walking by in front of it.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10472.3, "ram_available_mb": 112034.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [32.65, 32.65, 32.65], "power_watts_avg": 32.65, "power_watts_peak": 32.65, "energy_joules_est": 9.49, "sample_count": 3, "duration_seconds": 0.291}, "timestamp": "2026-01-25T20:02:14.963454"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 108.574, "latencies_ms": [108.574], "images_per_second": 9.21, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Bus: white, red and black", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10469.8, "ram_available_mb": 112036.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [32.65, 34.88], "power_watts_avg": 33.76, "power_watts_peak": 34.88, "energy_joules_est": 3.67, "sample_count": 2, "duration_seconds": 0.109}, "timestamp": "2026-01-25T20:02:15.167185"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 140.507, "latencies_ms": [140.507], "images_per_second": 7.117, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n 1. Bus is parked on a street with trees in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.8, "ram_available_mb": 112036.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [34.88, 34.88], "power_watts_avg": 34.88, "power_watts_peak": 34.88, "energy_joules_est": 4.91, "sample_count": 2, "duration_seconds": 0.141}, "timestamp": "2026-01-25T20:02:15.373007"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 585.585, "latencies_ms": [585.585], "images_per_second": 1.708, "prompt_tokens": 757, "response_tokens_est": 106, "n_tiles": 1, "output_text": "\nA white bus with red trim is parked on a city street, displaying \"Metro Transit System\" in large letters. The bus has two doors and a windshield, indicating that it's ready to pick up passengers. A few people can be seen standing nearby, possibly waiting for the bus or just passing by.\n\nIn addition to the bus, there are also cars parked on the street, one of which is located behind the bus. The scene suggests an urban environment with public transportation available for daily commuting and travel needs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.4, "ram_available_mb": 112045.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [34.88, 34.88, 32.86, 32.86, 32.86, 32.86], "power_watts_avg": 33.53, "power_watts_peak": 34.88, "energy_joules_est": 19.64, "sample_count": 6, "duration_seconds": 0.586}, "timestamp": "2026-01-25T20:02:15.980047"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 357.667, "latencies_ms": [357.667], "images_per_second": 2.796, "prompt_tokens": 756, "response_tokens_est": 59, "n_tiles": 1, "output_text": "\nThe bus is white and red with the words \"Metro Transit System\" written on it. The front of the bus has a blue stripe down the middle. There are two people sitting inside the bus, one person in the driver's seat and another person near the back of the bus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10462.6, "ram_available_mb": 112043.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [32.86, 33.69, 33.69, 33.69], "power_watts_avg": 33.48, "power_watts_peak": 33.69, "energy_joules_est": 11.98, "sample_count": 4, "duration_seconds": 0.358}, "timestamp": "2026-01-25T20:02:16.385848"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 235.778, "latencies_ms": [235.778], "images_per_second": 4.241, "prompt_tokens": 744, "response_tokens_est": 19, "n_tiles": 1, "output_text": "urn of dirt with a baseball glove and ball resting on top, partially hidden by it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.6, "ram_available_mb": 112043.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [33.69, 33.69, 41.17], "power_watts_avg": 36.19, "power_watts_peak": 41.17, "energy_joules_est": 8.56, "sample_count": 3, "duration_seconds": 0.237}, "timestamp": "2026-01-25T20:02:16.693709"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 86.614, "latencies_ms": [86.614], "images_per_second": 11.545, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Baseball glove", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [41.17], "power_watts_avg": 41.17, "power_watts_peak": 41.17, "energy_joules_est": 3.58, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T20:02:16.798976"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 136.222, "latencies_ms": [136.222], "images_per_second": 7.341, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA baseball glove is laying on its side next to a baseball hat.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10466.8, "ram_available_mb": 112039.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [41.17, 41.17], "power_watts_avg": 41.17, "power_watts_peak": 41.17, "energy_joules_est": 5.63, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T20:02:17.004868"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 159.381, "latencies_ms": [159.381], "images_per_second": 6.274, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA baseball glove lies on its side in a dirt field, with a black pole partially visible behind it.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 10466.8, "ram_available_mb": 112039.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10465.8, "ram_available_mb": 112040.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [41.17, 34.21], "power_watts_avg": 37.69, "power_watts_peak": 41.17, "energy_joules_est": 6.05, "sample_count": 2, "duration_seconds": 0.16}, "timestamp": "2026-01-25T20:02:17.210793"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 46.918, "latencies_ms": [46.918], "images_per_second": 21.314, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.8, "ram_available_mb": 112040.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 10463.6, "ram_available_mb": 112042.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [34.21], "power_watts_avg": 34.21, "power_watts_peak": 34.21, "energy_joules_est": 1.63, "sample_count": 1, "duration_seconds": 0.048}, "timestamp": "2026-01-25T20:02:17.315878"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 276.658, "latencies_ms": [276.658], "images_per_second": 3.615, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA surfer in a red shirt and black shorts rides a wave on their surfboard, skillfully navigating the ocean's surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.6, "ram_available_mb": 112042.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10466.7, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [34.21, 34.21, 34.21], "power_watts_avg": 34.21, "power_watts_peak": 34.21, "energy_joules_est": 9.49, "sample_count": 3, "duration_seconds": 0.277}, "timestamp": "2026-01-25T20:02:17.626083"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 66.436, "latencies_ms": [66.436], "images_per_second": 15.052, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.7, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10464.8, "ram_available_mb": 112041.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [31.35], "power_watts_avg": 31.35, "power_watts_peak": 31.35, "energy_joules_est": 2.09, "sample_count": 1, "duration_seconds": 0.067}, "timestamp": "2026-01-25T20:02:17.730697"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 121.111, "latencies_ms": [121.111], "images_per_second": 8.257, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe man is surfing on a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.8, "ram_available_mb": 112041.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10464.8, "ram_available_mb": 112041.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [31.35, 31.35], "power_watts_avg": 31.35, "power_watts_peak": 31.35, "energy_joules_est": 3.81, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T20:02:17.936006"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 172.173, "latencies_ms": [172.173], "images_per_second": 5.808, "prompt_tokens": 757, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA surfer in a red shirt rides a wave on their surfboard, skillfully navigating the ocean's surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.8, "ram_available_mb": 112041.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 22.0}, "power_stats": {"power_watts_samples": [31.35, 31.39], "power_watts_avg": 31.37, "power_watts_peak": 31.39, "energy_joules_est": 5.42, "sample_count": 2, "duration_seconds": 0.173}, "timestamp": "2026-01-25T20:02:18.142743"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 327.575, "latencies_ms": [327.575], "images_per_second": 3.053, "prompt_tokens": 756, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\n The image features a surfer riding a wave on his surfboard. The surfer is wearing a red shirt and black shorts while holding onto the surfboard with both hands. The background of the photo shows an ocean scene with blue water and white foam from the waves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10469.7, "ram_available_mb": 112036.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 22.0}, "power_stats": {"power_watts_samples": [31.39, 31.39, 31.39, 31.39], "power_watts_avg": 31.39, "power_watts_peak": 31.39, "energy_joules_est": 10.29, "sample_count": 4, "duration_seconds": 0.328}, "timestamp": "2026-01-25T20:02:18.548576"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 350.919, "latencies_ms": [350.919], "images_per_second": 2.85, "prompt_tokens": 744, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\nA black and white photograph of a bathroom, featuring a toilet with its seat up and lid down, a sink with a mirror above it, and a plant on the countertop to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.7, "ram_available_mb": 112036.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 22.0}, "power_stats": {"power_watts_samples": [31.24, 31.24, 31.24, 31.24], "power_watts_avg": 31.24, "power_watts_peak": 31.24, "energy_joules_est": 10.97, "sample_count": 4, "duration_seconds": 0.351}, "timestamp": "2026-01-25T20:02:18.956009"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 83.12, "latencies_ms": [83.12], "images_per_second": 12.031, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Toilet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [31.24], "power_watts_avg": 31.24, "power_watts_peak": 31.24, "energy_joules_est": 2.62, "sample_count": 1, "duration_seconds": 0.084}, "timestamp": "2026-01-25T20:02:19.061239"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 112.376, "latencies_ms": [112.376], "images_per_second": 8.899, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Toilet bowl with seat up and lid down.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [37.06, 37.06], "power_watts_avg": 37.06, "power_watts_peak": 37.06, "energy_joules_est": 4.17, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T20:02:19.266168"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 140.423, "latencies_ms": [140.423], "images_per_second": 7.121, "prompt_tokens": 757, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nA black and white photo of a bathroom with a toilet, sink, and mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [37.06, 37.06], "power_watts_avg": 37.06, "power_watts_peak": 37.06, "energy_joules_est": 5.23, "sample_count": 2, "duration_seconds": 0.141}, "timestamp": "2026-01-25T20:02:19.472078"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 119.142, "latencies_ms": [119.142], "images_per_second": 8.393, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. The toilet is white and the sink is black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10471.8, "ram_available_mb": 112034.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [37.06, 31.83], "power_watts_avg": 34.44, "power_watts_peak": 37.06, "energy_joules_est": 4.11, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T20:02:19.678062"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 249.674, "latencies_ms": [249.674], "images_per_second": 4.005, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urns of water are on top of a building, with a clock tower in the background and trees below.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10471.8, "ram_available_mb": 112034.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10473.9, "ram_available_mb": 112032.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 3.0}, "power_stats": {"power_watts_samples": [31.83, 31.83, 31.83], "power_watts_avg": 31.83, "power_watts_peak": 31.83, "energy_joules_est": 7.96, "sample_count": 3, "duration_seconds": 0.25}, "timestamp": "2026-01-25T20:02:19.992322"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 83.777, "latencies_ms": [83.777], "images_per_second": 11.936, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Clock tower", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.9, "ram_available_mb": 112032.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [31.83], "power_watts_avg": 31.83, "power_watts_peak": 31.83, "energy_joules_est": 2.69, "sample_count": 1, "duration_seconds": 0.084}, "timestamp": "2026-01-25T20:02:20.097287"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 114.091, "latencies_ms": [114.091], "images_per_second": 8.765, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nThe clock tower is on top of a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [30.52, 30.52], "power_watts_avg": 30.52, "power_watts_peak": 30.52, "energy_joules_est": 3.49, "sample_count": 2, "duration_seconds": 0.114}, "timestamp": "2026-01-25T20:02:20.302458"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 323.894, "latencies_ms": [323.894], "images_per_second": 3.087, "prompt_tokens": 757, "response_tokens_est": 54, "n_tiles": 1, "output_text": "\nThe image shows a white clock tower with a green roof, standing on top of a building. The clock face is visible from various angles, indicating its prominence in the scene. In the background, there are trees that add to the overall atmosphere of the setting.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [30.52, 30.52, 30.52, 32.72], "power_watts_avg": 31.07, "power_watts_peak": 32.72, "energy_joules_est": 10.07, "sample_count": 4, "duration_seconds": 0.324}, "timestamp": "2026-01-25T20:02:20.710665"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 283.331, "latencies_ms": [283.331], "images_per_second": 3.529, "prompt_tokens": 756, "response_tokens_est": 46, "n_tiles": 1, "output_text": "\n The image features a white clock tower with a green roof and a blue spire. The clock is positioned on the right side of the tower. The sky above has a clear blue color, suggesting that it's daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [32.72, 32.72, 32.72], "power_watts_avg": 32.72, "power_watts_peak": 32.72, "energy_joules_est": 9.28, "sample_count": 3, "duration_seconds": 0.284}, "timestamp": "2026-01-25T20:02:21.015684"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 275.736, "latencies_ms": [275.736], "images_per_second": 3.627, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA herd of elephants is walking along a dirt path in a wooded area, with some standing and others moving forward.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [32.72, 31.68, 31.68], "power_watts_avg": 32.03, "power_watts_peak": 32.72, "energy_joules_est": 8.84, "sample_count": 3, "duration_seconds": 0.276}, "timestamp": "2026-01-25T20:02:21.329299"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 97.619, "latencies_ms": [97.619], "images_per_second": 10.244, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Elephant : 0.38", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [31.68], "power_watts_avg": 31.68, "power_watts_peak": 31.68, "energy_joules_est": 3.1, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T20:02:21.433617"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 100.008, "latencies_ms": [100.008], "images_per_second": 9.999, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Elephant in front of trees and bushes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10464.6, "ram_available_mb": 112041.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [31.68], "power_watts_avg": 31.68, "power_watts_peak": 31.68, "energy_joules_est": 3.18, "sample_count": 1, "duration_seconds": 0.1}, "timestamp": "2026-01-25T20:02:21.537336"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 152.043, "latencies_ms": [152.043], "images_per_second": 6.577, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA herd of elephants, including a large elephant in front, are walking through a wooded area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.6, "ram_available_mb": 112041.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [36.45, 36.45], "power_watts_avg": 36.45, "power_watts_peak": 36.45, "energy_joules_est": 5.55, "sample_count": 2, "duration_seconds": 0.152}, "timestamp": "2026-01-25T20:02:21.742062"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 87.8, "latencies_ms": [87.8], "images_per_second": 11.39, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The elephant is gray and black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 79.0}, "power_stats": {"power_watts_samples": [36.45], "power_watts_avg": 36.45, "power_watts_peak": 36.45, "energy_joules_est": 3.22, "sample_count": 1, "duration_seconds": 0.088}, "timestamp": "2026-01-25T20:02:21.847190"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 252.954, "latencies_ms": [252.954], "images_per_second": 3.953, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "iced tea is sitting in an open refrigerator door, with a bottle of oil on the top shelf.\n", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [36.45, 36.45, 38.01], "power_watts_avg": 36.97, "power_watts_peak": 38.01, "energy_joules_est": 9.39, "sample_count": 3, "duration_seconds": 0.254}, "timestamp": "2026-01-25T20:02:22.157703"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 687.183, "latencies_ms": [687.183], "images_per_second": 1.455, "prompt_tokens": 759, "response_tokens_est": 128, "n_tiles": 1, "output_text": "\n 1. Door handle 2. Door handle 3. Door handle 4. Door handle 5. Door handle 6. Door handle 7. Door handle 8. Door handle 9. Door handle 10. Door handle 11. Door handle 12. Door handle 13. Door handle 14. Door handle 15. Door handle 16. Door handle 17. Door handle 18. Door handle 19. Door handle 20. Door handle 21. Door handle 22. Door handle 23. Door handle 24. Door handle 25. Door handle 26. Door handle 27. Door handle 28. Door handle 29. Door handle 30. Door handle 31. Door handle 32. Door", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [38.01, 38.01, 38.01, 38.01, 37.63, 37.63, 37.63], "power_watts_avg": 37.85, "power_watts_peak": 38.01, "energy_joules_est": 26.03, "sample_count": 7, "duration_seconds": 0.688}, "timestamp": "2026-01-25T20:02:22.866279"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 103.765, "latencies_ms": [103.765], "images_per_second": 9.637, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "iced tea bottle on top shelf of refrigerator.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10465.2, "ram_available_mb": 112041.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [37.63, 37.63], "power_watts_avg": 37.63, "power_watts_peak": 37.63, "energy_joules_est": 3.91, "sample_count": 2, "duration_seconds": 0.104}, "timestamp": "2026-01-25T20:02:23.072427"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 206.426, "latencies_ms": [206.426], "images_per_second": 4.844, "prompt_tokens": 757, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nThe image shows an open white refrigerator with its door ajar, revealing several shelves inside. The refrigerator appears to be empty at the moment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.2, "ram_available_mb": 112041.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10462.0, "ram_available_mb": 112044.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [37.21, 37.21, 37.21], "power_watts_avg": 37.21, "power_watts_peak": 37.21, "energy_joules_est": 7.7, "sample_count": 3, "duration_seconds": 0.207}, "timestamp": "2026-01-25T20:02:23.379185"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 338.718, "latencies_ms": [338.718], "images_per_second": 2.952, "prompt_tokens": 756, "response_tokens_est": 55, "n_tiles": 1, "output_text": "\n The image shows an open refrigerator with a white interior and brown floor. The top shelf contains several cartons of eggs, while the bottom shelf has a bottle on it. The light inside the fridge is turned off, creating a dimly lit atmosphere in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.9, "ram_available_mb": 112044.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10458.0, "ram_available_mb": 112048.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [37.21, 37.21, 32.09, 32.09], "power_watts_avg": 34.65, "power_watts_peak": 37.21, "energy_joules_est": 11.76, "sample_count": 4, "duration_seconds": 0.339}, "timestamp": "2026-01-25T20:02:23.787118"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 273.549, "latencies_ms": [273.549], "images_per_second": 3.656, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA bunch of bananas with stickers on them is displayed in a store, with some appearing to be ripe and others still green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.0, "ram_available_mb": 112048.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.09, 32.09, 32.09], "power_watts_avg": 32.09, "power_watts_peak": 32.09, "energy_joules_est": 8.79, "sample_count": 3, "duration_seconds": 0.274}, "timestamp": "2026-01-25T20:02:24.100034"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 91.833, "latencies_ms": [91.833], "images_per_second": 10.889, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Banana 0.46", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10457.8, "ram_available_mb": 112048.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.08], "power_watts_avg": 35.08, "power_watts_peak": 35.08, "energy_joules_est": 3.23, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T20:02:24.204518"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 165.649, "latencies_ms": [165.649], "images_per_second": 6.037, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n 1. A bunch of bananas on a table with other bananas in front and behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.8, "ram_available_mb": 112048.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10454.1, "ram_available_mb": 112052.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.08, 35.08], "power_watts_avg": 35.08, "power_watts_peak": 35.08, "energy_joules_est": 5.81, "sample_count": 2, "duration_seconds": 0.166}, "timestamp": "2026-01-25T20:02:24.408271"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 151.932, "latencies_ms": [151.932], "images_per_second": 6.582, "prompt_tokens": 757, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA display of ripe bananas with stickers on them, arranged in a neat row.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.1, "ram_available_mb": 112052.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.08, 35.08], "power_watts_avg": 35.08, "power_watts_peak": 35.08, "energy_joules_est": 5.35, "sample_count": 2, "duration_seconds": 0.153}, "timestamp": "2026-01-25T20:02:24.615101"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 126.147, "latencies_ms": [126.147], "images_per_second": 7.927, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A purple and blue background with a bunch of bananas on it.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10447.9, "ram_available_mb": 112058.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.42, 35.42], "power_watts_avg": 35.42, "power_watts_peak": 35.42, "energy_joules_est": 4.48, "sample_count": 2, "duration_seconds": 0.126}, "timestamp": "2026-01-25T20:02:24.821278"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 304.833, "latencies_ms": [304.833], "images_per_second": 3.28, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "urns of fire hydrant are lined up on a sidewalk in front of a building, with trees and people walking by in the background.\n", "error": null, "sys_before": {"cpu_percent": 3.3, "ram_used_mb": 10447.9, "ram_available_mb": 112058.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10439.9, "ram_available_mb": 112066.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [35.42, 35.42, 35.42, 31.24], "power_watts_avg": 34.37, "power_watts_peak": 35.42, "energy_joules_est": 10.49, "sample_count": 4, "duration_seconds": 0.305}, "timestamp": "2026-01-25T20:02:25.238981"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 93.92, "latencies_ms": [93.92], "images_per_second": 10.647, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Fire hydrant 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10439.9, "ram_available_mb": 112066.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10440.1, "ram_available_mb": 112066.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [31.24], "power_watts_avg": 31.24, "power_watts_peak": 31.24, "energy_joules_est": 2.95, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T20:02:25.348260"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 109.678, "latencies_ms": [109.678], "images_per_second": 9.118, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urns on left side of photo.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10440.1, "ram_available_mb": 112066.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10441.1, "ram_available_mb": 112065.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [31.24, 31.24], "power_watts_avg": 31.24, "power_watts_peak": 31.24, "energy_joules_est": 3.43, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T20:02:25.552443"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 176.57, "latencies_ms": [176.57], "images_per_second": 5.663, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urns of fire hydrant are lined up on a sidewalk in front of a building, with trees nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10441.1, "ram_available_mb": 112065.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10432.4, "ram_available_mb": 112073.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [30.15, 30.15], "power_watts_avg": 30.15, "power_watts_peak": 30.15, "energy_joules_est": 5.34, "sample_count": 2, "duration_seconds": 0.177}, "timestamp": "2026-01-25T20:02:25.757586"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 83.432, "latencies_ms": [83.432], "images_per_second": 11.986, "prompt_tokens": 756, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urns on the sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10432.4, "ram_available_mb": 112073.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10445.5, "ram_available_mb": 112060.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [30.15], "power_watts_avg": 30.15, "power_watts_peak": 30.15, "energy_joules_est": 2.53, "sample_count": 1, "duration_seconds": 0.084}, "timestamp": "2026-01-25T20:02:25.864909"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 300.133, "latencies_ms": [300.133], "images_per_second": 3.332, "prompt_tokens": 744, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA person wearing a helmet is riding in a horse-drawn carriage, with the number 8 on their harness and a banner for Magnum hanging nearby.", "error": null, "sys_before": {"cpu_percent": 3.3, "ram_used_mb": 10445.5, "ram_available_mb": 112060.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [30.15, 30.15, 30.43], "power_watts_avg": 30.24, "power_watts_peak": 30.43, "energy_joules_est": 9.1, "sample_count": 3, "duration_seconds": 0.301}, "timestamp": "2026-01-25T20:02:26.180858"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 89.07, "latencies_ms": [89.07], "images_per_second": 11.227, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Horse: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [30.43], "power_watts_avg": 30.43, "power_watts_peak": 30.43, "energy_joules_est": 2.72, "sample_count": 1, "duration_seconds": 0.09}, "timestamp": "2026-01-25T20:02:26.284910"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 100.633, "latencies_ms": [100.633], "images_per_second": 9.937, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nThe horse is running on a dirt track.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10454.5, "ram_available_mb": 112051.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [30.43, 30.43], "power_watts_avg": 30.43, "power_watts_peak": 30.43, "energy_joules_est": 3.08, "sample_count": 2, "duration_seconds": 0.101}, "timestamp": "2026-01-25T20:02:26.487101"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 157.125, "latencies_ms": [157.125], "images_per_second": 6.364, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA person in a yellow shirt rides a horse pulling a red sled, racing on a dirt track.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.5, "ram_available_mb": 112051.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10456.2, "ram_available_mb": 112050.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [30.43, 31.69], "power_watts_avg": 31.06, "power_watts_peak": 31.69, "energy_joules_est": 4.89, "sample_count": 2, "duration_seconds": 0.158}, "timestamp": "2026-01-25T20:02:26.692636"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 299.769, "latencies_ms": [299.769], "images_per_second": 3.336, "prompt_tokens": 756, "response_tokens_est": 49, "n_tiles": 1, "output_text": "\n The image shows a person riding in a horse-drawn carriage on a dirt road. The rider is wearing a white helmet and a yellow jacket with the number 8 on it. A banner can be seen hanging from a pole behind them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.2, "ram_available_mb": 112050.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10463.5, "ram_available_mb": 112042.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [31.69, 31.69, 31.69], "power_watts_avg": 31.69, "power_watts_peak": 31.69, "energy_joules_est": 9.51, "sample_count": 3, "duration_seconds": 0.3}, "timestamp": "2026-01-25T20:02:26.997578"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 287.967, "latencies_ms": [287.967], "images_per_second": 3.473, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "urn of water sits on a concrete slab, with a dog standing next to it in front of an orange tree and a wooden fence.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.5, "ram_available_mb": 112042.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10463.0, "ram_available_mb": 112043.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 78.0}, "power_stats": {"power_watts_samples": [31.69, 32.14, 32.14], "power_watts_avg": 31.99, "power_watts_peak": 32.14, "energy_joules_est": 9.22, "sample_count": 3, "duration_seconds": 0.288}, "timestamp": "2026-01-25T20:02:27.306546"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 214.627, "latencies_ms": [214.627], "images_per_second": 4.659, "prompt_tokens": 759, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\n 1. Tree trunk  2. Bush  3. Bush  4. Bush  5. Bush  6. Bush  7. Bush  8. Bush", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.0, "ram_available_mb": 112043.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10462.7, "ram_available_mb": 112043.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 78.0}, "power_stats": {"power_watts_samples": [32.14, 32.14, 32.14], "power_watts_avg": 32.14, "power_watts_peak": 32.14, "energy_joules_est": 6.91, "sample_count": 3, "duration_seconds": 0.215}, "timestamp": "2026-01-25T20:02:27.612674"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 123.784, "latencies_ms": [123.784], "images_per_second": 8.079, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA brown dog standing on a deck in front of an orange tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.7, "ram_available_mb": 112043.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10465.2, "ram_available_mb": 112041.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 78.0}, "power_stats": {"power_watts_samples": [39.28, 39.28], "power_watts_avg": 39.28, "power_watts_peak": 39.28, "energy_joules_est": 4.89, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T20:02:27.817163"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 236.734, "latencies_ms": [236.734], "images_per_second": 4.224, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA brown dog stands on a concrete block in front of an orange tree, with its tail wagging. The dog appears to be enjoying itself as it looks at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.2, "ram_available_mb": 112041.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10460.9, "ram_available_mb": 112045.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [39.28, 39.28, 39.28], "power_watts_avg": 39.28, "power_watts_peak": 39.28, "energy_joules_est": 9.33, "sample_count": 3, "duration_seconds": 0.238}, "timestamp": "2026-01-25T20:02:28.121049"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 92.051, "latencies_ms": [92.051], "images_per_second": 10.864, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The dog is brown and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.9, "ram_available_mb": 112045.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.35], "power_watts_avg": 36.35, "power_watts_peak": 36.35, "energy_joules_est": 3.37, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:02:28.225712"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 320.148, "latencies_ms": [320.148], "images_per_second": 3.124, "prompt_tokens": 744, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nA person is standing on a wooden bench, wearing red pants and blue shoes. The bench has a sign attached to it that says \"I want to be free\".", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.35, 36.35, 36.35, 36.35], "power_watts_avg": 36.35, "power_watts_peak": 36.35, "energy_joules_est": 11.64, "sample_count": 4, "duration_seconds": 0.32}, "timestamp": "2026-01-25T20:02:28.637064"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 72.494, "latencies_ms": [72.494], "images_per_second": 13.794, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Bench 1: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10466.8, "ram_available_mb": 112039.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.46], "power_watts_avg": 33.46, "power_watts_peak": 33.46, "energy_joules_est": 2.44, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T20:02:28.741214"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 155.906, "latencies_ms": [155.906], "images_per_second": 6.414, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nA person in red pants stands on a wooden bench with their feet on top of it.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10466.8, "ram_available_mb": 112039.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.46, 33.46], "power_watts_avg": 33.46, "power_watts_peak": 33.46, "energy_joules_est": 5.23, "sample_count": 2, "duration_seconds": 0.156}, "timestamp": "2026-01-25T20:02:28.946442"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 265.436, "latencies_ms": [265.436], "images_per_second": 3.767, "prompt_tokens": 757, "response_tokens_est": 42, "n_tiles": 1, "output_text": "\nA person in red pants stands on a wooden bench, with their feet resting on the back of the bench. The bench has a blue sign attached to it that says \"I want to be free\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 13.0}, "power_stats": {"power_watts_samples": [33.46, 31.87, 31.87], "power_watts_avg": 32.4, "power_watts_peak": 33.46, "energy_joules_est": 8.61, "sample_count": 3, "duration_seconds": 0.266}, "timestamp": "2026-01-25T20:02:29.250710"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 134.462, "latencies_ms": [134.462], "images_per_second": 7.437, "prompt_tokens": 756, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n The person is standing on a wooden bench with red pants and blue shoes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10462.4, "ram_available_mb": 112043.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 13.0}, "power_stats": {"power_watts_samples": [31.87, 31.87], "power_watts_avg": 31.87, "power_watts_peak": 31.87, "energy_joules_est": 4.31, "sample_count": 2, "duration_seconds": 0.135}, "timestamp": "2026-01-25T20:02:29.456289"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 248.103, "latencies_ms": [248.103], "images_per_second": 4.031, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urns of flowers sit on a table in front of a window, with two chairs and a lamp nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.4, "ram_available_mb": 112043.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 13.0}, "power_stats": {"power_watts_samples": [31.87, 29.55, 29.55], "power_watts_avg": 30.32, "power_watts_peak": 31.87, "energy_joules_est": 7.54, "sample_count": 3, "duration_seconds": 0.249}, "timestamp": "2026-01-25T20:02:29.767109"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 73.452, "latencies_ms": [73.452], "images_per_second": 13.614, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Couch 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 13.0}, "power_stats": {"power_watts_samples": [29.55], "power_watts_avg": 29.55, "power_watts_peak": 29.55, "energy_joules_est": 2.18, "sample_count": 1, "duration_seconds": 0.074}, "timestamp": "2026-01-25T20:02:29.872248"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 98.825, "latencies_ms": [98.825], "images_per_second": 10.119, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "urns on a table in front of window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 13.0}, "power_stats": {"power_watts_samples": [29.55], "power_watts_avg": 29.55, "power_watts_peak": 29.55, "energy_joules_est": 2.93, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T20:02:29.976826"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 336.165, "latencies_ms": [336.165], "images_per_second": 2.975, "prompt_tokens": 757, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\nThe image shows a cozy living room with a red couch, a round table set for two, and a flat-screen TV. The room has beige walls and a window with curtains that allow natural light to enter. A lamp can also be seen in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10471.0, "ram_available_mb": 112035.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [29.55, 33.91, 33.91, 33.91], "power_watts_avg": 32.82, "power_watts_peak": 33.91, "energy_joules_est": 11.04, "sample_count": 4, "duration_seconds": 0.336}, "timestamp": "2026-01-25T20:02:30.381954"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 110.9, "latencies_ms": [110.9], "images_per_second": 9.017, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "urns of flowers on the table and a vase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.0, "ram_available_mb": 112035.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [33.91, 33.91], "power_watts_avg": 33.91, "power_watts_peak": 33.91, "energy_joules_est": 3.77, "sample_count": 2, "duration_seconds": 0.111}, "timestamp": "2026-01-25T20:02:30.587076"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 460.946, "latencies_ms": [460.946], "images_per_second": 2.169, "prompt_tokens": 744, "response_tokens_est": 63, "n_tiles": 1, "output_text": "\nA black frying pan sits on a stovetop, filled with a colorful mix of vegetables including broccoli and carrots. The vegetables are cooked to perfection, their vibrant colors contrasting against the dark surface of the pan. A spatula rests in the pan, ready for use when it's time to serve the dish.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10472.0, "ram_available_mb": 112034.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.09, 35.09, 35.09, 35.09, 35.09], "power_watts_avg": 35.09, "power_watts_peak": 35.09, "energy_joules_est": 16.2, "sample_count": 5, "duration_seconds": 0.462}, "timestamp": "2026-01-25T20:02:31.097467"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.278, "latencies_ms": [76.278], "images_per_second": 13.11, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Broccoli", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.0, "ram_available_mb": 112034.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [38.41], "power_watts_avg": 38.41, "power_watts_peak": 38.41, "energy_joules_est": 2.96, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T20:02:31.203412"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 117.114, "latencies_ms": [117.114], "images_per_second": 8.539, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nA black frying pan with a piece of broccoli in it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10474.0, "ram_available_mb": 112032.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [38.41, 38.41], "power_watts_avg": 38.41, "power_watts_peak": 38.41, "energy_joules_est": 4.51, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:02:31.408963"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 299.212, "latencies_ms": [299.212], "images_per_second": 3.342, "prompt_tokens": 757, "response_tokens_est": 48, "n_tiles": 1, "output_text": "\nA black frying pan sits on a stovetop, filled with a colorful assortment of vegetables including broccoli, carrots, peppers, and sausage. A spatula rests in the pan, ready to be used for cooking or serving the dish.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.0, "ram_available_mb": 112032.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [38.41, 38.41, 33.96], "power_watts_avg": 36.93, "power_watts_peak": 38.41, "energy_joules_est": 11.07, "sample_count": 3, "duration_seconds": 0.3}, "timestamp": "2026-01-25T20:02:31.715667"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 57.141, "latencies_ms": [57.141], "images_per_second": 17.501, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10474.0, "ram_available_mb": 112032.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.96], "power_watts_avg": 33.96, "power_watts_peak": 33.96, "energy_joules_est": 1.95, "sample_count": 1, "duration_seconds": 0.057}, "timestamp": "2026-01-25T20:02:31.821006"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 243.154, "latencies_ms": [243.154], "images_per_second": 4.113, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "iced hot dogs with mustard and ketchup on a black plate, arranged in two rows of three.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10474.0, "ram_available_mb": 112032.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [33.96, 33.96, 33.96], "power_watts_avg": 33.96, "power_watts_peak": 33.96, "energy_joules_est": 8.28, "sample_count": 3, "duration_seconds": 0.244}, "timestamp": "2026-01-25T20:02:32.132464"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 125.067, "latencies_ms": [125.067], "images_per_second": 7.996, "prompt_tokens": 759, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Hot dog with mustard and ketchup - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10468.4, "ram_available_mb": 112037.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [32.66, 32.66], "power_watts_avg": 32.66, "power_watts_peak": 32.66, "energy_joules_est": 4.09, "sample_count": 2, "duration_seconds": 0.125}, "timestamp": "2026-01-25T20:02:32.339421"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 106.093, "latencies_ms": [106.093], "images_per_second": 9.426, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Hot dog with mustard on a plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.4, "ram_available_mb": 112037.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10468.4, "ram_available_mb": 112037.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [32.66, 32.66], "power_watts_avg": 32.66, "power_watts_peak": 32.66, "energy_joules_est": 3.48, "sample_count": 2, "duration_seconds": 0.107}, "timestamp": "2026-01-25T20:02:32.544374"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 248.762, "latencies_ms": [248.762], "images_per_second": 4.02, "prompt_tokens": 757, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\nThree hot dogs are sitting on a black plate, each covered in mustard. The plate is placed on a table or countertop with various items around it, including a magazine and a book.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10465.5, "ram_available_mb": 112040.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [32.16, 32.16, 32.16], "power_watts_avg": 32.16, "power_watts_peak": 32.16, "energy_joules_est": 8.01, "sample_count": 3, "duration_seconds": 0.249}, "timestamp": "2026-01-25T20:02:32.851017"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 420.329, "latencies_ms": [420.329], "images_per_second": 2.379, "prompt_tokens": 756, "response_tokens_est": 72, "n_tiles": 1, "output_text": "\n The image shows a plate with three hot dogs on it. Each hot dog has mustard drizzled over the top bun and is topped with ketchup. The background of the image features a black countertop that contrasts with the warm tones of the food items. A magazine can be seen in the background, adding to the casual atmosphere of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.5, "ram_available_mb": 112040.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [32.16, 32.16, 28.86, 28.86, 28.86], "power_watts_avg": 30.18, "power_watts_peak": 32.16, "energy_joules_est": 12.7, "sample_count": 5, "duration_seconds": 0.421}, "timestamp": "2026-01-25T20:02:33.358035"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 236.066, "latencies_ms": [236.066], "images_per_second": 4.236, "prompt_tokens": 744, "response_tokens_est": 20, "n_tiles": 1, "output_text": "urns of water are calm, with people swimming in them and others standing on the shore.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [28.86, 28.86, 37.06], "power_watts_avg": 31.59, "power_watts_peak": 37.06, "energy_joules_est": 7.47, "sample_count": 3, "duration_seconds": 0.236}, "timestamp": "2026-01-25T20:02:33.673064"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 253.834, "latencies_ms": [253.834], "images_per_second": 3.94, "prompt_tokens": 759, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\n 1. Green umbrella 2. Blue chair 3. Pink chair 4. Brown chair 5. Person in water 6. Person standing in water 7. Person swimming in water 8. Person sitting in water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10462.1, "ram_available_mb": 112044.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [37.06, 37.06, 37.06], "power_watts_avg": 37.06, "power_watts_peak": 37.06, "energy_joules_est": 9.43, "sample_count": 3, "duration_seconds": 0.254}, "timestamp": "2026-01-25T20:02:33.979199"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 116.738, "latencies_ms": [116.738], "images_per_second": 8.566, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe green umbrella is located in front of two beach chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.1, "ram_available_mb": 112044.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 19.0}, "power_stats": {"power_watts_samples": [37.06, 36.34], "power_watts_avg": 36.7, "power_watts_peak": 37.06, "energy_joules_est": 4.29, "sample_count": 2, "duration_seconds": 0.117}, "timestamp": "2026-01-25T20:02:34.184065"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 309.391, "latencies_ms": [309.391], "images_per_second": 3.232, "prompt_tokens": 757, "response_tokens_est": 52, "n_tiles": 1, "output_text": "\nA group of people are swimming in a body of water, with one person holding an umbrella. There are two beach chairs nearby, one blue and one pink, suggesting that this might be a relaxing spot for beachgoers to enjoy the sun and surf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 19.0}, "power_stats": {"power_watts_samples": [36.34, 36.34, 36.34, 36.34], "power_watts_avg": 36.34, "power_watts_peak": 36.34, "energy_joules_est": 11.25, "sample_count": 4, "duration_seconds": 0.31}, "timestamp": "2026-01-25T20:02:34.591028"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 299.798, "latencies_ms": [299.798], "images_per_second": 3.336, "prompt_tokens": 756, "response_tokens_est": 50, "n_tiles": 1, "output_text": "\n A green umbrella is open over a pair of beach chairs. The umbrellas are positioned on the right side and appear to be made from fabric or plastic material. There are two people swimming in the water near the center of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 19.0}, "power_stats": {"power_watts_samples": [32.58, 32.58, 32.58], "power_watts_avg": 32.58, "power_watts_peak": 32.58, "energy_joules_est": 9.78, "sample_count": 3, "duration_seconds": 0.3}, "timestamp": "2026-01-25T20:02:34.898163"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 268.263, "latencies_ms": [268.263], "images_per_second": 3.728, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "urns, bowls and a sink are arranged on a table in an old-fashioned kitchen with green walls and white appliances.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10470.5, "ram_available_mb": 112035.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10472.0, "ram_available_mb": 112034.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [32.58, 32.58, 35.88], "power_watts_avg": 33.68, "power_watts_peak": 35.88, "energy_joules_est": 9.04, "sample_count": 3, "duration_seconds": 0.269}, "timestamp": "2026-01-25T20:02:35.213127"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 267.928, "latencies_ms": [267.928], "images_per_second": 3.732, "prompt_tokens": 759, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\n 1. Refrigerator  2. Drawer 3. Sink 4. Table 5. Chair 6. Counter 7. Oven 8. Cupboards 9. Wallpaper 10. Towels", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.0, "ram_available_mb": 112034.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [35.88, 35.88, 35.88], "power_watts_avg": 35.88, "power_watts_peak": 35.88, "energy_joules_est": 9.64, "sample_count": 3, "duration_seconds": 0.269}, "timestamp": "2026-01-25T20:02:35.520479"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 129.942, "latencies_ms": [129.942], "images_per_second": 7.696, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A white refrigerator is next to a table with a sink and chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [35.88, 37.17], "power_watts_avg": 36.53, "power_watts_peak": 37.17, "energy_joules_est": 4.77, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T20:02:35.727019"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 408.957, "latencies_ms": [408.957], "images_per_second": 2.445, "prompt_tokens": 757, "response_tokens_est": 69, "n_tiles": 1, "output_text": "\nThe image shows a vintage kitchen with green wallpaper, featuring an old-fashioned sink, refrigerator, and table. The room has a cozy atmosphere, with various items such as bowls, cups, and a chair placed around the space. A calendar hangs on the wall above the sink, adding to the nostalgic ambiance of the scene.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10472.4, "ram_available_mb": 112033.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.17, 37.17, 37.17, 37.17, 34.61], "power_watts_avg": 36.66, "power_watts_peak": 37.17, "energy_joules_est": 15.01, "sample_count": 5, "duration_seconds": 0.41}, "timestamp": "2026-01-25T20:02:36.233354"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 98.074, "latencies_ms": [98.074], "images_per_second": 10.196, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n The room is painted green and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.4, "ram_available_mb": 112033.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10473.3, "ram_available_mb": 112033.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.61], "power_watts_avg": 34.61, "power_watts_peak": 34.61, "energy_joules_est": 3.42, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T20:02:36.338761"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 153.033, "latencies_ms": [153.033], "images_per_second": 6.535, "prompt_tokens": 744, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 10473.3, "ram_available_mb": 112033.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.61, 34.61], "power_watts_avg": 34.61, "power_watts_peak": 34.61, "energy_joules_est": 5.3, "sample_count": 2, "duration_seconds": 0.153}, "timestamp": "2026-01-25T20:02:36.552561"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 75.357, "latencies_ms": [75.357], "images_per_second": 13.27, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Tree trunk", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.7], "power_watts_avg": 33.7, "power_watts_peak": 33.7, "energy_joules_est": 2.55, "sample_count": 1, "duration_seconds": 0.076}, "timestamp": "2026-01-25T20:02:36.658006"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 154.332, "latencies_ms": [154.332], "images_per_second": 6.48, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nA black and white dog is playing with a frisbee in front of a tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10464.3, "ram_available_mb": 112042.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.7, 33.7], "power_watts_avg": 33.7, "power_watts_peak": 33.7, "energy_joules_est": 5.21, "sample_count": 2, "duration_seconds": 0.155}, "timestamp": "2026-01-25T20:02:36.864595"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 282.522, "latencies_ms": [282.522], "images_per_second": 3.54, "prompt_tokens": 757, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nA black and white dog with a blue collar is playing in a park, running towards a tree trunk. The dog appears to be focused on something under the tree, possibly a frisbee or another toy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [33.7, 33.7, 31.57], "power_watts_avg": 32.99, "power_watts_peak": 33.7, "energy_joules_est": 9.34, "sample_count": 3, "duration_seconds": 0.283}, "timestamp": "2026-01-25T20:02:37.171742"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 97.018, "latencies_ms": [97.018], "images_per_second": 10.307, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The dog is black and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [31.57], "power_watts_avg": 31.57, "power_watts_peak": 31.57, "energy_joules_est": 3.07, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T20:02:37.276765"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 318.235, "latencies_ms": [318.235], "images_per_second": 3.142, "prompt_tokens": 744, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA skier in a black jacket and white helmet is skiing down a snowy mountain trail, with their green ski poles visible as they navigate through the snow.", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [31.57, 31.57, 31.57, 34.4], "power_watts_avg": 32.28, "power_watts_peak": 34.4, "energy_joules_est": 10.28, "sample_count": 4, "duration_seconds": 0.318}, "timestamp": "2026-01-25T20:02:37.690743"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 87.059, "latencies_ms": [87.059], "images_per_second": 11.486, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1 Skier: 0", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10465.0, "ram_available_mb": 112041.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [34.4], "power_watts_avg": 34.4, "power_watts_peak": 34.4, "energy_joules_est": 3.0, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T20:02:37.795852"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 126.452, "latencies_ms": [126.452], "images_per_second": 7.908, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe skier is in front of a snow covered house.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.0, "ram_available_mb": 112041.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [34.4, 34.4], "power_watts_avg": 34.4, "power_watts_peak": 34.4, "energy_joules_est": 4.37, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:02:38.001146"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 131.494, "latencies_ms": [131.494], "images_per_second": 7.605, "prompt_tokens": 757, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA skier wearing a backpack is skiing down a snowy mountain trail.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [34.4, 32.92], "power_watts_avg": 33.66, "power_watts_peak": 34.4, "energy_joules_est": 4.45, "sample_count": 2, "duration_seconds": 0.132}, "timestamp": "2026-01-25T20:02:38.208325"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 107.221, "latencies_ms": [107.221], "images_per_second": 9.327, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The skier is wearing a black jacket and white hat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10462.4, "ram_available_mb": 112043.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [32.92, 32.92], "power_watts_avg": 32.92, "power_watts_peak": 32.92, "energy_joules_est": 3.54, "sample_count": 2, "duration_seconds": 0.108}, "timestamp": "2026-01-25T20:02:38.414472"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 305.801, "latencies_ms": [305.801], "images_per_second": 3.27, "prompt_tokens": 744, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA BNSF train engine, numbered 6096, is pulling a long line of freight cars through a rural landscape with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.4, "ram_available_mb": 112043.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10462.8, "ram_available_mb": 112043.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [32.92, 32.92, 27.61, 27.61], "power_watts_avg": 30.26, "power_watts_peak": 32.92, "energy_joules_est": 9.29, "sample_count": 4, "duration_seconds": 0.307}, "timestamp": "2026-01-25T20:02:38.826348"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 384.887, "latencies_ms": [384.887], "images_per_second": 2.598, "prompt_tokens": 759, "response_tokens_est": 67, "n_tiles": 1, "output_text": "\n 1. Train engine 6096 orange and black  2. Train engine 6096 orange and black  3. Train engine 6096 orange and black  4. Train engine 6096 orange and black  5. Train engine 6096 orange and black  6. Train engine 6096 orange and black", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.8, "ram_available_mb": 112043.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10462.8, "ram_available_mb": 112043.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [27.61, 27.61, 27.61, 33.2], "power_watts_avg": 29.01, "power_watts_peak": 33.2, "energy_joules_est": 11.17, "sample_count": 4, "duration_seconds": 0.385}, "timestamp": "2026-01-25T20:02:39.232915"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 111.289, "latencies_ms": [111.289], "images_per_second": 8.986, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe orange train is in front of a tree line.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.8, "ram_available_mb": 112043.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.2, 33.2], "power_watts_avg": 33.2, "power_watts_peak": 33.2, "energy_joules_est": 3.7, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T20:02:39.439216"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 249.649, "latencies_ms": [249.649], "images_per_second": 4.006, "prompt_tokens": 757, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\nA BNSF train engine, numbered 6096, is pulling a long line of freight cars through a rural area. The train is moving along the tracks with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.2, 33.2, 34.09], "power_watts_avg": 33.5, "power_watts_peak": 34.09, "energy_joules_est": 8.38, "sample_count": 3, "duration_seconds": 0.25}, "timestamp": "2026-01-25T20:02:39.744763"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 85.925, "latencies_ms": [85.925], "images_per_second": 11.638, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The train is orange and black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.09], "power_watts_avg": 34.09, "power_watts_peak": 34.09, "energy_joules_est": 2.94, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T20:02:39.849983"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 367.71, "latencies_ms": [367.71], "images_per_second": 2.72, "prompt_tokens": 744, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nThe image features a plate of food on a wooden table, including two pieces of toast and a bowl filled with broccoli. The plate is placed on a white plate that contrasts with the brown wood surface beneath it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [34.09, 34.09, 35.37, 35.37], "power_watts_avg": 34.73, "power_watts_peak": 35.37, "energy_joules_est": 12.78, "sample_count": 4, "duration_seconds": 0.368}, "timestamp": "2026-01-25T20:02:40.261503"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 112.796, "latencies_ms": [112.796], "images_per_second": 8.866, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Broccoli and avocado toast - 2", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [35.37, 35.37], "power_watts_avg": 35.37, "power_watts_peak": 35.37, "energy_joules_est": 4.02, "sample_count": 2, "duration_seconds": 0.114}, "timestamp": "2026-01-25T20:02:40.467322"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 154.628, "latencies_ms": [154.628], "images_per_second": 6.467, "prompt_tokens": 763, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\n 1. A bowl of broccoli on a plate next to two pieces of bread with white cream cheese spread.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10468.2, "ram_available_mb": 112038.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [35.37, 34.99], "power_watts_avg": 35.18, "power_watts_peak": 35.37, "energy_joules_est": 5.47, "sample_count": 2, "duration_seconds": 0.155}, "timestamp": "2026-01-25T20:02:40.674075"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 163.8, "latencies_ms": [163.8], "images_per_second": 6.105, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n A plate of food with a bowl of broccoli, a sandwich on bread, and some white sauce.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.2, "ram_available_mb": 112038.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10464.3, "ram_available_mb": 112042.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [34.99, 34.99], "power_watts_avg": 34.99, "power_watts_peak": 34.99, "energy_joules_est": 5.74, "sample_count": 2, "duration_seconds": 0.164}, "timestamp": "2026-01-25T20:02:40.879734"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 48.914, "latencies_ms": [48.914], "images_per_second": 20.444, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.3, "ram_available_mb": 112042.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10465.8, "ram_available_mb": 112040.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [34.99], "power_watts_avg": 34.99, "power_watts_peak": 34.99, "energy_joules_est": 1.72, "sample_count": 1, "duration_seconds": 0.049}, "timestamp": "2026-01-25T20:02:40.985331"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 290.466, "latencies_ms": [290.466], "images_per_second": 3.443, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA person is sleeping on a wooden bench in an outdoor setting, with their head resting on a blue blanket and an orange sleeping bag.", "error": null, "sys_before": {"cpu_percent": 8.0, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [34.99, 28.66, 28.66], "power_watts_avg": 30.77, "power_watts_peak": 34.99, "energy_joules_est": 8.95, "sample_count": 3, "duration_seconds": 0.291}, "timestamp": "2026-01-25T20:02:41.298204"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 87.313, "latencies_ms": [87.313], "images_per_second": 11.453, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bench - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10469.3, "ram_available_mb": 112037.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [28.66], "power_watts_avg": 28.66, "power_watts_peak": 28.66, "energy_joules_est": 2.51, "sample_count": 1, "duration_seconds": 0.088}, "timestamp": "2026-01-25T20:02:41.403109"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 120.508, "latencies_ms": [120.508], "images_per_second": 8.298, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA person is sleeping on a bench in front of a parking meter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.3, "ram_available_mb": 112037.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10469.3, "ram_available_mb": 112037.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [28.66, 28.66], "power_watts_avg": 28.66, "power_watts_peak": 28.66, "energy_joules_est": 3.47, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T20:02:41.609341"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 250.409, "latencies_ms": [250.409], "images_per_second": 3.993, "prompt_tokens": 757, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nA person is sleeping on a bench in a park, with an orange blanket covering them. The bench has a blue bag next to it, and there are two parking meters nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.3, "ram_available_mb": 112037.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [33.9, 33.9, 33.9], "power_watts_avg": 33.9, "power_watts_peak": 33.9, "energy_joules_est": 8.51, "sample_count": 3, "duration_seconds": 0.251}, "timestamp": "2026-01-25T20:02:41.918071"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 285.852, "latencies_ms": [285.852], "images_per_second": 3.498, "prompt_tokens": 756, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\n A person is sleeping on a bench in the park. The person has an orange blanket and a blue bag next to them. There are two parking meters nearby with one of them showing that it's half full.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10476.5, "ram_available_mb": 112029.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [33.9, 33.9, 34.64], "power_watts_avg": 34.14, "power_watts_peak": 34.64, "energy_joules_est": 9.78, "sample_count": 3, "duration_seconds": 0.286}, "timestamp": "2026-01-25T20:02:42.224180"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 256.947, "latencies_ms": [256.947], "images_per_second": 3.892, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urns of different sizes and shapes are displayed on a white shelf, with some vases containing dried plants in them.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10476.5, "ram_available_mb": 112029.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10472.3, "ram_available_mb": 112034.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [34.64, 34.64, 34.64], "power_watts_avg": 34.64, "power_watts_peak": 34.64, "energy_joules_est": 8.91, "sample_count": 3, "duration_seconds": 0.257}, "timestamp": "2026-01-25T20:02:42.538131"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.142, "latencies_ms": [82.142], "images_per_second": 12.174, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Vase", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.3, "ram_available_mb": 112034.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10472.6, "ram_available_mb": 112033.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [34.64], "power_watts_avg": 34.64, "power_watts_peak": 34.64, "energy_joules_est": 2.87, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:02:42.644600"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 74.05, "latencies_ms": [74.05], "images_per_second": 13.504, "prompt_tokens": 763, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urns on a white shelf", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.6, "ram_available_mb": 112033.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10470.8, "ram_available_mb": 112035.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [35.68], "power_watts_avg": 35.68, "power_watts_peak": 35.68, "energy_joules_est": 2.65, "sample_count": 1, "duration_seconds": 0.074}, "timestamp": "2026-01-25T20:02:42.749417"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 270.209, "latencies_ms": [270.209], "images_per_second": 3.701, "prompt_tokens": 757, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nA white pedestal display case holds a variety of pottery, including vases and bowls. The largest item in the display is a tall, narrow vase with a unique design featuring dried plants and flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.8, "ram_available_mb": 112035.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.68, 35.68, 35.68], "power_watts_avg": 35.68, "power_watts_peak": 35.68, "energy_joules_est": 9.66, "sample_count": 3, "duration_seconds": 0.271}, "timestamp": "2026-01-25T20:02:43.055741"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 79.616, "latencies_ms": [79.616], "images_per_second": 12.56, "prompt_tokens": 756, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urns on a shelf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10472.9, "ram_available_mb": 112033.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.39], "power_watts_avg": 37.39, "power_watts_peak": 37.39, "energy_joules_est": 2.99, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T20:02:43.161089"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 300.965, "latencies_ms": [300.965], "images_per_second": 3.323, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA skateboarder in a white shirt and blue jeans is performing a trick on a concrete ramp, with their arms outstretched for balance.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10472.9, "ram_available_mb": 112033.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10471.6, "ram_available_mb": 112034.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.39, 37.39, 37.39, 37.39], "power_watts_avg": 37.39, "power_watts_peak": 37.39, "energy_joules_est": 11.27, "sample_count": 4, "duration_seconds": 0.301}, "timestamp": "2026-01-25T20:02:43.574114"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 99.859, "latencies_ms": [99.859], "images_per_second": 10.014, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Skateboarder", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.6, "ram_available_mb": 112034.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.5], "power_watts_avg": 33.5, "power_watts_peak": 33.5, "energy_joules_est": 3.35, "sample_count": 1, "duration_seconds": 0.1}, "timestamp": "2026-01-25T20:02:43.678033"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 163.199, "latencies_ms": [163.199], "images_per_second": 6.127, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nThe skateboarder is riding his skateboard on a ramp in an empty skate park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.5, 33.5], "power_watts_avg": 33.5, "power_watts_peak": 33.5, "energy_joules_est": 5.47, "sample_count": 2, "duration_seconds": 0.163}, "timestamp": "2026-01-25T20:02:43.883177"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 49.324, "latencies_ms": [49.324], "images_per_second": 20.274, "prompt_tokens": 757, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 10470.5, "ram_available_mb": 112035.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.5], "power_watts_avg": 33.5, "power_watts_peak": 33.5, "energy_joules_est": 1.67, "sample_count": 1, "duration_seconds": 0.05}, "timestamp": "2026-01-25T20:02:43.990524"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 126.048, "latencies_ms": [126.048], "images_per_second": 7.933, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n The skateboarder is wearing a helmet and knee pads for safety.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10470.5, "ram_available_mb": 112035.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10473.3, "ram_available_mb": 112033.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [33.5, 29.57], "power_watts_avg": 31.54, "power_watts_peak": 33.5, "energy_joules_est": 4.0, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:02:44.197826"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 331.142, "latencies_ms": [331.142], "images_per_second": 3.02, "prompt_tokens": 744, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nA large clock tower stands in a city square, surrounded by twinkling Christmas lights and festive decorations. The clock face is illuminated against the dark sky, creating a beautiful scene.", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 10473.3, "ram_available_mb": 112033.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [29.57, 29.57, 29.57, 29.57], "power_watts_avg": 29.57, "power_watts_peak": 29.57, "energy_joules_est": 9.8, "sample_count": 4, "duration_seconds": 0.331}, "timestamp": "2026-01-25T20:02:44.607966"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 235.036, "latencies_ms": [235.036], "images_per_second": 4.255, "prompt_tokens": 759, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\n 1. Christmas tree  2. Clock tower  3. Building  4. Street lamp  5. Person  6. Person  7. Person  8. Person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [31.91, 31.91, 31.91], "power_watts_avg": 31.91, "power_watts_peak": 31.91, "energy_joules_est": 7.52, "sample_count": 3, "duration_seconds": 0.236}, "timestamp": "2026-01-25T20:02:44.914415"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 116.119, "latencies_ms": [116.119], "images_per_second": 8.612, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nA large clock tower is in front of a Christmas tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [31.91, 31.91], "power_watts_avg": 31.91, "power_watts_peak": 31.91, "energy_joules_est": 3.73, "sample_count": 2, "duration_seconds": 0.117}, "timestamp": "2026-01-25T20:02:45.119735"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 503.447, "latencies_ms": [503.447], "images_per_second": 1.986, "prompt_tokens": 757, "response_tokens_est": 90, "n_tiles": 1, "output_text": "\nThe image shows a large clock tower with two clocks, one on each side of the structure. The clock tower stands tall in the center of a city square, surrounded by people walking around and enjoying the festive atmosphere. In front of the clock tower, there's a Christmas tree adorned with lights that adds to the holiday spirit. The scene is illuminated by streetlights and other artificial lighting, creating a warm and inviting ambiance for the visitors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [35.12, 35.12, 35.12, 35.12, 35.12, 32.42], "power_watts_avg": 34.67, "power_watts_peak": 35.12, "energy_joules_est": 17.47, "sample_count": 6, "duration_seconds": 0.504}, "timestamp": "2026-01-25T20:02:45.726462"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 363.223, "latencies_ms": [363.223], "images_per_second": 2.753, "prompt_tokens": 756, "response_tokens_est": 63, "n_tiles": 1, "output_text": "\n The image features a large clock tower with two clocks on it. The clock face is white and black, while the clock hands are red. The clock tower stands tall in front of a Christmas tree decorated with lights that illuminate the scene. People can be seen walking around the area, enjoying the festive atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.42, 32.42, 32.42, 32.42], "power_watts_avg": 32.42, "power_watts_peak": 32.42, "energy_joules_est": 11.79, "sample_count": 4, "duration_seconds": 0.364}, "timestamp": "2026-01-25T20:02:46.130691"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 283.719, "latencies_ms": [283.719], "images_per_second": 3.525, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA young man in a blue shirt and black shorts is playing tennis, swinging his racket at an incoming ball on a court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10469.6, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.42, 35.42, 35.42], "power_watts_avg": 35.42, "power_watts_peak": 35.42, "energy_joules_est": 10.08, "sample_count": 3, "duration_seconds": 0.285}, "timestamp": "2026-01-25T20:02:46.445265"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 153.142, "latencies_ms": [153.142], "images_per_second": 6.53, "prompt_tokens": 759, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n [0.39, 0.13, 0.66, 0.6]", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.6, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.42, 36.79], "power_watts_avg": 36.1, "power_watts_peak": 36.79, "energy_joules_est": 5.55, "sample_count": 2, "duration_seconds": 0.154}, "timestamp": "2026-01-25T20:02:46.650475"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 143.361, "latencies_ms": [143.361], "images_per_second": 6.975, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. A man in a blue shirt playing tennis on a court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10461.8, "ram_available_mb": 112044.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.79, 36.79], "power_watts_avg": 36.79, "power_watts_peak": 36.79, "energy_joules_est": 5.28, "sample_count": 2, "duration_seconds": 0.144}, "timestamp": "2026-01-25T20:02:46.854798"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 194.601, "latencies_ms": [194.601], "images_per_second": 5.139, "prompt_tokens": 757, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA young man in a blue shirt and black shorts is playing tennis on an outdoor court, swinging his racket at a ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.8, "ram_available_mb": 112044.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [36.79, 36.79], "power_watts_avg": 36.79, "power_watts_peak": 36.79, "energy_joules_est": 7.18, "sample_count": 2, "duration_seconds": 0.195}, "timestamp": "2026-01-25T20:02:47.059457"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 320.644, "latencies_ms": [320.644], "images_per_second": 3.119, "prompt_tokens": 756, "response_tokens_est": 55, "n_tiles": 1, "output_text": "\n The image shows a man playing tennis on an outdoor court. He is wearing a blue shirt and black shorts while holding a yellow tennis racket in his right hand as he prepares to hit the ball. The scene takes place during daylight hours with natural light illuminating the surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10462.8, "ram_available_mb": 112043.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [34.24, 34.24, 34.24, 34.24], "power_watts_avg": 34.24, "power_watts_peak": 34.24, "energy_joules_est": 10.99, "sample_count": 4, "duration_seconds": 0.321}, "timestamp": "2026-01-25T20:02:47.465830"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 287.987, "latencies_ms": [287.987], "images_per_second": 3.472, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "urns of gold and silver are placed on a table in front of two white chairs, with a fireplace and bookshelves behind them.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.7, "ram_available_mb": 112043.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10463.9, "ram_available_mb": 112042.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [34.24, 32.14, 32.14], "power_watts_avg": 32.84, "power_watts_peak": 34.24, "energy_joules_est": 9.48, "sample_count": 3, "duration_seconds": 0.289}, "timestamp": "2026-01-25T20:02:47.776385"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 68.442, "latencies_ms": [68.442], "images_per_second": 14.611, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Chair", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10463.9, "ram_available_mb": 112042.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [32.14], "power_watts_avg": 32.14, "power_watts_peak": 32.14, "energy_joules_est": 2.21, "sample_count": 1, "duration_seconds": 0.069}, "timestamp": "2026-01-25T20:02:47.881611"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 106.377, "latencies_ms": [106.377], "images_per_second": 9.401, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "urns on a table in front of the fireplace.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10464.4, "ram_available_mb": 112041.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [32.14, 32.14], "power_watts_avg": 32.14, "power_watts_peak": 32.14, "energy_joules_est": 3.43, "sample_count": 2, "duration_seconds": 0.107}, "timestamp": "2026-01-25T20:02:48.088787"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 319.669, "latencies_ms": [319.669], "images_per_second": 3.128, "prompt_tokens": 757, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\nThe image shows a living room with white walls, a fireplace, two lamps on either side of the fireplace, and several bookshelves filled with numerous books. The room also features a comfortable chair in front of the fireplace, creating an inviting atmosphere for relaxation or reading.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.4, "ram_available_mb": 112041.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [35.89, 35.89, 35.89, 35.89], "power_watts_avg": 35.89, "power_watts_peak": 35.89, "energy_joules_est": 11.48, "sample_count": 4, "duration_seconds": 0.32}, "timestamp": "2026-01-25T20:02:48.495734"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 79.306, "latencies_ms": [79.306], "images_per_second": 12.609, "prompt_tokens": 756, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urns on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10467.8, "ram_available_mb": 112038.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [35.89], "power_watts_avg": 35.89, "power_watts_peak": 35.89, "energy_joules_est": 2.87, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T20:02:48.601519"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 256.837, "latencies_ms": [256.837], "images_per_second": 3.894, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nThree zebras are grazing on a grassy plain, with their heads bowed down to eat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.8, "ram_available_mb": 112038.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [34.79, 34.79, 34.79], "power_watts_avg": 34.79, "power_watts_peak": 34.79, "energy_joules_est": 8.96, "sample_count": 3, "duration_seconds": 0.258}, "timestamp": "2026-01-25T20:02:48.913334"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 88.516, "latencies_ms": [88.516], "images_per_second": 11.297, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Zebra in foreground", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [34.79], "power_watts_avg": 34.79, "power_watts_peak": 34.79, "energy_joules_est": 3.1, "sample_count": 1, "duration_seconds": 0.089}, "timestamp": "2026-01-25T20:02:49.018361"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 114.033, "latencies_ms": [114.033], "images_per_second": 8.769, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Zebra on left side of photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10463.5, "ram_available_mb": 112042.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [34.79, 33.62], "power_watts_avg": 34.2, "power_watts_peak": 34.79, "energy_joules_est": 3.92, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T20:02:49.223432"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 257.425, "latencies_ms": [257.425], "images_per_second": 3.885, "prompt_tokens": 757, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\n Three zebras are grazing on grass in a field, with one zebra facing the camera. The other two zebras have their backs to the camera as they graze peacefully.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.5, "ram_available_mb": 112042.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10460.0, "ram_available_mb": 112046.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [33.62, 33.62, 33.62], "power_watts_avg": 33.62, "power_watts_peak": 33.62, "energy_joules_est": 8.67, "sample_count": 3, "duration_seconds": 0.258}, "timestamp": "2026-01-25T20:02:49.529448"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 105.85, "latencies_ms": [105.85], "images_per_second": 9.447, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The sky is blue and clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10459.3, "ram_available_mb": 112047.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [33.62, 33.13], "power_watts_avg": 33.37, "power_watts_peak": 33.62, "energy_joules_est": 3.54, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T20:02:49.734749"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 260.547, "latencies_ms": [260.547], "images_per_second": 3.838, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "urns of water on tables, a clock on the wall, and people sitting at tables in a restaurant setting.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 10459.3, "ram_available_mb": 112047.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10463.7, "ram_available_mb": 112042.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [33.13, 33.13, 33.13], "power_watts_avg": 33.13, "power_watts_peak": 33.13, "energy_joules_est": 8.65, "sample_count": 3, "duration_seconds": 0.261}, "timestamp": "2026-01-25T20:02:50.045429"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 219.428, "latencies_ms": [219.428], "images_per_second": 4.557, "prompt_tokens": 759, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\n 1. Tablecloth  2. Chair  3. Chair  4. Chair  5. Chair  6. Chair  7. Chair  8. Chair", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.7, "ram_available_mb": 112042.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10462.5, "ram_available_mb": 112043.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [32.41, 32.41, 32.41], "power_watts_avg": 32.41, "power_watts_peak": 32.41, "energy_joules_est": 7.12, "sample_count": 3, "duration_seconds": 0.22}, "timestamp": "2026-01-25T20:02:50.351124"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 131.48, "latencies_ms": [131.48], "images_per_second": 7.606, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Man in suit sitting at table with others.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.5, "ram_available_mb": 112043.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [32.41, 32.41], "power_watts_avg": 32.41, "power_watts_peak": 32.41, "energy_joules_est": 4.27, "sample_count": 2, "duration_seconds": 0.132}, "timestamp": "2026-01-25T20:02:50.556589"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 267.893, "latencies_ms": [267.893], "images_per_second": 3.733, "prompt_tokens": 757, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\nA group of people are gathered around a table in a restaurant, playing a game. The table has several cups on it, suggesting that they might be enjoying some refreshments during their gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10459.7, "ram_available_mb": 112046.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [31.68, 31.68, 31.68], "power_watts_avg": 31.68, "power_watts_peak": 31.68, "energy_joules_est": 8.5, "sample_count": 3, "duration_seconds": 0.268}, "timestamp": "2026-01-25T20:02:50.861864"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 54.541, "latencies_ms": [54.541], "images_per_second": 18.335, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.7, "ram_available_mb": 112046.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 10459.8, "ram_available_mb": 112046.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [31.68], "power_watts_avg": 31.68, "power_watts_peak": 31.68, "energy_joules_est": 1.74, "sample_count": 1, "duration_seconds": 0.055}, "timestamp": "2026-01-25T20:02:50.966762"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 374.015, "latencies_ms": [374.015], "images_per_second": 2.674, "prompt_tokens": 744, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nA group of white swans are swimming in a body of water, with some standing and others floating on their backs. The scene is set against a backdrop of boats docked at a pier or marina.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.8, "ram_available_mb": 112046.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10457.1, "ram_available_mb": 112049.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [31.68, 27.08, 27.08, 27.08], "power_watts_avg": 28.23, "power_watts_peak": 31.68, "energy_joules_est": 10.57, "sample_count": 4, "duration_seconds": 0.374}, "timestamp": "2026-01-25T20:02:51.378602"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.033, "latencies_ms": [79.033], "images_per_second": 12.653, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Boat: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.1, "ram_available_mb": 112049.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10457.5, "ram_available_mb": 112048.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [27.08], "power_watts_avg": 27.08, "power_watts_peak": 27.08, "energy_joules_est": 2.15, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T20:02:51.483648"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 120.923, "latencies_ms": [120.923], "images_per_second": 8.27, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Swan in front of boat on left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.5, "ram_available_mb": 112048.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10458.5, "ram_available_mb": 112047.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [27.08, 33.09], "power_watts_avg": 30.08, "power_watts_peak": 33.09, "energy_joules_est": 3.65, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T20:02:51.688351"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 274.482, "latencies_ms": [274.482], "images_per_second": 3.643, "prompt_tokens": 757, "response_tokens_est": 45, "n_tiles": 1, "output_text": "\nA group of white swans are swimming in a body of water, with some floating on the surface and others swimming close to the bottom. The water appears calm, and there are several boats visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10455.6, "ram_available_mb": 112050.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [33.09, 33.09, 33.09], "power_watts_avg": 33.09, "power_watts_peak": 33.09, "energy_joules_est": 9.09, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T20:02:51.994060"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 359.922, "latencies_ms": [359.922], "images_per_second": 2.778, "prompt_tokens": 756, "response_tokens_est": 60, "n_tiles": 1, "output_text": "\n The image features a group of white swans swimming in the water. The swans are surrounded by boats and other watercraft, creating an interesting contrast between nature and human-made elements. The sky is clear blue with some clouds scattered across it, providing a beautiful backdrop to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.6, "ram_available_mb": 112050.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [33.09, 36.04, 36.04, 36.04], "power_watts_avg": 35.3, "power_watts_peak": 36.04, "energy_joules_est": 12.72, "sample_count": 4, "duration_seconds": 0.36}, "timestamp": "2026-01-25T20:02:52.400047"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 250.771, "latencies_ms": [250.771], "images_per_second": 3.988, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA newlywed couple stands on a dance floor, cutting their wedding cake with white icing and pink flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10458.1, "ram_available_mb": 112048.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [36.04, 36.04, 36.8], "power_watts_avg": 36.29, "power_watts_peak": 36.8, "energy_joules_est": 9.13, "sample_count": 3, "duration_seconds": 0.252}, "timestamp": "2026-01-25T20:02:52.710109"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 164.07, "latencies_ms": [164.07], "images_per_second": 6.095, "prompt_tokens": 759, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n 1. Bride and groom standing under a white tent with a cake table in front of them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.1, "ram_available_mb": 112048.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10458.1, "ram_available_mb": 112048.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [36.8, 36.8], "power_watts_avg": 36.8, "power_watts_peak": 36.8, "energy_joules_est": 6.05, "sample_count": 2, "duration_seconds": 0.164}, "timestamp": "2026-01-25T20:02:52.915069"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 105.208, "latencies_ms": [105.208], "images_per_second": 9.505, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nThe bride and groom are cutting their wedding cake.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.1, "ram_available_mb": 112048.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [36.8, 36.8], "power_watts_avg": 36.8, "power_watts_peak": 36.8, "energy_joules_est": 3.9, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T20:02:53.120605"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 326.868, "latencies_ms": [326.868], "images_per_second": 3.059, "prompt_tokens": 757, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\nA newlywed couple stands on a dance floor, cutting their wedding cake. The bride wears a white dress with a red train, while the groom sports a gray suit. A DJ booth can be seen in the background, suggesting that music will be played during the reception.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.4, "ram_available_mb": 112045.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10460.9, "ram_available_mb": 112045.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [36.74, 36.74, 36.74, 36.74], "power_watts_avg": 36.74, "power_watts_peak": 36.74, "energy_joules_est": 12.02, "sample_count": 4, "duration_seconds": 0.327}, "timestamp": "2026-01-25T20:02:53.527224"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 85.911, "latencies_ms": [85.911], "images_per_second": 11.64, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urns of candles on the table.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.9, "ram_available_mb": 112045.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [36.74], "power_watts_avg": 36.74, "power_watts_peak": 36.74, "energy_joules_est": 3.17, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T20:02:53.632857"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 267.531, "latencies_ms": [267.531], "images_per_second": 3.738, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urns of flowers sit on a table in front of two windows with white curtains, and a lamp is turned off.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [32.4, 32.4, 32.4], "power_watts_avg": 32.4, "power_watts_peak": 32.4, "energy_joules_est": 8.7, "sample_count": 3, "duration_seconds": 0.268}, "timestamp": "2026-01-25T20:02:53.946300"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 229.613, "latencies_ms": [229.613], "images_per_second": 4.355, "prompt_tokens": 759, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\n 1. Sofa  2. Table 3. Lamp 4. Picture 5. Plant 6. Chair 7. Table 8. Lamp 9. Picture 10.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10455.8, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 8.0}, "power_stats": {"power_watts_samples": [32.4, 33.9, 33.9], "power_watts_avg": 33.4, "power_watts_peak": 33.9, "energy_joules_est": 7.68, "sample_count": 3, "duration_seconds": 0.23}, "timestamp": "2026-01-25T20:02:54.253401"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 96.889, "latencies_ms": [96.889], "images_per_second": 10.321, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urn with plant on table in front of couch", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.8, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10456.0, "ram_available_mb": 112050.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 8.0}, "power_stats": {"power_watts_samples": [33.9], "power_watts_avg": 33.9, "power_watts_peak": 33.9, "energy_joules_est": 3.3, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T20:02:54.358094"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 196.568, "latencies_ms": [196.568], "images_per_second": 5.087, "prompt_tokens": 757, "response_tokens_est": 29, "n_tiles": 1, "output_text": "urns of plants sit on a table in front of two windows with white curtains, one of which has a painting hanging above it.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.0, "ram_available_mb": 112050.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10462.8, "ram_available_mb": 112043.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 8.0}, "power_stats": {"power_watts_samples": [33.9, 33.9], "power_watts_avg": 33.9, "power_watts_peak": 33.9, "energy_joules_est": 6.67, "sample_count": 2, "duration_seconds": 0.197}, "timestamp": "2026-01-25T20:02:54.562694"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 130.404, "latencies_ms": [130.404], "images_per_second": 7.668, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "urns of flowers on table and a painting above the couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.8, "ram_available_mb": 112043.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10462.8, "ram_available_mb": 112043.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 8.0}, "power_stats": {"power_watts_samples": [35.07, 35.07], "power_watts_avg": 35.07, "power_watts_peak": 35.07, "energy_joules_est": 4.58, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T20:02:54.767623"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 269.514, "latencies_ms": [269.514], "images_per_second": 3.71, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urn of gold and red liquid sits on a table, with two clocks displaying different times in front of it.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10462.8, "ram_available_mb": 112043.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.07, 35.07, 35.07], "power_watts_avg": 35.07, "power_watts_peak": 35.07, "energy_joules_est": 9.46, "sample_count": 3, "duration_seconds": 0.27}, "timestamp": "2026-01-25T20:02:55.079223"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 227.389, "latencies_ms": [227.389], "images_per_second": 4.398, "prompt_tokens": 759, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\n 1. Clock face  2. Doll head  3. Doll body  4. Doll hands  5. Doll feet  6. Doll eyes  7. Doll mouth", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.97, 34.97, 34.97], "power_watts_avg": 34.97, "power_watts_peak": 34.97, "energy_joules_est": 7.96, "sample_count": 3, "duration_seconds": 0.228}, "timestamp": "2026-01-25T20:02:55.387645"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 185.493, "latencies_ms": [185.493], "images_per_second": 5.391, "prompt_tokens": 763, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\n 1. A doll with red hair and a white face is sitting in front of two clocks that are facing each other.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.97, 34.97], "power_watts_avg": 34.97, "power_watts_peak": 34.97, "energy_joules_est": 6.5, "sample_count": 2, "duration_seconds": 0.186}, "timestamp": "2026-01-25T20:02:55.595460"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 273.551, "latencies_ms": [273.551], "images_per_second": 3.656, "prompt_tokens": 757, "response_tokens_est": 45, "n_tiles": 1, "output_text": "\nTwo dolls with red hair are sitting on a table, each wearing a clock face. One doll has a candle in its lap, while the other does not. The clocks show that it's 11:11 am.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [33.98, 33.98, 33.98], "power_watts_avg": 33.98, "power_watts_peak": 33.98, "energy_joules_est": 9.31, "sample_count": 3, "duration_seconds": 0.274}, "timestamp": "2026-01-25T20:02:55.902011"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 338.901, "latencies_ms": [338.901], "images_per_second": 2.951, "prompt_tokens": 756, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\n The image features a doll with red hair and an orange face sitting in front of two clocks. One clock is white with black numbers and hands, while the other is yellow with black numbers and hands. A candle is also present on the table between the dolls and the clocks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10456.8, "ram_available_mb": 112049.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [33.98, 33.98, 33.21, 33.21], "power_watts_avg": 33.59, "power_watts_peak": 33.98, "energy_joules_est": 11.41, "sample_count": 4, "duration_seconds": 0.34}, "timestamp": "2026-01-25T20:02:56.309660"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 277.061, "latencies_ms": [277.061], "images_per_second": 3.609, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA person wearing a helmet is sitting on a motorcycle, with another person standing nearby and a third person in the background.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10456.8, "ram_available_mb": 112049.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10456.5, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [33.21, 33.21, 33.21], "power_watts_avg": 33.21, "power_watts_peak": 33.21, "energy_joules_est": 9.21, "sample_count": 3, "duration_seconds": 0.277}, "timestamp": "2026-01-25T20:02:56.622673"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.259, "latencies_ms": [78.259], "images_per_second": 12.778, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Helmet", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.5, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10456.5, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [38.33], "power_watts_avg": 38.33, "power_watts_peak": 38.33, "energy_joules_est": 3.01, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T20:02:56.728799"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 128.761, "latencies_ms": [128.761], "images_per_second": 7.766, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n 1. Person in front of motorcycle helmet and person behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.5, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10459.1, "ram_available_mb": 112047.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [38.33, 38.33], "power_watts_avg": 38.33, "power_watts_peak": 38.33, "energy_joules_est": 4.95, "sample_count": 2, "duration_seconds": 0.129}, "timestamp": "2026-01-25T20:02:56.935953"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 235.905, "latencies_ms": [235.905], "images_per_second": 4.239, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nIn a busy street, a person wearing a helmet sits on a motorcycle. The rider appears to be looking at their phone while holding onto the handlebars of the scooter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.8, "ram_available_mb": 112048.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10457.5, "ram_available_mb": 112048.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [38.33, 38.33, 34.36], "power_watts_avg": 37.0, "power_watts_peak": 38.33, "energy_joules_est": 8.74, "sample_count": 3, "duration_seconds": 0.236}, "timestamp": "2026-01-25T20:02:57.240027"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 209.843, "latencies_ms": [209.843], "images_per_second": 4.765, "prompt_tokens": 756, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\n The image shows a man sitting on a motorcycle wearing a helmet. He is looking at his phone and appears to be focused on something outside the frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.5, "ram_available_mb": 112048.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10459.3, "ram_available_mb": 112047.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [34.36, 34.36, 34.36], "power_watts_avg": 34.36, "power_watts_peak": 34.36, "energy_joules_est": 7.22, "sample_count": 3, "duration_seconds": 0.21}, "timestamp": "2026-01-25T20:02:57.546078"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 236.909, "latencies_ms": [236.909], "images_per_second": 4.221, "prompt_tokens": 744, "response_tokens_est": 19, "n_tiles": 1, "output_text": "iced pizza with cheese and spinach on top sits on a wooden table in a restaurant kitchen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.3, "ram_available_mb": 112047.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [29.72, 29.72, 29.72], "power_watts_avg": 29.72, "power_watts_peak": 29.72, "energy_joules_est": 7.06, "sample_count": 3, "duration_seconds": 0.237}, "timestamp": "2026-01-25T20:02:57.857265"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.897, "latencies_ms": [78.897], "images_per_second": 12.675, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Pizza cutter", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.4, "ram_available_mb": 112045.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10455.3, "ram_available_mb": 112051.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [29.72], "power_watts_avg": 29.72, "power_watts_peak": 29.72, "energy_joules_est": 2.35, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T20:02:57.964186"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 99.665, "latencies_ms": [99.665], "images_per_second": 10.034, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nA large pizza is sitting on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.3, "ram_available_mb": 112051.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10453.7, "ram_available_mb": 112052.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 33.0}, "power_stats": {"power_watts_samples": [29.72], "power_watts_avg": 29.72, "power_watts_peak": 29.72, "energy_joules_est": 2.97, "sample_count": 1, "duration_seconds": 0.1}, "timestamp": "2026-01-25T20:02:58.069201"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 105.749, "latencies_ms": [105.749], "images_per_second": 9.456, "prompt_tokens": 757, "response_tokens_est": 13, "n_tiles": 1, "output_text": "urn of water on a table in front of a pizza.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.7, "ram_available_mb": 112052.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10456.4, "ram_available_mb": 112049.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 33.0}, "power_stats": {"power_watts_samples": [33.3, 33.3], "power_watts_avg": 33.3, "power_watts_peak": 33.3, "energy_joules_est": 3.53, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T20:02:58.274688"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 104.761, "latencies_ms": [104.761], "images_per_second": 9.546, "prompt_tokens": 756, "response_tokens_est": 11, "n_tiles": 1, "output_text": "urns of water on the stove top.\n", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10456.4, "ram_available_mb": 112049.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10456.7, "ram_available_mb": 112049.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 33.0}, "power_stats": {"power_watts_samples": [33.3, 33.3], "power_watts_avg": 33.3, "power_watts_peak": 33.3, "energy_joules_est": 3.5, "sample_count": 2, "duration_seconds": 0.105}, "timestamp": "2026-01-25T20:02:58.480963"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 328.471, "latencies_ms": [328.471], "images_per_second": 3.044, "prompt_tokens": 744, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA female tennis player, dressed in white, is captured mid-swing on a grass court, with her racket raised high above her head and her body twisted to the side.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10456.7, "ram_available_mb": 112049.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10464.7, "ram_available_mb": 112041.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 33.0}, "power_stats": {"power_watts_samples": [33.3, 28.42, 28.42, 28.42], "power_watts_avg": 29.64, "power_watts_peak": 33.3, "energy_joules_est": 9.75, "sample_count": 4, "duration_seconds": 0.329}, "timestamp": "2026-01-25T20:02:58.893295"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 92.435, "latencies_ms": [92.435], "images_per_second": 10.818, "prompt_tokens": 759, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Net  2. Racket", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.7, "ram_available_mb": 112041.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 33.0}, "power_stats": {"power_watts_samples": [28.42], "power_watts_avg": 28.42, "power_watts_peak": 28.42, "energy_joules_est": 2.64, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:02:58.999101"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 120.946, "latencies_ms": [120.946], "images_per_second": 8.268, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nThe woman is holding a tennis racket and her body is in motion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [28.42, 31.42], "power_watts_avg": 29.92, "power_watts_peak": 31.42, "energy_joules_est": 3.63, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T20:02:59.202878"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 194.899, "latencies_ms": [194.899], "images_per_second": 5.131, "prompt_tokens": 757, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA female tennis player in a white outfit stands on a grass court, holding her racket up high as she prepares to hit an incoming ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 10469.3, "ram_available_mb": 112037.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [31.42, 31.42], "power_watts_avg": 31.42, "power_watts_peak": 31.42, "energy_joules_est": 6.13, "sample_count": 2, "duration_seconds": 0.195}, "timestamp": "2026-01-25T20:02:59.407076"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 46.639, "latencies_ms": [46.639], "images_per_second": 21.441, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.3, "ram_available_mb": 112037.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10470.5, "ram_available_mb": 112035.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [31.42], "power_watts_avg": 31.42, "power_watts_peak": 31.42, "energy_joules_est": 1.48, "sample_count": 1, "duration_seconds": 0.047}, "timestamp": "2026-01-25T20:02:59.512746"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 259.996, "latencies_ms": [259.996], "images_per_second": 3.846, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urn of water on a shelf in a bathroom, with a towel hanging from it and a shower curtain nearby.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10470.5, "ram_available_mb": 112035.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10470.5, "ram_available_mb": 112035.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [31.42, 33.91, 33.91], "power_watts_avg": 33.08, "power_watts_peak": 33.91, "energy_joules_est": 8.61, "sample_count": 3, "duration_seconds": 0.26}, "timestamp": "2026-01-25T20:02:59.824256"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 239.342, "latencies_ms": [239.342], "images_per_second": 4.178, "prompt_tokens": 759, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\n 1. Toilet  2. Shower curtain  3. Towel rack  4. Closet door  5. Shower curtain  6. Closet door", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.5, "ram_available_mb": 112035.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.91, 33.91, 33.91], "power_watts_avg": 33.91, "power_watts_peak": 33.91, "energy_joules_est": 8.12, "sample_count": 3, "duration_seconds": 0.24}, "timestamp": "2026-01-25T20:03:00.130552"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 146.337, "latencies_ms": [146.337], "images_per_second": 6.834, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n 1. Toilet is white and located in a bathroom with brown floor tiles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10470.5, "ram_available_mb": 112035.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.94, 34.94], "power_watts_avg": 34.94, "power_watts_peak": 34.94, "energy_joules_est": 5.13, "sample_count": 2, "duration_seconds": 0.147}, "timestamp": "2026-01-25T20:03:00.337871"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 365.51, "latencies_ms": [365.51], "images_per_second": 2.736, "prompt_tokens": 757, "response_tokens_est": 66, "n_tiles": 1, "output_text": "\nThe image shows a bathroom with white walls, a toilet, and a shower. The toilet has a brown towel hanging on a rack above it. A shower curtain is partially drawn back in one corner of the room. On the right side of the photo, there are two shelves holding various items such as towels and clothes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.5, "ram_available_mb": 112035.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10465.4, "ram_available_mb": 112040.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.94, 34.94, 34.94, 36.11], "power_watts_avg": 35.23, "power_watts_peak": 36.11, "energy_joules_est": 12.9, "sample_count": 4, "duration_seconds": 0.366}, "timestamp": "2026-01-25T20:03:00.744974"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 133.402, "latencies_ms": [133.402], "images_per_second": 7.496, "prompt_tokens": 756, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n 1. The toilet is white and the shower curtain has a maroon color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.4, "ram_available_mb": 112040.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10469.3, "ram_available_mb": 112037.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.11, 36.11], "power_watts_avg": 36.11, "power_watts_peak": 36.11, "energy_joules_est": 4.83, "sample_count": 2, "duration_seconds": 0.134}, "timestamp": "2026-01-25T20:03:00.951291"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 256.37, "latencies_ms": [256.37], "images_per_second": 3.901, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA man and a woman are sitting at a table, each holding a glass of red wine.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10469.3, "ram_available_mb": 112037.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [36.11, 31.47, 31.47], "power_watts_avg": 33.01, "power_watts_peak": 36.11, "energy_joules_est": 8.48, "sample_count": 3, "duration_seconds": 0.257}, "timestamp": "2026-01-25T20:03:01.263895"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 197.626, "latencies_ms": [197.626], "images_per_second": 5.06, "prompt_tokens": 759, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\n 1. Woman in green shirt holding wine glass 2. Man in black t-shirt smiling at camera 3. Person sitting at table 4.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [31.47, 31.47], "power_watts_avg": 31.47, "power_watts_peak": 31.47, "energy_joules_est": 6.23, "sample_count": 2, "duration_seconds": 0.198}, "timestamp": "2026-01-25T20:03:01.468952"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 109.253, "latencies_ms": [109.253], "images_per_second": 9.153, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nTwo people are holding wine glasses in front of a window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10467.8, "ram_available_mb": 112038.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [31.47, 36.34], "power_watts_avg": 33.91, "power_watts_peak": 36.34, "energy_joules_est": 3.71, "sample_count": 2, "duration_seconds": 0.109}, "timestamp": "2026-01-25T20:03:01.673927"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 200.108, "latencies_ms": [200.108], "images_per_second": 4.997, "prompt_tokens": 757, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\n A man in a black shirt smiles at the camera while holding a glass of red wine, with two glasses of wine in front of him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.9, "ram_available_mb": 112038.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [36.34, 36.34], "power_watts_avg": 36.34, "power_watts_peak": 36.34, "energy_joules_est": 7.28, "sample_count": 2, "duration_seconds": 0.2}, "timestamp": "2026-01-25T20:03:01.877935"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 92.469, "latencies_ms": [92.469], "images_per_second": 10.814, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urns of water on the table.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10471.2, "ram_available_mb": 112035.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [36.34], "power_watts_avg": 36.34, "power_watts_peak": 36.34, "energy_joules_est": 3.37, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:03:01.981590"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 278.532, "latencies_ms": [278.532], "images_per_second": 3.59, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA surfer in a black wetsuit rides a wave on their surfboard, skillfully navigating the powerful force of nature.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10471.2, "ram_available_mb": 112035.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [36.34, 35.16, 35.16], "power_watts_avg": 35.55, "power_watts_peak": 36.34, "energy_joules_est": 9.91, "sample_count": 3, "duration_seconds": 0.279}, "timestamp": "2026-01-25T20:03:02.290492"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.808, "latencies_ms": [82.808], "images_per_second": 12.076, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Water - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [35.16], "power_watts_avg": 35.16, "power_watts_peak": 35.16, "energy_joules_est": 2.92, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:03:02.395646"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 109.116, "latencies_ms": [109.116], "images_per_second": 9.165, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe man is surfing on a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10472.9, "ram_available_mb": 112033.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [35.16, 35.16], "power_watts_avg": 35.16, "power_watts_peak": 35.16, "energy_joules_est": 3.85, "sample_count": 2, "duration_seconds": 0.109}, "timestamp": "2026-01-25T20:03:02.600682"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 561.373, "latencies_ms": [561.373], "images_per_second": 1.781, "prompt_tokens": 757, "response_tokens_est": 99, "n_tiles": 1, "output_text": "\nIn the image, a surfer in a black wetsuit is riding a wave on his surfboard. The surfer appears to be crouched down as he skillfully maneuvers through the water. The ocean surrounding him has a beautiful blue-green color with white foam on its surface, indicating that it's a sunny day perfect for surfing.\n\nIn the background, there are several people enjoying their time at the beach, possibly watching the surfer or engaging in other activities.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10472.9, "ram_available_mb": 112033.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.58, 34.58, 34.58, 34.58, 34.58, 37.27], "power_watts_avg": 35.03, "power_watts_peak": 37.27, "energy_joules_est": 19.69, "sample_count": 6, "duration_seconds": 0.562}, "timestamp": "2026-01-25T20:03:03.208237"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 415.936, "latencies_ms": [415.936], "images_per_second": 2.404, "prompt_tokens": 756, "response_tokens_est": 72, "n_tiles": 1, "output_text": "\n The image shows a surfer riding a wave in the ocean. The surfer is wearing a black shirt and shorts while standing on his surfboard. The water surrounding him has a greenish hue, suggesting that it might be a cloudy day or there could be algae blooms. The sky above appears to be blue with some clouds scattered across it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.27, 37.27, 37.27, 37.27, 37.03], "power_watts_avg": 37.22, "power_watts_peak": 37.27, "energy_joules_est": 15.5, "sample_count": 5, "duration_seconds": 0.416}, "timestamp": "2026-01-25T20:03:03.715911"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 247.343, "latencies_ms": [247.343], "images_per_second": 4.043, "prompt_tokens": 744, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n A wooden desk holds a variety of electronic devices, including four laptops and two backpacks.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.03, 37.03, 37.03], "power_watts_avg": 37.03, "power_watts_peak": 37.03, "energy_joules_est": 9.18, "sample_count": 3, "duration_seconds": 0.248}, "timestamp": "2026-01-25T20:03:04.025527"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 92.935, "latencies_ms": [92.935], "images_per_second": 10.76, "prompt_tokens": 759, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Laptop (black): 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [37.03], "power_watts_avg": 37.03, "power_watts_peak": 37.03, "energy_joules_est": 3.45, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:03:04.130136"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 102.772, "latencies_ms": [102.772], "images_per_second": 9.73, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Laptop on left side of table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [37.55, 37.55], "power_watts_avg": 37.55, "power_watts_peak": 37.55, "energy_joules_est": 3.87, "sample_count": 2, "duration_seconds": 0.103}, "timestamp": "2026-01-25T20:03:04.335407"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 147.462, "latencies_ms": [147.462], "images_per_second": 6.781, "prompt_tokens": 757, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA table with a red backpack on top of it, surrounded by several laptops.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [37.55, 37.55], "power_watts_avg": 37.55, "power_watts_peak": 37.55, "energy_joules_est": 5.55, "sample_count": 2, "duration_seconds": 0.148}, "timestamp": "2026-01-25T20:03:04.540685"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 138.898, "latencies_ms": [138.898], "images_per_second": 7.2, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A black and red backpack sits on a table next to several laptops.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10457.7, "ram_available_mb": 112048.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [37.55, 31.79], "power_watts_avg": 34.67, "power_watts_peak": 37.55, "energy_joules_est": 4.84, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T20:03:04.746730"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 281.492, "latencies_ms": [281.492], "images_per_second": 3.552, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "iced skiers in action, with a person mid-air performing a trick on their skis and another skier below them.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.7, "ram_available_mb": 112048.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.79, 31.79, 31.79], "power_watts_avg": 31.79, "power_watts_peak": 31.79, "energy_joules_est": 8.97, "sample_count": 3, "duration_seconds": 0.282}, "timestamp": "2026-01-25T20:03:05.056890"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 158.691, "latencies_ms": [158.691], "images_per_second": 6.302, "prompt_tokens": 759, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n 1. Skier in orange and yellow jacket and green helmet doing a trick on skis.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.13, 32.13], "power_watts_avg": 32.13, "power_watts_peak": 32.13, "energy_joules_est": 5.11, "sample_count": 2, "duration_seconds": 0.159}, "timestamp": "2026-01-25T20:03:05.262903"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 160.483, "latencies_ms": [160.483], "images_per_second": 6.231, "prompt_tokens": 763, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nThe skier is in midair and appears to be jumping over another person on skis.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10464.4, "ram_available_mb": 112041.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.13, 32.13], "power_watts_avg": 32.13, "power_watts_peak": 32.13, "energy_joules_est": 5.18, "sample_count": 2, "duration_seconds": 0.161}, "timestamp": "2026-01-25T20:03:05.468819"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 212.074, "latencies_ms": [212.074], "images_per_second": 4.715, "prompt_tokens": 757, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA skier in a green jacket and yellow pants is performing an aerial trick on their skis, while another skier watches from below.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.4, "ram_available_mb": 112041.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.13, 33.3, 33.3], "power_watts_avg": 32.91, "power_watts_peak": 33.3, "energy_joules_est": 7.0, "sample_count": 3, "duration_seconds": 0.213}, "timestamp": "2026-01-25T20:03:05.776244"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 121.727, "latencies_ms": [121.727], "images_per_second": 8.215, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The skier is wearing a green helmet and orange pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.3, 33.3], "power_watts_avg": 33.3, "power_watts_peak": 33.3, "energy_joules_est": 4.07, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:03:05.982397"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 137.281, "latencies_ms": [137.281], "images_per_second": 7.284, "prompt_tokens": 744, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10465.7, "ram_available_mb": 112040.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [33.3, 27.8], "power_watts_avg": 30.55, "power_watts_peak": 33.3, "energy_joules_est": 4.21, "sample_count": 2, "duration_seconds": 0.138}, "timestamp": "2026-01-25T20:03:06.190609"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 83.281, "latencies_ms": [83.281], "images_per_second": 12.008, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Window sill", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.7, "ram_available_mb": 112040.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [27.8], "power_watts_avg": 27.8, "power_watts_peak": 27.8, "energy_joules_est": 2.33, "sample_count": 1, "duration_seconds": 0.084}, "timestamp": "2026-01-25T20:03:06.296038"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 100.308, "latencies_ms": [100.308], "images_per_second": 9.969, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nA bird is sitting on a window sill.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [27.8], "power_watts_avg": 27.8, "power_watts_peak": 27.8, "energy_joules_est": 2.81, "sample_count": 1, "duration_seconds": 0.101}, "timestamp": "2026-01-25T20:03:06.401558"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 134.728, "latencies_ms": [134.728], "images_per_second": 7.422, "prompt_tokens": 757, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA black bird perches on a window sill, looking out at the water.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [27.8, 27.8], "power_watts_avg": 27.8, "power_watts_peak": 27.8, "energy_joules_est": 3.75, "sample_count": 2, "duration_seconds": 0.135}, "timestamp": "2026-01-25T20:03:06.606288"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 100.202, "latencies_ms": [100.202], "images_per_second": 9.98, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n A black bird is perched on a window sill.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10466.4, "ram_available_mb": 112039.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [31.45, 31.45], "power_watts_avg": 31.45, "power_watts_peak": 31.45, "energy_joules_est": 3.17, "sample_count": 2, "duration_seconds": 0.101}, "timestamp": "2026-01-25T20:03:06.811911"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 274.692, "latencies_ms": [274.692], "images_per_second": 3.64, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "urns of water and cleaning supplies are on the floor, with a man in a gray shirt leaning over to inspect them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.4, "ram_available_mb": 112039.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.45, 31.45, 31.45], "power_watts_avg": 31.45, "power_watts_peak": 31.45, "energy_joules_est": 8.66, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T20:03:07.123331"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 65.262, "latencies_ms": [65.262], "images_per_second": 15.323, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n Bucket: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10459.6, "ram_available_mb": 112046.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.46], "power_watts_avg": 34.46, "power_watts_peak": 34.46, "energy_joules_est": 2.26, "sample_count": 1, "duration_seconds": 0.066}, "timestamp": "2026-01-25T20:03:07.228438"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 110.556, "latencies_ms": [110.556], "images_per_second": 9.045, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Man in red hat leaning over toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.6, "ram_available_mb": 112046.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10459.6, "ram_available_mb": 112046.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.46, 34.46], "power_watts_avg": 34.46, "power_watts_peak": 34.46, "energy_joules_est": 3.82, "sample_count": 2, "duration_seconds": 0.111}, "timestamp": "2026-01-25T20:03:07.433518"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 302.674, "latencies_ms": [302.674], "images_per_second": 3.304, "prompt_tokens": 757, "response_tokens_est": 48, "n_tiles": 1, "output_text": "\nIn a bathroom, two people are engaged in cleaning. One person is bending over to clean the toilet while another person is standing nearby holding a mop. The room contains various items such as bottles on shelves and a trash can.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.6, "ram_available_mb": 112046.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10454.1, "ram_available_mb": 112052.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.46, 34.46, 29.61, 29.61], "power_watts_avg": 32.03, "power_watts_peak": 34.46, "energy_joules_est": 9.7, "sample_count": 4, "duration_seconds": 0.303}, "timestamp": "2026-01-25T20:03:07.840771"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 94.799, "latencies_ms": [94.799], "images_per_second": 10.549, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "urns of water on the floor and a bucket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.1, "ram_available_mb": 112052.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10456.3, "ram_available_mb": 112050.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [29.61], "power_watts_avg": 29.61, "power_watts_peak": 29.61, "energy_joules_est": 2.82, "sample_count": 1, "duration_seconds": 0.095}, "timestamp": "2026-01-25T20:03:07.945608"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 249.838, "latencies_ms": [249.838], "images_per_second": 4.003, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urn of water on a table, with a man standing in front of it holding an umbrella over his head.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10456.3, "ram_available_mb": 112050.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10455.4, "ram_available_mb": 112050.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [29.61, 30.34, 30.34], "power_watts_avg": 30.1, "power_watts_peak": 30.34, "energy_joules_est": 7.53, "sample_count": 3, "duration_seconds": 0.25}, "timestamp": "2026-01-25T20:03:08.259717"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 72.569, "latencies_ms": [72.569], "images_per_second": 13.78, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Doorway", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.4, "ram_available_mb": 112050.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10456.7, "ram_available_mb": 112049.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [30.34], "power_watts_avg": 30.34, "power_watts_peak": 30.34, "energy_joules_est": 2.21, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T20:03:08.365774"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 166.737, "latencies_ms": [166.737], "images_per_second": 5.997, "prompt_tokens": 763, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n 1. Person holding umbrella 2. Person is standing under umbrella 3. Person is not holding an umbrella", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.7, "ram_available_mb": 112049.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [30.34, 30.34], "power_watts_avg": 30.34, "power_watts_peak": 30.34, "energy_joules_est": 5.07, "sample_count": 2, "duration_seconds": 0.167}, "timestamp": "2026-01-25T20:03:08.571678"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 174.052, "latencies_ms": [174.052], "images_per_second": 5.745, "prompt_tokens": 757, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA man in a blue shirt stands in a hallway holding an umbrella, with a picture on the wall behind him.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10454.4, "ram_available_mb": 112051.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [34.89, 34.89], "power_watts_avg": 34.89, "power_watts_peak": 34.89, "energy_joules_est": 6.09, "sample_count": 2, "duration_seconds": 0.174}, "timestamp": "2026-01-25T20:03:08.778200"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 121.9, "latencies_ms": [121.9], "images_per_second": 8.203, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The man is holding a black umbrella in front of him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.4, "ram_available_mb": 112051.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10454.4, "ram_available_mb": 112051.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [34.89, 34.89], "power_watts_avg": 34.89, "power_watts_peak": 34.89, "energy_joules_est": 4.26, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:03:08.985811"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 268.97, "latencies_ms": [268.97], "images_per_second": 3.718, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA man in a red shirt and blue jeans stands on a rocky path, holding a walking stick and wearing a backpack.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10454.4, "ram_available_mb": 112051.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [34.89, 31.35, 31.35], "power_watts_avg": 32.53, "power_watts_peak": 34.89, "energy_joules_est": 8.76, "sample_count": 3, "duration_seconds": 0.269}, "timestamp": "2026-01-25T20:03:09.299677"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 709.804, "latencies_ms": [709.804], "images_per_second": 1.409, "prompt_tokens": 759, "response_tokens_est": 128, "n_tiles": 1, "output_text": "\n 1. Person in orange shirt and backpack 2. Person in yellow shirt and walking stick 3. Person in red shirt and black backpack 4. Person in blue jeans and black backpack 5. Person in red shirt and black backpack 6. Person in red shirt and black backpack 7. Person in red shirt and black backpack 8. Person in red shirt and black backpack 9. Person in red shirt and black backpack 10. Person in red shirt and black backpack 11. Person in red shirt and black backpack 12. Person in red shirt and black backpack 13. Person in red shirt and black backpack 14. Person in red shirt and black backpack 15.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10455.2, "ram_available_mb": 112051.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 93.0}, "power_stats": {"power_watts_samples": [31.35, 31.35, 31.35, 34.99, 34.99, 34.99, 34.99, 34.99], "power_watts_avg": 33.63, "power_watts_peak": 34.99, "energy_joules_est": 23.88, "sample_count": 8, "duration_seconds": 0.71}, "timestamp": "2026-01-25T20:03:10.108021"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 142.956, "latencies_ms": [142.956], "images_per_second": 6.995, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA man with a backpack and hiking pole is standing in front of a sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.2, "ram_available_mb": 112051.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10455.0, "ram_available_mb": 112051.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 93.0}, "power_stats": {"power_watts_samples": [42.3, 42.3], "power_watts_avg": 42.3, "power_watts_peak": 42.3, "energy_joules_est": 6.07, "sample_count": 2, "duration_seconds": 0.144}, "timestamp": "2026-01-25T20:03:10.313975"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 196.785, "latencies_ms": [196.785], "images_per_second": 5.082, "prompt_tokens": 757, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA man in a red shirt stands on a rocky path, holding a walking stick. Another person wearing an orange jacket is also present nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.0, "ram_available_mb": 112051.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10452.8, "ram_available_mb": 112053.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 93.0}, "power_stats": {"power_watts_samples": [42.3, 42.3], "power_watts_avg": 42.3, "power_watts_peak": 42.3, "energy_joules_est": 8.35, "sample_count": 2, "duration_seconds": 0.197}, "timestamp": "2026-01-25T20:03:10.518519"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 86.827, "latencies_ms": [86.827], "images_per_second": 11.517, "prompt_tokens": 756, "response_tokens_est": 8, "n_tiles": 1, "output_text": "urns of water and rocks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.8, "ram_available_mb": 112053.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10452.2, "ram_available_mb": 112054.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 93.0}, "power_stats": {"power_watts_samples": [42.3], "power_watts_avg": 42.3, "power_watts_peak": 42.3, "energy_joules_est": 3.68, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T20:03:10.623701"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 227.359, "latencies_ms": [227.359], "images_per_second": 4.398, "prompt_tokens": 744, "response_tokens_est": 19, "n_tiles": 1, "output_text": "urns of wine are on a table in front of four men posing for a photo.", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 10452.2, "ram_available_mb": 112054.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10451.2, "ram_available_mb": 112055.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 93.0}, "power_stats": {"power_watts_samples": [35.19, 35.19, 35.19], "power_watts_avg": 35.19, "power_watts_peak": 35.19, "energy_joules_est": 8.01, "sample_count": 3, "duration_seconds": 0.228}, "timestamp": "2026-01-25T20:03:10.937454"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 132.336, "latencies_ms": [132.336], "images_per_second": 7.557, "prompt_tokens": 759, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n Chair 1: 2, Chair 2: 3, Chair 3: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.2, "ram_available_mb": 112055.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 20.0}, "power_stats": {"power_watts_samples": [35.19, 35.19], "power_watts_avg": 35.19, "power_watts_peak": 35.19, "energy_joules_est": 4.68, "sample_count": 2, "duration_seconds": 0.133}, "timestamp": "2026-01-25T20:03:11.143783"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 111.724, "latencies_ms": [111.724], "images_per_second": 8.951, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urns on a table in front of men", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10452.4, "ram_available_mb": 112053.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 20.0}, "power_stats": {"power_watts_samples": [32.8, 32.8], "power_watts_avg": 32.8, "power_watts_peak": 32.8, "energy_joules_est": 3.68, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T20:03:11.350595"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 405.168, "latencies_ms": [405.168], "images_per_second": 2.468, "prompt_tokens": 757, "response_tokens_est": 67, "n_tiles": 1, "output_text": "\n Four men are standing together in a room, posing for a photo. They are all smiling and appear to be enjoying each other's company. The room has several chairs placed around, with one chair being red. There are also wine glasses on the table next to them, suggesting that they might have been drinking or celebrating something.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.4, "ram_available_mb": 112053.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10453.8, "ram_available_mb": 112052.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 20.0}, "power_stats": {"power_watts_samples": [32.8, 32.8, 31.67, 31.67, 31.67], "power_watts_avg": 32.12, "power_watts_peak": 32.8, "energy_joules_est": 13.03, "sample_count": 5, "duration_seconds": 0.406}, "timestamp": "2026-01-25T20:03:11.856933"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 97.033, "latencies_ms": [97.033], "images_per_second": 10.306, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urns of wine on the table.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.8, "ram_available_mb": 112052.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 20.0}, "power_stats": {"power_watts_samples": [31.67], "power_watts_avg": 31.67, "power_watts_peak": 31.67, "energy_joules_est": 3.09, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T20:03:11.962287"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 283.568, "latencies_ms": [283.568], "images_per_second": 3.526, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA yellow and black sign is leaning against a pole on the side of a street, with a building in the background.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [31.67, 29.41, 29.41], "power_watts_avg": 30.17, "power_watts_peak": 31.67, "energy_joules_est": 8.56, "sample_count": 3, "duration_seconds": 0.284}, "timestamp": "2026-01-25T20:03:12.275906"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 97.653, "latencies_ms": [97.653], "images_per_second": 10.24, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Traffic light  3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [29.41], "power_watts_avg": 29.41, "power_watts_peak": 29.41, "energy_joules_est": 2.88, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T20:03:12.380341"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 127.067, "latencies_ms": [127.067], "images_per_second": 7.87, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nA yellow sign on a pole in front of a street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10461.4, "ram_available_mb": 112044.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [29.41, 29.41], "power_watts_avg": 29.41, "power_watts_peak": 29.41, "energy_joules_est": 3.74, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:03:12.584932"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 241.628, "latencies_ms": [241.628], "images_per_second": 4.139, "prompt_tokens": 757, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\nA yellow sign with a black arrow on top of a pole stands in the middle of an empty street. The street has no traffic, and there are no people visible around the area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.4, "ram_available_mb": 112044.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [34.78, 34.78, 34.78], "power_watts_avg": 34.78, "power_watts_peak": 34.78, "energy_joules_est": 8.43, "sample_count": 3, "duration_seconds": 0.242}, "timestamp": "2026-01-25T20:03:12.892758"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 109.17, "latencies_ms": [109.17], "images_per_second": 9.16, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The street is grey and the sky is cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.5, "ram_available_mb": 112042.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10463.8, "ram_available_mb": 112042.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [34.78, 34.78], "power_watts_avg": 34.78, "power_watts_peak": 34.78, "energy_joules_est": 3.82, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T20:03:13.100024"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 296.767, "latencies_ms": [296.767], "images_per_second": 3.37, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA young man in a white shirt and black shorts is playing tennis on an outdoor court, holding his racket up to swing at a ball.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10463.8, "ram_available_mb": 112042.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [34.22, 34.22, 34.22], "power_watts_avg": 34.22, "power_watts_peak": 34.22, "energy_joules_est": 10.17, "sample_count": 3, "duration_seconds": 0.297}, "timestamp": "2026-01-25T20:03:13.414043"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 64.015, "latencies_ms": [64.015], "images_per_second": 15.621, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Fence", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 7.4, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [34.22], "power_watts_avg": 34.22, "power_watts_peak": 34.22, "energy_joules_est": 2.2, "sample_count": 1, "duration_seconds": 0.064}, "timestamp": "2026-01-25T20:03:13.518179"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 121.003, "latencies_ms": [121.003], "images_per_second": 8.264, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nThe tennis player is holding a racket and standing on a court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [34.22, 32.71], "power_watts_avg": 33.46, "power_watts_peak": 34.22, "energy_joules_est": 4.06, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T20:03:13.723098"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 196.729, "latencies_ms": [196.729], "images_per_second": 5.083, "prompt_tokens": 757, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA young man in a white shirt and black shorts is playing tennis on an outdoor court, holding his racket up to swing at a ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 10466.5, "ram_available_mb": 112039.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [32.71, 32.71], "power_watts_avg": 32.71, "power_watts_peak": 32.71, "energy_joules_est": 6.45, "sample_count": 2, "duration_seconds": 0.197}, "timestamp": "2026-01-25T20:03:13.928337"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 237.559, "latencies_ms": [237.559], "images_per_second": 4.209, "prompt_tokens": 756, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\nThe image shows a tennis player on the court wearing a white shirt and black shorts. The player is holding a tennis racket in their right hand while standing near the baseline of the court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.5, "ram_available_mb": 112039.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10469.5, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [32.71, 32.71, 34.31], "power_watts_avg": 33.24, "power_watts_peak": 34.31, "energy_joules_est": 7.9, "sample_count": 3, "duration_seconds": 0.238}, "timestamp": "2026-01-25T20:03:14.233136"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 262.781, "latencies_ms": [262.781], "images_per_second": 3.805, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "iced skis and ski poles are visible in a snowy field, with two people standing next to each other wearing helmets.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10469.5, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10464.8, "ram_available_mb": 112041.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [34.31, 34.31, 34.31], "power_watts_avg": 34.31, "power_watts_peak": 34.31, "energy_joules_est": 9.03, "sample_count": 3, "duration_seconds": 0.263}, "timestamp": "2026-01-25T20:03:14.543004"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 677.843, "latencies_ms": [677.843], "images_per_second": 1.475, "prompt_tokens": 759, "response_tokens_est": 128, "n_tiles": 1, "output_text": "\n 1. Skier in black jacket and red pants 2. Skier in white helmet and gray suit 3. Skier in blue helmet and gray suit 4. Skier in black jacket and red pants 5. Skier in black jacket and gray suit 6. Skier in black jacket and red pants 7. Skier in black jacket and gray suit 8. Skier in black jacket and red pants 9. Skier in black jacket and gray suit 10. Skier in black jacket and red pants 11. Skier in black jacket and gray suit 12. Skier in black jacket and red pants 13. Skier in black jacket", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10464.8, "ram_available_mb": 112041.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.31, 35.67, 35.67, 35.67, 35.67, 35.67, 40.79], "power_watts_avg": 36.2, "power_watts_peak": 40.79, "energy_joules_est": 24.55, "sample_count": 7, "duration_seconds": 0.678}, "timestamp": "2026-01-25T20:03:15.251318"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 116.035, "latencies_ms": [116.035], "images_per_second": 8.618, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe skier in front is wearing a helmet and goggles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10451.9, "ram_available_mb": 112054.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [40.79, 40.79], "power_watts_avg": 40.79, "power_watts_peak": 40.79, "energy_joules_est": 4.74, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T20:03:15.456347"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 301.684, "latencies_ms": [301.684], "images_per_second": 3.315, "prompt_tokens": 757, "response_tokens_est": 50, "n_tiles": 1, "output_text": "\nIn a snowy landscape, three skiers are standing in front of a blue fence. Two of them are wearing red pants while one has on black pants. They appear to be taking a break or preparing for their next run down the slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.9, "ram_available_mb": 112054.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10454.1, "ram_available_mb": 112052.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [40.79, 36.66, 36.66, 36.66], "power_watts_avg": 37.69, "power_watts_peak": 40.79, "energy_joules_est": 11.38, "sample_count": 4, "duration_seconds": 0.302}, "timestamp": "2026-01-25T20:03:15.862565"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 305.484, "latencies_ms": [305.484], "images_per_second": 3.273, "prompt_tokens": 756, "response_tokens_est": 51, "n_tiles": 1, "output_text": "\n The image shows a group of people on skis in the snow. There are two women wearing ski gear and one man standing next to them. They appear to be enjoying their time together as they prepare for skiing or take a break from skiing.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 10454.1, "ram_available_mb": 112052.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10463.7, "ram_available_mb": 112042.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [36.66, 36.66, 33.13, 33.13], "power_watts_avg": 34.9, "power_watts_peak": 36.66, "energy_joules_est": 10.67, "sample_count": 4, "duration_seconds": 0.306}, "timestamp": "2026-01-25T20:03:16.272085"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 296.469, "latencies_ms": [296.469], "images_per_second": 3.373, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA blue semi-truck is driving down a wet street, with other cars in the background and houses visible on either side of the road.", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 10463.7, "ram_available_mb": 112042.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [33.13, 33.13, 33.13], "power_watts_avg": 33.13, "power_watts_peak": 33.13, "energy_joules_est": 9.83, "sample_count": 3, "duration_seconds": 0.297}, "timestamp": "2026-01-25T20:03:16.587194"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.853, "latencies_ms": [82.853], "images_per_second": 12.07, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Truck", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10468.5, "ram_available_mb": 112037.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [37.44], "power_watts_avg": 37.44, "power_watts_peak": 37.44, "energy_joules_est": 3.11, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:03:16.691536"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 132.336, "latencies_ms": [132.336], "images_per_second": 7.556, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA blue semi truck is driving down a street next to some houses.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.5, "ram_available_mb": 112037.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [37.44, 37.44], "power_watts_avg": 37.44, "power_watts_peak": 37.44, "energy_joules_est": 4.96, "sample_count": 2, "duration_seconds": 0.133}, "timestamp": "2026-01-25T20:03:16.897475"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 215.626, "latencies_ms": [215.626], "images_per_second": 4.638, "prompt_tokens": 757, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA blue semi-truck is driving down a wet street, surrounded by houses. The truck has its lights on as it travels along the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [37.44, 37.44, 34.43], "power_watts_avg": 36.44, "power_watts_peak": 37.44, "energy_joules_est": 7.88, "sample_count": 3, "duration_seconds": 0.216}, "timestamp": "2026-01-25T20:03:17.203889"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 128.415, "latencies_ms": [128.415], "images_per_second": 7.787, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n A blue semi-truck is driving down a street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10464.4, "ram_available_mb": 112041.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [34.43, 34.43], "power_watts_avg": 34.43, "power_watts_peak": 34.43, "energy_joules_est": 4.43, "sample_count": 2, "duration_seconds": 0.129}, "timestamp": "2026-01-25T20:03:17.408926"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 241.519, "latencies_ms": [241.519], "images_per_second": 4.14, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA large airplane is taking off from a runway, with its landing gear down and engines running.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10464.4, "ram_available_mb": 112041.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [34.43, 34.43, 30.74], "power_watts_avg": 33.2, "power_watts_peak": 34.43, "energy_joules_est": 8.03, "sample_count": 3, "duration_seconds": 0.242}, "timestamp": "2026-01-25T20:03:17.718820"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 81.761, "latencies_ms": [81.761], "images_per_second": 12.231, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Airplane", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [30.74], "power_watts_avg": 30.74, "power_watts_peak": 30.74, "energy_joules_est": 2.52, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T20:03:17.823896"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 148.654, "latencies_ms": [148.654], "images_per_second": 6.727, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n 1. Airplane in background  \n 2. Airplane in front of camera", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [30.74, 30.74], "power_watts_avg": 30.74, "power_watts_peak": 30.74, "energy_joules_est": 4.58, "sample_count": 2, "duration_seconds": 0.149}, "timestamp": "2026-01-25T20:03:18.031327"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 196.709, "latencies_ms": [196.709], "images_per_second": 5.084, "prompt_tokens": 757, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA large airplane is taking off from a runway, with its landing gear down. The airport has several other airplanes visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10464.6, "ram_available_mb": 112041.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [30.74, 30.73], "power_watts_avg": 30.73, "power_watts_peak": 30.74, "energy_joules_est": 6.06, "sample_count": 2, "duration_seconds": 0.197}, "timestamp": "2026-01-25T20:03:18.237028"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 108.683, "latencies_ms": [108.683], "images_per_second": 9.201, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n A plane is taking off from an airport runway.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.6, "ram_available_mb": 112041.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10464.8, "ram_available_mb": 112041.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [30.73, 30.73], "power_watts_avg": 30.73, "power_watts_peak": 30.73, "energy_joules_est": 3.35, "sample_count": 2, "duration_seconds": 0.109}, "timestamp": "2026-01-25T20:03:18.441668"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 259.55, "latencies_ms": [259.55], "images_per_second": 3.853, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nTwo women stand on a sandy beach, with one holding a tennis racket and the other wearing an orange shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.8, "ram_available_mb": 112041.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10465.0, "ram_available_mb": 112041.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [30.73, 30.73, 31.3], "power_watts_avg": 30.92, "power_watts_peak": 31.3, "energy_joules_est": 8.03, "sample_count": 3, "duration_seconds": 0.26}, "timestamp": "2026-01-25T20:03:18.752368"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 153.763, "latencies_ms": [153.763], "images_per_second": 6.504, "prompt_tokens": 759, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n 1. American flag  2. Beach umbrella  3. Car  4. Tennis racket", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.0, "ram_available_mb": 112041.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10464.6, "ram_available_mb": 112041.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [31.3, 31.3], "power_watts_avg": 31.3, "power_watts_peak": 31.3, "energy_joules_est": 4.82, "sample_count": 2, "duration_seconds": 0.154}, "timestamp": "2026-01-25T20:03:18.958586"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 112.657, "latencies_ms": [112.657], "images_per_second": 8.877, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe woman holding a tennis racket is in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.6, "ram_available_mb": 112041.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10464.4, "ram_available_mb": 112041.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [31.3, 33.79], "power_watts_avg": 32.54, "power_watts_peak": 33.79, "energy_joules_est": 3.69, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T20:03:19.165163"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 222.208, "latencies_ms": [222.208], "images_per_second": 4.5, "prompt_tokens": 757, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nA woman in an orange shirt stands on a sandy beach, holding a tennis racket. A group of people are walking nearby, with one person carrying a blue bag.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.4, "ram_available_mb": 112041.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10468.3, "ram_available_mb": 112038.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 7.0}, "power_stats": {"power_watts_samples": [33.79, 33.79, 33.79], "power_watts_avg": 33.79, "power_watts_peak": 33.79, "energy_joules_est": 7.53, "sample_count": 3, "duration_seconds": 0.223}, "timestamp": "2026-01-25T20:03:19.471151"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 505.335, "latencies_ms": [505.335], "images_per_second": 1.979, "prompt_tokens": 756, "response_tokens_est": 90, "n_tiles": 1, "output_text": "\n The image shows a group of people walking on the beach. One person is holding a tennis racket and appears to be angry or upset about something. There are several other individuals in the scene as well, some carrying handbags. The colors in the photo are predominantly browns and grays, with some reds and blues present. The lighting seems to be coming from the sun, creating shadows on the sand and highlighting the people's faces.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.3, "ram_available_mb": 112038.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10469.5, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [33.79, 31.06, 31.06, 31.06, 31.06, 31.06], "power_watts_avg": 31.51, "power_watts_peak": 33.79, "energy_joules_est": 15.94, "sample_count": 6, "duration_seconds": 0.506}, "timestamp": "2026-01-25T20:03:20.080706"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 278.597, "latencies_ms": [278.597], "images_per_second": 3.589, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA tennis player, dressed in a blue shirt and white shorts, stands on a blue court with his racket at the ready.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10469.5, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [33.66, 33.66, 33.66], "power_watts_avg": 33.66, "power_watts_peak": 33.66, "energy_joules_est": 9.39, "sample_count": 3, "duration_seconds": 0.279}, "timestamp": "2026-01-25T20:03:20.391714"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 87.274, "latencies_ms": [87.274], "images_per_second": 11.458, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Racket", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [33.66], "power_watts_avg": 33.66, "power_watts_peak": 33.66, "energy_joules_est": 2.95, "sample_count": 1, "duration_seconds": 0.088}, "timestamp": "2026-01-25T20:03:20.497426"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 126.828, "latencies_ms": [126.828], "images_per_second": 7.885, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nThe man is holding a tennis racket and ball on his right hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [33.66, 37.66], "power_watts_avg": 35.66, "power_watts_peak": 37.66, "energy_joules_est": 4.53, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:03:20.702422"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 237.979, "latencies_ms": [237.979], "images_per_second": 4.202, "prompt_tokens": 757, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA tennis player in a blue shirt stands on a blue court, holding a tennis racket. The player appears to be waiting for their opponent or preparing for an upcoming match.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [37.66, 37.66, 37.66], "power_watts_avg": 37.66, "power_watts_peak": 37.66, "energy_joules_est": 8.99, "sample_count": 3, "duration_seconds": 0.239}, "timestamp": "2026-01-25T20:03:21.008931"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 108.696, "latencies_ms": [108.696], "images_per_second": 9.2, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The tennis player is wearing a blue shirt and white shorts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [37.66, 32.64], "power_watts_avg": 35.15, "power_watts_peak": 37.66, "energy_joules_est": 3.83, "sample_count": 2, "duration_seconds": 0.109}, "timestamp": "2026-01-25T20:03:21.215213"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 261.613, "latencies_ms": [261.613], "images_per_second": 3.822, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urns of water and tea on a stove in a kitchen, with a woman standing over it.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [32.64, 32.64, 32.64], "power_watts_avg": 32.64, "power_watts_peak": 32.64, "energy_joules_est": 8.56, "sample_count": 3, "duration_seconds": 0.262}, "timestamp": "2026-01-25T20:03:21.528320"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 299.817, "latencies_ms": [299.817], "images_per_second": 3.335, "prompt_tokens": 759, "response_tokens_est": 49, "n_tiles": 1, "output_text": "\n 1. Oven door  2. Oven door  3. Oven door  4. Oven door  5. Oven door  6. Oven door  7. Oven door  8. Oven door", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10469.4, "ram_available_mb": 112036.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [32.64, 32.51, 32.51], "power_watts_avg": 32.55, "power_watts_peak": 32.64, "energy_joules_est": 9.78, "sample_count": 3, "duration_seconds": 0.3}, "timestamp": "2026-01-25T20:03:21.833872"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 72.488, "latencies_ms": [72.488], "images_per_second": 13.795, "prompt_tokens": 763, "response_tokens_est": 5, "n_tiles": 1, "output_text": "urn on stove top", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10469.4, "ram_available_mb": 112036.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [32.51], "power_watts_avg": 32.51, "power_watts_peak": 32.51, "energy_joules_est": 2.36, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T20:03:21.939609"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 214.767, "latencies_ms": [214.767], "images_per_second": 4.656, "prompt_tokens": 757, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA woman in a pink shirt stands at an old-fashioned stove, possibly cooking or cleaning. The kitchen features wooden cabinets and a black oven above the stove.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.4, "ram_available_mb": 112036.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 22.0}, "power_stats": {"power_watts_samples": [32.51, 32.51, 35.02], "power_watts_avg": 33.35, "power_watts_peak": 35.02, "energy_joules_est": 7.18, "sample_count": 3, "duration_seconds": 0.215}, "timestamp": "2026-01-25T20:03:22.246735"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 104.849, "latencies_ms": [104.849], "images_per_second": 9.538, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The woman is wearing a pink shirt and tan shorts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 22.0}, "power_stats": {"power_watts_samples": [35.02, 35.02], "power_watts_avg": 35.02, "power_watts_peak": 35.02, "energy_joules_est": 3.71, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T20:03:22.454267"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 254.602, "latencies_ms": [254.602], "images_per_second": 3.928, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nTwo giraffes stand in front of a building, with their long necks reaching towards the sky.", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 22.0}, "power_stats": {"power_watts_samples": [35.02, 31.57, 31.57], "power_watts_avg": 32.72, "power_watts_peak": 35.02, "energy_joules_est": 8.36, "sample_count": 3, "duration_seconds": 0.256}, "timestamp": "2026-01-25T20:03:22.769087"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 81.621, "latencies_ms": [81.621], "images_per_second": 12.252, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Tree", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10461.8, "ram_available_mb": 112044.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 22.0}, "power_stats": {"power_watts_samples": [31.57], "power_watts_avg": 31.57, "power_watts_peak": 31.57, "energy_joules_est": 2.58, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T20:03:22.874294"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 142.264, "latencies_ms": [142.264], "images_per_second": 7.029, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. Giraffe on left is taller than giraffe on right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.1, "ram_available_mb": 112044.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10465.5, "ram_available_mb": 112040.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 68.0}, "power_stats": {"power_watts_samples": [31.57, 31.57], "power_watts_avg": 31.57, "power_watts_peak": 31.57, "energy_joules_est": 4.5, "sample_count": 2, "duration_seconds": 0.143}, "timestamp": "2026-01-25T20:03:23.080853"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 245.735, "latencies_ms": [245.735], "images_per_second": 4.069, "prompt_tokens": 757, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nTwo giraffes stand in a zoo enclosure, one facing the camera while the other faces away. The giraffes are standing on a rocky area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.5, "ram_available_mb": 112040.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 68.0}, "power_stats": {"power_watts_samples": [31.67, 31.67, 31.67], "power_watts_avg": 31.67, "power_watts_peak": 31.67, "energy_joules_est": 7.79, "sample_count": 3, "duration_seconds": 0.246}, "timestamp": "2026-01-25T20:03:23.387636"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 135.913, "latencies_ms": [135.913], "images_per_second": 7.358, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n The giraffes are standing in front of a brown building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10466.8, "ram_available_mb": 112039.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 68.0}, "power_stats": {"power_watts_samples": [31.67, 31.67], "power_watts_avg": 31.67, "power_watts_peak": 31.67, "energy_joules_est": 4.32, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T20:03:23.593937"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 208.285, "latencies_ms": [208.285], "images_per_second": 4.801, "prompt_tokens": 744, "response_tokens_est": 15, "n_tiles": 1, "output_text": "urn of water on a table, with a baseball in front of it", "error": null, "sys_before": {"cpu_percent": 3.2, "ram_used_mb": 10466.8, "ram_available_mb": 112039.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 68.0}, "power_stats": {"power_watts_samples": [32.88, 32.88, 32.88], "power_watts_avg": 32.88, "power_watts_peak": 32.88, "energy_joules_est": 6.87, "sample_count": 3, "duration_seconds": 0.209}, "timestamp": "2026-01-25T20:03:23.911602"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 81.525, "latencies_ms": [81.525], "images_per_second": 12.266, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Fence - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10456.5, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 68.0}, "power_stats": {"power_watts_samples": [32.88], "power_watts_avg": 32.88, "power_watts_peak": 32.88, "energy_joules_est": 2.71, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T20:03:24.016590"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 117.876, "latencies_ms": [117.876], "images_per_second": 8.483, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. A boy swinging a bat at a ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.5, "ram_available_mb": 112049.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10450.1, "ram_available_mb": 112056.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [32.88, 30.72], "power_watts_avg": 31.8, "power_watts_peak": 32.88, "energy_joules_est": 3.76, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:03:24.221657"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 200.485, "latencies_ms": [200.485], "images_per_second": 4.988, "prompt_tokens": 757, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA young boy in a green shirt, gray pants, and black helmet stands at home plate holding a baseball bat, ready to hit an incoming ball.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10450.4, "ram_available_mb": 112055.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10443.6, "ram_available_mb": 112062.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [30.72, 30.72, 30.72], "power_watts_avg": 30.72, "power_watts_peak": 30.72, "energy_joules_est": 6.17, "sample_count": 3, "duration_seconds": 0.201}, "timestamp": "2026-01-25T20:03:24.527189"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 318.078, "latencies_ms": [318.078], "images_per_second": 3.144, "prompt_tokens": 756, "response_tokens_est": 54, "n_tiles": 1, "output_text": "\n The image shows a young boy swinging a baseball bat at a ball. He is wearing a green shirt and gray pants while holding the bat with both hands. In the background, there are two cars parked behind the fence, one of which appears to be white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10443.6, "ram_available_mb": 112062.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10444.2, "ram_available_mb": 112062.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [30.72, 29.54, 29.54, 29.54], "power_watts_avg": 29.84, "power_watts_peak": 30.72, "energy_joules_est": 9.5, "sample_count": 4, "duration_seconds": 0.318}, "timestamp": "2026-01-25T20:03:24.933769"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 281.926, "latencies_ms": [281.926], "images_per_second": 3.547, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA group of old cars and a bus are parked in front of a brick building, with people walking around on the street.", "error": null, "sys_before": {"cpu_percent": 11.8, "ram_used_mb": 10444.2, "ram_available_mb": 112062.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10447.2, "ram_available_mb": 112059.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 22.0}, "power_stats": {"power_watts_samples": [29.54, 29.54, 32.5], "power_watts_avg": 30.53, "power_watts_peak": 32.5, "energy_joules_est": 8.62, "sample_count": 3, "duration_seconds": 0.282}, "timestamp": "2026-01-25T20:03:25.244541"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 75.45, "latencies_ms": [75.45], "images_per_second": 13.254, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Bus", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.4, "ram_available_mb": 112058.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10451.6, "ram_available_mb": 112054.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 22.0}, "power_stats": {"power_watts_samples": [32.5], "power_watts_avg": 32.5, "power_watts_peak": 32.5, "energy_joules_est": 2.46, "sample_count": 1, "duration_seconds": 0.076}, "timestamp": "2026-01-25T20:03:25.350020"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 169.922, "latencies_ms": [169.922], "images_per_second": 5.885, "prompt_tokens": 763, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n 1. A blue car is parked next to a red bus and several motorcycles in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.6, "ram_available_mb": 112054.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10451.6, "ram_available_mb": 112054.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 22.0}, "power_stats": {"power_watts_samples": [32.5, 32.5], "power_watts_avg": 32.5, "power_watts_peak": 32.5, "energy_joules_est": 5.53, "sample_count": 2, "duration_seconds": 0.17}, "timestamp": "2026-01-25T20:03:25.555333"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 330.929, "latencies_ms": [330.929], "images_per_second": 3.022, "prompt_tokens": 757, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\n A group of old cars, including a bus, are parked on a cobblestone street. The vehicles appear to be in good condition and are displayed for people to admire. There are also several people walking around the area, possibly admiring or discussing the vintage cars.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.6, "ram_available_mb": 112054.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 22.0}, "power_stats": {"power_watts_samples": [35.99, 35.99, 35.99, 35.99], "power_watts_avg": 35.99, "power_watts_peak": 35.99, "energy_joules_est": 11.92, "sample_count": 4, "duration_seconds": 0.331}, "timestamp": "2026-01-25T20:03:25.960391"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 142.782, "latencies_ms": [142.782], "images_per_second": 7.004, "prompt_tokens": 756, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n 1. A red bus parked in front of a group of old cars and motorcycles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.7, "ram_available_mb": 112050.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [35.99, 34.76], "power_watts_avg": 35.37, "power_watts_peak": 35.99, "energy_joules_est": 5.07, "sample_count": 2, "duration_seconds": 0.143}, "timestamp": "2026-01-25T20:03:26.166725"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 298.116, "latencies_ms": [298.116], "images_per_second": 3.354, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nTwo black parking meters stand side by side on a city street, with their tops facing each other and a building visible in the background.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [34.76, 34.76, 34.76], "power_watts_avg": 34.76, "power_watts_peak": 34.76, "energy_joules_est": 10.37, "sample_count": 3, "duration_seconds": 0.298}, "timestamp": "2026-01-25T20:03:26.476012"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 105.532, "latencies_ms": [105.532], "images_per_second": 9.476, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Parking meter closest to camera - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10460.9, "ram_available_mb": 112045.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [34.76, 36.03], "power_watts_avg": 35.39, "power_watts_peak": 36.03, "energy_joules_est": 3.74, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T20:03:26.680902"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 80.312, "latencies_ms": [80.312], "images_per_second": 12.451, "prompt_tokens": 763, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Right parking meter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.9, "ram_available_mb": 112045.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10462.6, "ram_available_mb": 112043.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [36.03], "power_watts_avg": 36.03, "power_watts_peak": 36.03, "energy_joules_est": 2.9, "sample_count": 1, "duration_seconds": 0.081}, "timestamp": "2026-01-25T20:03:26.785683"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 321.854, "latencies_ms": [321.854], "images_per_second": 3.107, "prompt_tokens": 757, "response_tokens_est": 54, "n_tiles": 1, "output_text": "\n Two black parking meters are attached to a pole, with one of them displaying the time. The meter on the left has its handle extended upwards, while the other one remains closed. In the background, there's a cityscape featuring buildings under a sunset sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.6, "ram_available_mb": 112043.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.03, 36.03, 36.03, 34.72], "power_watts_avg": 35.7, "power_watts_peak": 36.03, "energy_joules_est": 11.51, "sample_count": 4, "duration_seconds": 0.322}, "timestamp": "2026-01-25T20:03:27.191381"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 124.714, "latencies_ms": [124.714], "images_per_second": 8.018, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n The sun is setting and the sky has a warm orange hue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.72, 34.72], "power_watts_avg": 34.72, "power_watts_peak": 34.72, "energy_joules_est": 4.36, "sample_count": 2, "duration_seconds": 0.125}, "timestamp": "2026-01-25T20:03:27.397926"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 222.774, "latencies_ms": [222.774], "images_per_second": 4.489, "prompt_tokens": 744, "response_tokens_est": 17, "n_tiles": 1, "output_text": "urn of a suitcase with stickers from different countries, including India and Thailand.\n", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10467.8, "ram_available_mb": 112038.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.72, 34.72, 32.27], "power_watts_avg": 33.9, "power_watts_peak": 34.72, "energy_joules_est": 7.58, "sample_count": 3, "duration_seconds": 0.224}, "timestamp": "2026-01-25T20:03:27.711319"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 102.365, "latencies_ms": [102.365], "images_per_second": 9.769, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Brown suitcase with white stickers - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.8, "ram_available_mb": 112038.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.27, 32.27], "power_watts_avg": 32.27, "power_watts_peak": 32.27, "energy_joules_est": 3.32, "sample_count": 2, "duration_seconds": 0.103}, "timestamp": "2026-01-25T20:03:27.917907"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 117.356, "latencies_ms": [117.356], "images_per_second": 8.521, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nA couple standing next to a suitcase with travel stickers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [32.27, 32.27], "power_watts_avg": 32.27, "power_watts_peak": 32.27, "energy_joules_est": 3.8, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:03:28.124077"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 263.939, "latencies_ms": [263.939], "images_per_second": 3.789, "prompt_tokens": 757, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\nA couple stands next to a large suitcase with various stickers, including one that says \"INDIA\". They are in front of a building labeled \"FEDLITY\" and a statue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10456.2, "ram_available_mb": 112050.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [30.04, 30.04, 30.04], "power_watts_avg": 30.04, "power_watts_peak": 30.04, "energy_joules_est": 7.95, "sample_count": 3, "duration_seconds": 0.265}, "timestamp": "2026-01-25T20:03:28.430813"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 133.988, "latencies_ms": [133.988], "images_per_second": 7.463, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n The couple is standing in front of a suitcase with stickers on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.2, "ram_available_mb": 112050.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10458.4, "ram_available_mb": 112047.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [30.04, 30.04], "power_watts_avg": 30.04, "power_watts_peak": 30.04, "energy_joules_est": 4.03, "sample_count": 2, "duration_seconds": 0.134}, "timestamp": "2026-01-25T20:03:28.637830"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 280.125, "latencies_ms": [280.125], "images_per_second": 3.57, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA white plate holds a meal of mushrooms, broccoli, and chicken. The dish is garnished with parsley on top.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 38.0}, "power_stats": {"power_watts_samples": [27.92, 27.92, 27.92], "power_watts_avg": 27.92, "power_watts_peak": 27.92, "energy_joules_est": 7.83, "sample_count": 3, "duration_seconds": 0.28}, "timestamp": "2026-01-25T20:03:28.950667"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 94.314, "latencies_ms": [94.314], "images_per_second": 10.603, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Mushrooms and broccoli", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10462.3, "ram_available_mb": 112044.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [27.92], "power_watts_avg": 27.92, "power_watts_peak": 27.92, "energy_joules_est": 2.64, "sample_count": 1, "duration_seconds": 0.095}, "timestamp": "2026-01-25T20:03:29.056708"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 104.601, "latencies_ms": [104.601], "images_per_second": 9.56, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n A plate of food with mushrooms and broccoli.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.3, "ram_available_mb": 112044.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [33.3, 33.3], "power_watts_avg": 33.3, "power_watts_peak": 33.3, "energy_joules_est": 3.49, "sample_count": 2, "duration_seconds": 0.105}, "timestamp": "2026-01-25T20:03:29.261844"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 373.696, "latencies_ms": [373.696], "images_per_second": 2.676, "prompt_tokens": 757, "response_tokens_est": 66, "n_tiles": 1, "output_text": "\nThe image shows a plate of food with mushrooms, broccoli, and meat. The dish appears to be a well-prepared meal that includes various ingredients such as mushrooms, broccoli, and possibly chicken or pork. The vegetables are arranged on the plate in an appetizing manner, while the meat is placed next to them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10453.4, "ram_available_mb": 112052.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [33.3, 33.3, 33.3, 34.5], "power_watts_avg": 33.6, "power_watts_peak": 34.5, "energy_joules_est": 12.57, "sample_count": 4, "duration_seconds": 0.374}, "timestamp": "2026-01-25T20:03:29.670529"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 39.174, "latencies_ms": [39.174], "images_per_second": 25.527, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.4, "ram_available_mb": 112052.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 10453.8, "ram_available_mb": 112052.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [34.5], "power_watts_avg": 34.5, "power_watts_peak": 34.5, "energy_joules_est": 1.36, "sample_count": 1, "duration_seconds": 0.039}, "timestamp": "2026-01-25T20:03:29.776201"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 241.046, "latencies_ms": [241.046], "images_per_second": 4.149, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "iced tea and carrots are displayed in a market, with the carrots being the main focus of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.8, "ram_available_mb": 112052.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10454.3, "ram_available_mb": 112052.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.5, 34.5, 34.5], "power_watts_avg": 34.5, "power_watts_peak": 34.5, "energy_joules_est": 8.33, "sample_count": 3, "duration_seconds": 0.241}, "timestamp": "2026-01-25T20:03:30.085808"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 81.384, "latencies_ms": [81.384], "images_per_second": 12.287, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Carrots", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.3, "ram_available_mb": 112052.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10450.2, "ram_available_mb": 112056.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.87], "power_watts_avg": 34.87, "power_watts_peak": 34.87, "energy_joules_est": 2.86, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T20:03:30.190855"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 111.364, "latencies_ms": [111.364], "images_per_second": 8.98, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Carrots on left side of basket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.2, "ram_available_mb": 112056.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10450.0, "ram_available_mb": 112056.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.87, 34.87], "power_watts_avg": 34.87, "power_watts_peak": 34.87, "energy_joules_est": 3.89, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T20:03:30.397041"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 252.042, "latencies_ms": [252.042], "images_per_second": 3.968, "prompt_tokens": 757, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\nA wooden display case holds a variety of fresh vegetables, including carrots in different sizes. The carrots are arranged neatly on shelves within the display case, showcasing their vibrant colors against the wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.0, "ram_available_mb": 112056.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10448.0, "ram_available_mb": 112058.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.87, 34.87, 31.55], "power_watts_avg": 33.77, "power_watts_peak": 34.87, "energy_joules_est": 8.53, "sample_count": 3, "duration_seconds": 0.253}, "timestamp": "2026-01-25T20:03:30.704134"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 124.86, "latencies_ms": [124.86], "images_per_second": 8.009, "prompt_tokens": 756, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n A basket of carrots and a bunch of broccoli are displayed on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.0, "ram_available_mb": 112058.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10447.1, "ram_available_mb": 112059.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.55, 31.55], "power_watts_avg": 31.55, "power_watts_peak": 31.55, "energy_joules_est": 3.96, "sample_count": 2, "duration_seconds": 0.125}, "timestamp": "2026-01-25T20:03:30.911023"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 256.619, "latencies_ms": [256.619], "images_per_second": 3.897, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "iced doughnuts are being made in a factory, with workers standing behind conveyor belts and other equipment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.1, "ram_available_mb": 112059.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [31.55, 31.55, 30.0], "power_watts_avg": 31.04, "power_watts_peak": 31.55, "energy_joules_est": 7.98, "sample_count": 3, "duration_seconds": 0.257}, "timestamp": "2026-01-25T20:03:31.223083"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 91.761, "latencies_ms": [91.761], "images_per_second": 10.898, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Doughnuts: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10448.9, "ram_available_mb": 112057.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [30.0], "power_watts_avg": 30.0, "power_watts_peak": 30.0, "energy_joules_est": 2.77, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T20:03:31.328257"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 129.052, "latencies_ms": [129.052], "images_per_second": 7.749, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A donut machine with a red fire extinguisher behind it.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10448.9, "ram_available_mb": 112057.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10450.1, "ram_available_mb": 112056.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [30.0, 30.0], "power_watts_avg": 30.0, "power_watts_peak": 30.0, "energy_joules_est": 3.88, "sample_count": 2, "duration_seconds": 0.129}, "timestamp": "2026-01-25T20:03:31.533210"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 132.504, "latencies_ms": [132.504], "images_per_second": 7.547, "prompt_tokens": 757, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nA doughnut factory with a conveyor belt of freshly made doughnuts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.1, "ram_available_mb": 112056.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10453.3, "ram_available_mb": 112053.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [30.0, 32.25], "power_watts_avg": 31.13, "power_watts_peak": 32.25, "energy_joules_est": 4.14, "sample_count": 2, "duration_seconds": 0.133}, "timestamp": "2026-01-25T20:03:31.738547"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 105.057, "latencies_ms": [105.057], "images_per_second": 9.519, "prompt_tokens": 756, "response_tokens_est": 11, "n_tiles": 1, "output_text": "urns of oil and water on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.3, "ram_available_mb": 112053.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10457.7, "ram_available_mb": 112048.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [32.25, 32.25], "power_watts_avg": 32.25, "power_watts_peak": 32.25, "energy_joules_est": 3.41, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T20:03:31.946431"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 302.781, "latencies_ms": [302.781], "images_per_second": 3.303, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA man in a green jacket and white socks is playing frisbee in a forest clearing, with his arm extended to catch it.", "error": null, "sys_before": {"cpu_percent": 3.6, "ram_used_mb": 10457.7, "ram_available_mb": 112048.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10458.5, "ram_available_mb": 112047.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [32.25, 27.78, 27.78, 27.78], "power_watts_avg": 28.9, "power_watts_peak": 32.25, "energy_joules_est": 8.77, "sample_count": 4, "duration_seconds": 0.303}, "timestamp": "2026-01-25T20:03:32.363864"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 86.387, "latencies_ms": [86.387], "images_per_second": 11.576, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Tree trunk", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.5, "ram_available_mb": 112047.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10462.2, "ram_available_mb": 112044.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [27.78], "power_watts_avg": 27.78, "power_watts_peak": 27.78, "energy_joules_est": 2.42, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T20:03:32.468968"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 143.019, "latencies_ms": [143.019], "images_per_second": 6.992, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA man in a green shirt is throwing a frisbee in a forest.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.2, "ram_available_mb": 112044.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [27.78, 29.94], "power_watts_avg": 28.86, "power_watts_peak": 29.94, "energy_joules_est": 4.14, "sample_count": 2, "duration_seconds": 0.143}, "timestamp": "2026-01-25T20:03:32.673207"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 145.163, "latencies_ms": [145.163], "images_per_second": 6.889, "prompt_tokens": 757, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nA man in a green shirt and white socks is playing frisbee in a forest.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [29.94, 29.94], "power_watts_avg": 29.94, "power_watts_peak": 29.94, "energy_joules_est": 4.36, "sample_count": 2, "duration_seconds": 0.145}, "timestamp": "2026-01-25T20:03:32.878760"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 486.011, "latencies_ms": [486.011], "images_per_second": 2.058, "prompt_tokens": 756, "response_tokens_est": 85, "n_tiles": 1, "output_text": "\n The image shows a man in the woods holding a frisbee. He is wearing a green shirt and white socks. The frisbee appears to be white with orange accents on it. The trees surrounding him are tall and leafless, suggesting that they might be deciduous or coniferous trees. The lighting seems to be coming from above, casting shadows and highlighting the textures of the forest floor.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10457.2, "ram_available_mb": 112049.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_watts_samples": [29.94, 29.94, 32.26, 32.26, 32.26], "power_watts_avg": 31.33, "power_watts_peak": 32.26, "energy_joules_est": 15.24, "sample_count": 5, "duration_seconds": 0.486}, "timestamp": "2026-01-25T20:03:33.385840"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 248.083, "latencies_ms": [248.083], "images_per_second": 4.031, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urns of blue and green flowers sit on a bathroom counter, with a can of cleaning product nearby.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10457.4, "ram_available_mb": 112048.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10455.6, "ram_available_mb": 112050.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_watts_samples": [32.26, 32.26, 37.94], "power_watts_avg": 34.15, "power_watts_peak": 37.94, "energy_joules_est": 8.49, "sample_count": 3, "duration_seconds": 0.249}, "timestamp": "2026-01-25T20:03:33.696006"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 91.62, "latencies_ms": [91.62], "images_per_second": 10.915, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Shower curtain - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.6, "ram_available_mb": 112050.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10450.0, "ram_available_mb": 112056.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_watts_samples": [37.94], "power_watts_avg": 37.94, "power_watts_peak": 37.94, "energy_joules_est": 3.51, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:03:33.800631"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 115.31, "latencies_ms": [115.31], "images_per_second": 8.672, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Shower curtain - right side of bathroom", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.0, "ram_available_mb": 112056.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10450.8, "ram_available_mb": 112055.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_watts_samples": [37.94, 37.94], "power_watts_avg": 37.94, "power_watts_peak": 37.94, "energy_joules_est": 4.38, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T20:03:34.006876"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 269.907, "latencies_ms": [269.907], "images_per_second": 3.705, "prompt_tokens": 757, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\nThe image shows a small bathroom with beige tiles on the floor. The bathroom features a white sink, toilet, and shower. A blue can of cleaning product sits on the counter next to the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10450.8, "ram_available_mb": 112055.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10451.6, "ram_available_mb": 112054.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 9.0}, "power_stats": {"power_watts_samples": [37.94, 35.63, 35.63], "power_watts_avg": 36.4, "power_watts_peak": 37.94, "energy_joules_est": 9.85, "sample_count": 3, "duration_seconds": 0.271}, "timestamp": "2026-01-25T20:03:34.314066"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 225.226, "latencies_ms": [225.226], "images_per_second": 4.44, "prompt_tokens": 756, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nThe bathroom is white and has a gray door. The shower curtain is blue with stripes. There are two bottles of shampoo on the countertop next to the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.6, "ram_available_mb": 112054.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10455.6, "ram_available_mb": 112050.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 9.0}, "power_stats": {"power_watts_samples": [35.63, 35.63, 35.63], "power_watts_avg": 35.63, "power_watts_peak": 35.63, "energy_joules_est": 8.04, "sample_count": 3, "duration_seconds": 0.226}, "timestamp": "2026-01-25T20:03:34.620974"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 249.475, "latencies_ms": [249.475], "images_per_second": 4.008, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA kitchen with a black countertop and wooden cabinets, featuring a sink and faucet.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 10455.6, "ram_available_mb": 112050.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10459.2, "ram_available_mb": 112047.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 9.0}, "power_stats": {"power_watts_samples": [31.99, 31.99, 31.99], "power_watts_avg": 31.99, "power_watts_peak": 31.99, "energy_joules_est": 8.0, "sample_count": 3, "duration_seconds": 0.25}, "timestamp": "2026-01-25T20:03:34.929807"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 264.845, "latencies_ms": [264.845], "images_per_second": 3.776, "prompt_tokens": 759, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\n 1. Cabinet door  2. Cabinet door  3. Cabinet door  4. Cabinet door  5. Cabinet door  6. Cabinet door  7. Cabinet door  8. Cabinet door", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10459.2, "ram_available_mb": 112047.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10452.4, "ram_available_mb": 112053.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 24.0}, "power_stats": {"power_watts_samples": [31.99, 31.99, 36.44], "power_watts_avg": 33.47, "power_watts_peak": 36.44, "energy_joules_est": 8.88, "sample_count": 3, "duration_seconds": 0.265}, "timestamp": "2026-01-25T20:03:35.236388"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 111.073, "latencies_ms": [111.073], "images_per_second": 9.003, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n A kitchen counter with a sink and faucet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.4, "ram_available_mb": 112053.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10456.6, "ram_available_mb": 112049.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 24.0}, "power_stats": {"power_watts_samples": [36.44, 36.44], "power_watts_avg": 36.44, "power_watts_peak": 36.44, "energy_joules_est": 4.07, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T20:03:35.442442"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 193.821, "latencies_ms": [193.821], "images_per_second": 5.159, "prompt_tokens": 757, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA kitchen with a black countertop, wooden cabinets, and a sink. The countertop has a bar area in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.6, "ram_available_mb": 112049.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 24.0}, "power_stats": {"power_watts_samples": [36.44, 36.44], "power_watts_avg": 36.44, "power_watts_peak": 36.44, "energy_joules_est": 7.08, "sample_count": 2, "duration_seconds": 0.194}, "timestamp": "2026-01-25T20:03:35.647877"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 112.634, "latencies_ms": [112.634], "images_per_second": 8.878, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The kitchen has a black countertop and wooden cabinets.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 24.0}, "power_stats": {"power_watts_samples": [32.77, 32.77], "power_watts_avg": 32.77, "power_watts_peak": 32.77, "energy_joules_est": 3.7, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T20:03:35.852484"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 255.824, "latencies_ms": [255.824], "images_per_second": 3.909, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urn of flowers is on a bed with a white pillow and blue blanket, creating an atmosphere of tranquility.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10461.0, "ram_available_mb": 112045.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [32.77, 32.77, 33.84], "power_watts_avg": 33.13, "power_watts_peak": 33.84, "energy_joules_est": 8.5, "sample_count": 3, "duration_seconds": 0.257}, "timestamp": "2026-01-25T20:03:36.162552"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 75.407, "latencies_ms": [75.407], "images_per_second": 13.261, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bed: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.0, "ram_available_mb": 112045.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [33.84], "power_watts_avg": 33.84, "power_watts_peak": 33.84, "energy_joules_est": 2.57, "sample_count": 1, "duration_seconds": 0.076}, "timestamp": "2026-01-25T20:03:36.268310"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 131.146, "latencies_ms": [131.146], "images_per_second": 7.625, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA person is sleeping on a bed with a floral comforter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10462.1, "ram_available_mb": 112044.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [33.84, 33.84], "power_watts_avg": 33.84, "power_watts_peak": 33.84, "energy_joules_est": 4.46, "sample_count": 2, "duration_seconds": 0.132}, "timestamp": "2026-01-25T20:03:36.475305"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 225.161, "latencies_ms": [225.161], "images_per_second": 4.441, "prompt_tokens": 757, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA person is sleeping on a bed with a black and white floral comforter. The bed has a blue blanket, and there are two pillows on the bed.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10462.1, "ram_available_mb": 112044.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [33.84, 31.99, 31.99], "power_watts_avg": 32.61, "power_watts_peak": 33.84, "energy_joules_est": 7.36, "sample_count": 3, "duration_seconds": 0.226}, "timestamp": "2026-01-25T20:03:36.782043"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 401.115, "latencies_ms": [401.115], "images_per_second": 2.493, "prompt_tokens": 756, "response_tokens_est": 70, "n_tiles": 1, "output_text": "\n The image shows a person sleeping on an unmade bed with a black and white patterned comforter. The bed is covered in a floral-patterned blanket that has been partially pulled down to cover the person's head. The room appears dark, which might suggest it could be nighttime or that there are no other lights present.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10469.7, "ram_available_mb": 112036.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.99, 31.99, 31.99, 31.28], "power_watts_avg": 31.81, "power_watts_peak": 31.99, "energy_joules_est": 12.78, "sample_count": 4, "duration_seconds": 0.402}, "timestamp": "2026-01-25T20:03:37.188261"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 280.735, "latencies_ms": [280.735], "images_per_second": 3.562, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA skateboarder is performing a trick on a ramp, with another person watching from behind a wall in the background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 10469.7, "ram_available_mb": 112036.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10470.4, "ram_available_mb": 112035.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.28, 31.28, 31.28], "power_watts_avg": 31.28, "power_watts_peak": 31.28, "energy_joules_est": 8.79, "sample_count": 3, "duration_seconds": 0.281}, "timestamp": "2026-01-25T20:03:37.497504"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 110.15, "latencies_ms": [110.15], "images_per_second": 9.079, "prompt_tokens": 759, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Skateboarder's shadow", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.4, "ram_available_mb": 112035.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.28, 38.75], "power_watts_avg": 35.01, "power_watts_peak": 38.75, "energy_joules_est": 3.88, "sample_count": 2, "duration_seconds": 0.111}, "timestamp": "2026-01-25T20:03:37.703261"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 185.916, "latencies_ms": [185.916], "images_per_second": 5.379, "prompt_tokens": 763, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\n 1. Person on skateboard in background  2. Person in background  3. Skateboarder's shadow on ground", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10470.0, "ram_available_mb": 112036.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [38.75, 38.75], "power_watts_avg": 38.75, "power_watts_peak": 38.75, "energy_joules_est": 7.23, "sample_count": 2, "duration_seconds": 0.187}, "timestamp": "2026-01-25T20:03:37.909696"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 347.857, "latencies_ms": [347.857], "images_per_second": 2.875, "prompt_tokens": 757, "response_tokens_est": 61, "n_tiles": 1, "output_text": "\nIn this black and white photo, a person is riding a skateboard on top of a ramp. The skateboarder is wearing a pair of sneakers with the letters DC on them. Another person can be seen in the background, possibly watching or waiting for their turn to ride the skateboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.0, "ram_available_mb": 112036.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10477.6, "ram_available_mb": 112028.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [38.75, 38.75, 36.61, 36.61], "power_watts_avg": 37.68, "power_watts_peak": 38.75, "energy_joules_est": 13.12, "sample_count": 4, "duration_seconds": 0.348}, "timestamp": "2026-01-25T20:03:38.316323"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 451.558, "latencies_ms": [451.558], "images_per_second": 2.215, "prompt_tokens": 756, "response_tokens_est": 79, "n_tiles": 1, "output_text": "\n The image is a black and white photo of a skateboarder performing a trick on his skateboard. The skateboarder is wearing a pair of sneakers with the letters DC on them. In the background, there are two people watching the skateboarder's performance. One person can be seen in the foreground while the other one is located further back and slightly to the right side.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10477.6, "ram_available_mb": 112028.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10481.0, "ram_available_mb": 112025.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [36.61, 36.61, 36.61, 34.36, 34.36], "power_watts_avg": 35.71, "power_watts_peak": 36.61, "energy_joules_est": 16.13, "sample_count": 5, "duration_seconds": 0.452}, "timestamp": "2026-01-25T20:03:38.821767"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 283.523, "latencies_ms": [283.523], "images_per_second": 3.527, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA man is laying on a carpeted floor, surrounded by various items including a laptop, camera, water bottle, and book.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.0, "ram_available_mb": 112025.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10480.0, "ram_available_mb": 112026.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [34.36, 34.36, 34.36], "power_watts_avg": 34.36, "power_watts_peak": 34.36, "energy_joules_est": 9.76, "sample_count": 3, "duration_seconds": 0.284}, "timestamp": "2026-01-25T20:03:39.132928"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 108.585, "latencies_ms": [108.585], "images_per_second": 9.209, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Laptop (0.32)", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.0, "ram_available_mb": 112026.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10476.9, "ram_available_mb": 112029.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [40.11, 40.11], "power_watts_avg": 40.11, "power_watts_peak": 40.11, "energy_joules_est": 4.37, "sample_count": 2, "duration_seconds": 0.109}, "timestamp": "2026-01-25T20:03:39.339503"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 97.93, "latencies_ms": [97.93], "images_per_second": 10.211, "prompt_tokens": 763, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n - A laptop on a carpet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.9, "ram_available_mb": 112029.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [40.11], "power_watts_avg": 40.11, "power_watts_peak": 40.11, "energy_joules_est": 3.95, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T20:03:39.444396"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 202.94, "latencies_ms": [202.94], "images_per_second": 4.928, "prompt_tokens": 757, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA man lies on a carpeted floor with various items around him, including a laptop, camera, water bottle, book, and tennis racket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10479.1, "ram_available_mb": 112027.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [40.11, 40.11, 35.3], "power_watts_avg": 38.51, "power_watts_peak": 40.11, "energy_joules_est": 7.82, "sample_count": 3, "duration_seconds": 0.203}, "timestamp": "2026-01-25T20:03:39.750752"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 176.039, "latencies_ms": [176.039], "images_per_second": 5.681, "prompt_tokens": 756, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\n 1. A black and white photo of a man laying on the floor with his head resting on his hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.1, "ram_available_mb": 112027.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [35.3, 35.3], "power_watts_avg": 35.3, "power_watts_peak": 35.3, "energy_joules_est": 6.24, "sample_count": 2, "duration_seconds": 0.177}, "timestamp": "2026-01-25T20:03:39.957374"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 267.481, "latencies_ms": [267.481], "images_per_second": 3.739, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "urn of spices sits on a black stove top in a kitchen with light beige tile walls and a wooden cabinet above it.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10478.3, "ram_available_mb": 112028.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 20.0}, "power_stats": {"power_watts_samples": [35.3, 30.06, 30.06], "power_watts_avg": 31.8, "power_watts_peak": 35.3, "energy_joules_est": 8.53, "sample_count": 3, "duration_seconds": 0.268}, "timestamp": "2026-01-25T20:03:40.272398"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 309.596, "latencies_ms": [309.596], "images_per_second": 3.23, "prompt_tokens": 759, "response_tokens_est": 49, "n_tiles": 1, "output_text": "\n 1. Oven door  2. Oven door  3. Oven door  4. Oven door  5. Oven door  6. Oven door  7. Oven door  8. Oven door", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 10478.3, "ram_available_mb": 112028.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10479.5, "ram_available_mb": 112026.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 20.0}, "power_stats": {"power_watts_samples": [30.06, 30.06, 30.06, 36.63], "power_watts_avg": 31.7, "power_watts_peak": 36.63, "energy_joules_est": 9.83, "sample_count": 4, "duration_seconds": 0.31}, "timestamp": "2026-01-25T20:03:40.680334"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 106.123, "latencies_ms": [106.123], "images_per_second": 9.423, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urn of spices on a stove top.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.5, "ram_available_mb": 112026.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10479.5, "ram_available_mb": 112026.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 20.0}, "power_stats": {"power_watts_samples": [36.63, 36.63], "power_watts_avg": 36.63, "power_watts_peak": 36.63, "energy_joules_est": 3.91, "sample_count": 2, "duration_seconds": 0.107}, "timestamp": "2026-01-25T20:03:40.885783"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 182.31, "latencies_ms": [182.31], "images_per_second": 5.485, "prompt_tokens": 757, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA kitchen counter with a black stove top, a spice rack filled with various spices, and a towel hanging on a hook.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.5, "ram_available_mb": 112026.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10480.5, "ram_available_mb": 112025.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [36.63, 36.63], "power_watts_avg": 36.63, "power_watts_peak": 36.63, "energy_joules_est": 6.69, "sample_count": 2, "duration_seconds": 0.183}, "timestamp": "2026-01-25T20:03:41.092836"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 105.455, "latencies_ms": [105.455], "images_per_second": 9.483, "prompt_tokens": 756, "response_tokens_est": 11, "n_tiles": 1, "output_text": "urn of orange and red peppers on the stove.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.5, "ram_available_mb": 112025.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10472.2, "ram_available_mb": 112034.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [34.22, 34.22], "power_watts_avg": 34.22, "power_watts_peak": 34.22, "energy_joules_est": 3.62, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T20:03:41.298257"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 286.005, "latencies_ms": [286.005], "images_per_second": 3.496, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "iced coffee and chocolate doughnuts are displayed on a table in a restaurant, with a man sitting at the table eating a chocolate donut.\n", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [34.22, 34.22, 34.22], "power_watts_avg": 34.22, "power_watts_peak": 34.22, "energy_joules_est": 9.81, "sample_count": 3, "duration_seconds": 0.287}, "timestamp": "2026-01-25T20:03:41.607487"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 73.544, "latencies_ms": [73.544], "images_per_second": 13.597, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Donut: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 10474.1, "ram_available_mb": 112032.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [31.47], "power_watts_avg": 31.47, "power_watts_peak": 31.47, "energy_joules_est": 2.32, "sample_count": 1, "duration_seconds": 0.074}, "timestamp": "2026-01-25T20:03:41.712556"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 200.291, "latencies_ms": [200.291], "images_per_second": 4.993, "prompt_tokens": 763, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\n 1. A man eating a chocolate donut in front of another man who is sitting at a table with food and drinks on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.1, "ram_available_mb": 112032.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [31.47, 31.47], "power_watts_avg": 31.47, "power_watts_peak": 31.47, "energy_joules_est": 6.31, "sample_count": 2, "duration_seconds": 0.201}, "timestamp": "2026-01-25T20:03:41.917292"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 406.602, "latencies_ms": [406.602], "images_per_second": 2.459, "prompt_tokens": 757, "response_tokens_est": 69, "n_tiles": 1, "output_text": "\nIn a coffee shop, a man in a plaid shirt sits at a table with two plates of donuts. One plate has a chocolate donut topped with sprinkles, while the other has an orange-colored donut with a chocolate glaze. The man holds one of the donuts and appears to be enjoying his treat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.5, "ram_available_mb": 112032.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [31.47, 31.47, 34.32, 34.32, 34.32], "power_watts_avg": 33.18, "power_watts_peak": 34.32, "energy_joules_est": 13.5, "sample_count": 5, "duration_seconds": 0.407}, "timestamp": "2026-01-25T20:03:42.422216"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 58.898, "latencies_ms": [58.898], "images_per_second": 16.979, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [34.32], "power_watts_avg": 34.32, "power_watts_peak": 34.32, "energy_joules_est": 2.05, "sample_count": 1, "duration_seconds": 0.06}, "timestamp": "2026-01-25T20:03:42.528022"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 290.929, "latencies_ms": [290.929], "images_per_second": 3.437, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "urns of water sit on a black and white checkered floor, with a toilet in the background featuring a cow design.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10475.5, "ram_available_mb": 112030.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [34.32, 31.48, 31.48], "power_watts_avg": 32.42, "power_watts_peak": 34.32, "energy_joules_est": 9.45, "sample_count": 3, "duration_seconds": 0.291}, "timestamp": "2026-01-25T20:03:42.836575"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 89.932, "latencies_ms": [89.932], "images_per_second": 11.12, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Toilet: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 10475.5, "ram_available_mb": 112030.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10476.0, "ram_available_mb": 112030.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [31.48], "power_watts_avg": 31.48, "power_watts_peak": 31.48, "energy_joules_est": 2.84, "sample_count": 1, "duration_seconds": 0.09}, "timestamp": "2026-01-25T20:03:42.940971"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 135.723, "latencies_ms": [135.723], "images_per_second": 7.368, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n A toilet with a black and white cow pattern sits next to a sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.0, "ram_available_mb": 112030.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [31.48, 31.48], "power_watts_avg": 31.48, "power_watts_peak": 31.48, "energy_joules_est": 4.28, "sample_count": 2, "duration_seconds": 0.136}, "timestamp": "2026-01-25T20:03:43.148540"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 155.368, "latencies_ms": [155.368], "images_per_second": 6.436, "prompt_tokens": 757, "response_tokens_est": 20, "n_tiles": 1, "output_text": "urns of water on a black and white checkered floor, with one containing water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [34.83, 34.83], "power_watts_avg": 34.83, "power_watts_peak": 34.83, "energy_joules_est": 5.42, "sample_count": 2, "duration_seconds": 0.156}, "timestamp": "2026-01-25T20:03:43.355592"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 96.98, "latencies_ms": [96.98], "images_per_second": 10.311, "prompt_tokens": 756, "response_tokens_est": 8, "n_tiles": 1, "output_text": "urns are white and black.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [34.83], "power_watts_avg": 34.83, "power_watts_peak": 34.83, "energy_joules_est": 3.39, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T20:03:43.461011"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 291.573, "latencies_ms": [291.573], "images_per_second": 3.43, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "urns of teddy bears are placed on a cross in front of a house, with the names Joseph and Paris displayed above it.\n", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [34.83, 33.06, 33.06], "power_watts_avg": 33.65, "power_watts_peak": 34.83, "energy_joules_est": 9.82, "sample_count": 3, "duration_seconds": 0.292}, "timestamp": "2026-01-25T20:03:43.776737"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 86.373, "latencies_ms": [86.373], "images_per_second": 11.578, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Teddy bear: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10465.7, "ram_available_mb": 112040.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [33.06], "power_watts_avg": 33.06, "power_watts_peak": 33.06, "energy_joules_est": 2.87, "sample_count": 1, "duration_seconds": 0.087}, "timestamp": "2026-01-25T20:03:43.882297"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 113.886, "latencies_ms": [113.886], "images_per_second": 8.781, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Teddy bear on top of a cross.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.7, "ram_available_mb": 112040.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10465.5, "ram_available_mb": 112040.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [33.06, 33.06], "power_watts_avg": 33.06, "power_watts_peak": 33.06, "energy_joules_est": 3.77, "sample_count": 2, "duration_seconds": 0.114}, "timestamp": "2026-01-25T20:03:44.087813"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 191.314, "latencies_ms": [191.314], "images_per_second": 5.227, "prompt_tokens": 757, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA statue of a woman with teddy bears on her shoulders sits in front of a cross, surrounded by a brick walkway.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.5, "ram_available_mb": 112040.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10466.4, "ram_available_mb": 112039.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [32.52, 32.52], "power_watts_avg": 32.52, "power_watts_peak": 32.52, "energy_joules_est": 6.25, "sample_count": 2, "duration_seconds": 0.192}, "timestamp": "2026-01-25T20:03:44.293285"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 117.599, "latencies_ms": [117.599], "images_per_second": 8.503, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe statue is made of stone and has a white base.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.4, "ram_available_mb": 112039.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [32.52, 32.52], "power_watts_avg": 32.52, "power_watts_peak": 32.52, "energy_joules_est": 3.83, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:03:44.498374"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 247.225, "latencies_ms": [247.225], "images_per_second": 4.045, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urn of candles sits on a table in a restaurant, with people seated at tables and enjoying their meals.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10466.4, "ram_available_mb": 112039.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [32.52, 32.78, 32.78], "power_watts_avg": 32.7, "power_watts_peak": 32.78, "energy_joules_est": 8.1, "sample_count": 3, "duration_seconds": 0.248}, "timestamp": "2026-01-25T20:03:44.806825"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 108.681, "latencies_ms": [108.681], "images_per_second": 9.201, "prompt_tokens": 759, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Clock - 2:00 pm", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.4, "ram_available_mb": 112039.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [32.78, 32.78], "power_watts_avg": 32.78, "power_watts_peak": 32.78, "energy_joules_est": 3.58, "sample_count": 2, "duration_seconds": 0.109}, "timestamp": "2026-01-25T20:03:45.012948"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 124.609, "latencies_ms": [124.609], "images_per_second": 8.025, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A large clock is in a restaurant with people sitting at tables.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 9.0}, "power_stats": {"power_watts_samples": [32.78, 30.75], "power_watts_avg": 31.76, "power_watts_peak": 32.78, "energy_joules_est": 3.98, "sample_count": 2, "duration_seconds": 0.125}, "timestamp": "2026-01-25T20:03:45.219127"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 263.216, "latencies_ms": [263.216], "images_per_second": 3.799, "prompt_tokens": 757, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\nA large clock hangs from the ceiling in a restaurant, surrounded by tables with people seated at them. The room has wooden beams on the ceiling and walls, giving it an old-world charm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 9.0}, "power_stats": {"power_watts_samples": [30.75, 30.75, 30.75], "power_watts_avg": 30.75, "power_watts_peak": 30.75, "energy_joules_est": 8.1, "sample_count": 3, "duration_seconds": 0.264}, "timestamp": "2026-01-25T20:03:45.526496"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 97.003, "latencies_ms": [97.003], "images_per_second": 10.309, "prompt_tokens": 756, "response_tokens_est": 8, "n_tiles": 1, "output_text": "urns of candles on tables.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 9.0}, "power_stats": {"power_watts_samples": [30.75], "power_watts_avg": 30.75, "power_watts_peak": 30.75, "energy_joules_est": 3.0, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T20:03:45.631540"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 271.184, "latencies_ms": [271.184], "images_per_second": 3.688, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "iced coffee in a cup on a table, with a person wearing a pink jacket and another person in black standing nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 9.0}, "power_stats": {"power_watts_samples": [32.84, 32.84, 32.84], "power_watts_avg": 32.84, "power_watts_peak": 32.84, "energy_joules_est": 8.91, "sample_count": 3, "duration_seconds": 0.271}, "timestamp": "2026-01-25T20:03:45.941196"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.736, "latencies_ms": [78.736], "images_per_second": 12.701, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Sun in background", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 9.0}, "power_stats": {"power_watts_samples": [32.84], "power_watts_avg": 32.84, "power_watts_peak": 32.84, "energy_joules_est": 2.6, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T20:03:46.047520"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 107.312, "latencies_ms": [107.312], "images_per_second": 9.319, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Person on left side of photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [32.84, 33.87], "power_watts_avg": 33.35, "power_watts_peak": 33.87, "energy_joules_est": 3.6, "sample_count": 2, "duration_seconds": 0.108}, "timestamp": "2026-01-25T20:03:46.254155"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 213.818, "latencies_ms": [213.818], "images_per_second": 4.677, "prompt_tokens": 757, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nIn a snowy landscape, two people are skiing down a slope. One person is standing on skis while the other is sitting in a chair with ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10465.7, "ram_available_mb": 112040.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [33.87, 33.87, 33.87], "power_watts_avg": 33.87, "power_watts_peak": 33.87, "energy_joules_est": 7.25, "sample_count": 3, "duration_seconds": 0.214}, "timestamp": "2026-01-25T20:03:46.560273"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 277.906, "latencies_ms": [277.906], "images_per_second": 3.598, "prompt_tokens": 756, "response_tokens_est": 45, "n_tiles": 1, "output_text": "\n The image shows a man and a child on skis in the snow. The man is wearing black ski gear while the child has pink ski gear. They are standing on a snowy hill with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.7, "ram_available_mb": 112040.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [32.6, 32.6, 32.6], "power_watts_avg": 32.6, "power_watts_peak": 32.6, "energy_joules_est": 9.08, "sample_count": 3, "duration_seconds": 0.278}, "timestamp": "2026-01-25T20:03:46.866880"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 277.367, "latencies_ms": [277.367], "images_per_second": 3.605, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nThe image shows a person's feet wearing black flip-flops, with three cell phones on the wooden floor in front of them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10461.9, "ram_available_mb": 112044.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.6, 32.6, 34.6], "power_watts_avg": 33.27, "power_watts_peak": 34.6, "energy_joules_est": 9.25, "sample_count": 3, "duration_seconds": 0.278}, "timestamp": "2026-01-25T20:03:47.175634"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 92.125, "latencies_ms": [92.125], "images_per_second": 10.855, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Flip phone: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.9, "ram_available_mb": 112044.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.6], "power_watts_avg": 34.6, "power_watts_peak": 34.6, "energy_joules_est": 3.21, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:03:47.279840"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 109.608, "latencies_ms": [109.608], "images_per_second": 9.123, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n - A black flip flop on a wooden floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.6, 34.6], "power_watts_avg": 34.6, "power_watts_peak": 34.6, "energy_joules_est": 3.8, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T20:03:47.485419"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 167.94, "latencies_ms": [167.94], "images_per_second": 5.954, "prompt_tokens": 757, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\n A person's feet are propped up on a wooden floor, with three cell phones lying next to them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.6, 34.74], "power_watts_avg": 34.67, "power_watts_peak": 34.74, "energy_joules_est": 5.84, "sample_count": 2, "duration_seconds": 0.168}, "timestamp": "2026-01-25T20:03:47.689779"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 118.255, "latencies_ms": [118.255], "images_per_second": 8.456, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. A black flip flop on a wooden floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.74, 34.74], "power_watts_avg": 34.74, "power_watts_peak": 34.74, "energy_joules_est": 4.12, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T20:03:47.894119"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 403.078, "latencies_ms": [403.078], "images_per_second": 2.481, "prompt_tokens": 744, "response_tokens_est": 52, "n_tiles": 1, "output_text": "\nThe Palace of Westminster, also known as the Houses of Parliament, is a large stone building with two towers and a clock tower in London, England. The photograph captures the scene from across the River Thames, featuring boats on the water near the palace.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10462.0, "ram_available_mb": 112044.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [34.74, 34.74, 32.15, 32.15, 32.15], "power_watts_avg": 33.18, "power_watts_peak": 34.74, "energy_joules_est": 13.39, "sample_count": 5, "duration_seconds": 0.404}, "timestamp": "2026-01-25T20:03:48.408179"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.669, "latencies_ms": [76.669], "images_per_second": 13.043, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Tower of London", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.0, "ram_available_mb": 112044.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [32.15], "power_watts_avg": 32.15, "power_watts_peak": 32.15, "energy_joules_est": 2.48, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T20:03:48.514961"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 125.145, "latencies_ms": [125.145], "images_per_second": 7.991, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA boat is floating on a river in front of a large building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [32.15, 33.82], "power_watts_avg": 32.98, "power_watts_peak": 33.82, "energy_joules_est": 4.15, "sample_count": 2, "duration_seconds": 0.126}, "timestamp": "2026-01-25T20:03:48.721589"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 240.579, "latencies_ms": [240.579], "images_per_second": 4.157, "prompt_tokens": 757, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\n A large boat sails on a river, with two smaller boats nearby. In the distance, the Palace of Westminster can be seen, lit up against an overcast sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [33.82, 33.82, 33.82], "power_watts_avg": 33.82, "power_watts_peak": 33.82, "energy_joules_est": 8.14, "sample_count": 3, "duration_seconds": 0.241}, "timestamp": "2026-01-25T20:03:49.028598"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 348.043, "latencies_ms": [348.043], "images_per_second": 2.873, "prompt_tokens": 756, "response_tokens_est": 60, "n_tiles": 1, "output_text": "\n The image shows a large building with two clock towers and a bridge in the background. The sky is cloudy, which gives an overcast appearance to the scene. In front of the building, there are boats on the water, including one that appears to be a barge or ferry boat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10462.0, "ram_available_mb": 112044.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [33.82, 31.26, 31.26, 31.26], "power_watts_avg": 31.9, "power_watts_peak": 33.82, "energy_joules_est": 11.12, "sample_count": 4, "duration_seconds": 0.349}, "timestamp": "2026-01-25T20:03:49.435260"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 267.451, "latencies_ms": [267.451], "images_per_second": 3.739, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urns of flowers sit on a table in the corner, with a television and stereo system nearby.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.0, "ram_available_mb": 112044.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10462.7, "ram_available_mb": 112043.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [31.26, 31.26, 33.81], "power_watts_avg": 32.11, "power_watts_peak": 33.81, "energy_joules_est": 8.6, "sample_count": 3, "duration_seconds": 0.268}, "timestamp": "2026-01-25T20:03:49.744786"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 72.749, "latencies_ms": [72.749], "images_per_second": 13.746, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Couch: 2", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10462.7, "ram_available_mb": 112043.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [33.81], "power_watts_avg": 33.81, "power_watts_peak": 33.81, "energy_joules_est": 2.49, "sample_count": 1, "duration_seconds": 0.074}, "timestamp": "2026-01-25T20:03:49.851543"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 107.746, "latencies_ms": [107.746], "images_per_second": 9.281, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n A white ceiling fan hangs from a light fixture.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [33.81, 33.81], "power_watts_avg": 33.81, "power_watts_peak": 33.81, "energy_joules_est": 3.65, "sample_count": 2, "duration_seconds": 0.108}, "timestamp": "2026-01-25T20:03:50.058140"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 619.996, "latencies_ms": [619.996], "images_per_second": 1.613, "prompt_tokens": 757, "response_tokens_est": 112, "n_tiles": 1, "output_text": "\nThe image shows a living room with hardwood floors, white walls, and various furniture pieces. There are two couches in the room - one green couch on the left side and another black couch on the right side. A chair can be seen near the center of the room. The space is filled with several potted plants placed throughout the room, adding a touch of greenery to the scene. Additionally, there are multiple TVs visible in the living area, suggesting that it may serve as a multi-purpose entertainment and relaxation space for the occupants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10465.4, "ram_available_mb": 112040.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [35.05, 35.05, 35.05, 35.05, 35.05, 36.17, 36.17], "power_watts_avg": 35.37, "power_watts_peak": 36.17, "energy_joules_est": 21.94, "sample_count": 7, "duration_seconds": 0.62}, "timestamp": "2026-01-25T20:03:50.768643"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 126.432, "latencies_ms": [126.432], "images_per_second": 7.909, "prompt_tokens": 756, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n The living room is filled with natural light and has a warm color palette.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.4, "ram_available_mb": 112040.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [36.17, 36.17], "power_watts_avg": 36.17, "power_watts_peak": 36.17, "energy_joules_est": 4.59, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:03:50.974480"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 273.978, "latencies_ms": [273.978], "images_per_second": 3.65, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA red parking meter stands on a sidewalk next to a building, with a sign that reads \"40 Years of Saving Lives\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [36.17, 34.9, 34.9], "power_watts_avg": 35.33, "power_watts_peak": 36.17, "energy_joules_est": 9.7, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T20:03:51.287042"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 84.271, "latencies_ms": [84.271], "images_per_second": 11.867, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Parking meter", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [34.9], "power_watts_avg": 34.9, "power_watts_peak": 34.9, "energy_joules_est": 2.97, "sample_count": 1, "duration_seconds": 0.085}, "timestamp": "2026-01-25T20:03:51.391509"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 214.695, "latencies_ms": [214.695], "images_per_second": 4.658, "prompt_tokens": 763, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\n 1. A parking meter on a red pole in front of a building with a picture of two people and the words \"40 years of saving lives\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [34.9, 34.9, 34.53], "power_watts_avg": 34.78, "power_watts_peak": 34.9, "energy_joules_est": 7.47, "sample_count": 3, "duration_seconds": 0.215}, "timestamp": "2026-01-25T20:03:51.697319"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 232.803, "latencies_ms": [232.803], "images_per_second": 4.295, "prompt_tokens": 757, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA red parking meter stands on a sidewalk next to a building, with a sign that says \"40 Years of Saving Lives\". The meter has two coin slots for payment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [34.53, 34.53, 34.53], "power_watts_avg": 34.53, "power_watts_peak": 34.53, "energy_joules_est": 8.07, "sample_count": 3, "duration_seconds": 0.234}, "timestamp": "2026-01-25T20:03:52.005165"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 243.824, "latencies_ms": [243.824], "images_per_second": 4.101, "prompt_tokens": 756, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\n The image shows a red parking meter with two black coin slots on top of it. In the background, there is a building and a window displaying an advertisement for saving lives.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [34.53, 34.47, 34.47], "power_watts_avg": 34.49, "power_watts_peak": 34.53, "energy_joules_est": 8.43, "sample_count": 3, "duration_seconds": 0.244}, "timestamp": "2026-01-25T20:03:52.312579"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 275.101, "latencies_ms": [275.101], "images_per_second": 3.635, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "urn of flowers sits on a coffee table, with two people sitting on a couch and playing video games in front of it.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [34.47, 34.47, 34.47], "power_watts_avg": 34.47, "power_watts_peak": 34.47, "energy_joules_est": 9.49, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T20:03:52.626563"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 85.533, "latencies_ms": [85.533], "images_per_second": 11.691, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Couch - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10449.7, "ram_available_mb": 112056.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [34.99], "power_watts_avg": 34.99, "power_watts_peak": 34.99, "energy_joules_est": 3.02, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T20:03:52.732840"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 141.236, "latencies_ms": [141.236], "images_per_second": 7.08, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. A man and a woman sitting on a couch watching television.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.7, "ram_available_mb": 112056.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10451.8, "ram_available_mb": 112054.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [34.99, 34.99], "power_watts_avg": 34.99, "power_watts_peak": 34.99, "energy_joules_est": 4.95, "sample_count": 2, "duration_seconds": 0.141}, "timestamp": "2026-01-25T20:03:52.938913"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 370.509, "latencies_ms": [370.509], "images_per_second": 2.699, "prompt_tokens": 757, "response_tokens_est": 62, "n_tiles": 1, "output_text": "\nA man and a woman are sitting on a couch in front of a television, playing video games. The room has a brown color scheme with wooden furniture, including a coffee table that holds snacks such as chips and candy bars. A vase filled with flowers adds a touch of elegance to the space.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10451.8, "ram_available_mb": 112054.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10448.7, "ram_available_mb": 112057.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 18.0}, "power_stats": {"power_watts_samples": [34.99, 34.99, 34.45, 34.45], "power_watts_avg": 34.72, "power_watts_peak": 34.99, "energy_joules_est": 12.89, "sample_count": 4, "duration_seconds": 0.371}, "timestamp": "2026-01-25T20:03:53.345370"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 309.496, "latencies_ms": [309.496], "images_per_second": 3.231, "prompt_tokens": 756, "response_tokens_est": 50, "n_tiles": 1, "output_text": "\n The image shows a man and woman sitting on a couch in front of a television. They are playing video games together while enjoying snacks like chips and candy bars. A vase with flowers is placed between them, adding to the cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10448.7, "ram_available_mb": 112057.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10448.7, "ram_available_mb": 112057.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 18.0}, "power_stats": {"power_watts_samples": [34.45, 34.45, 34.45, 33.05], "power_watts_avg": 34.1, "power_watts_peak": 34.45, "energy_joules_est": 10.57, "sample_count": 4, "duration_seconds": 0.31}, "timestamp": "2026-01-25T20:03:53.752613"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 244.355, "latencies_ms": [244.355], "images_per_second": 4.092, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urn with a white lid and silver handle, standing on a gray floor in front of a white wall.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10448.7, "ram_available_mb": 112057.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10449.4, "ram_available_mb": 112056.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [33.05, 33.05, 33.05], "power_watts_avg": 33.05, "power_watts_peak": 33.05, "energy_joules_est": 8.1, "sample_count": 3, "duration_seconds": 0.245}, "timestamp": "2026-01-25T20:03:54.066119"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 89.309, "latencies_ms": [89.309], "images_per_second": 11.197, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Remote control", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10449.4, "ram_available_mb": 112056.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10453.1, "ram_available_mb": 112053.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [38.32], "power_watts_avg": 38.32, "power_watts_peak": 38.32, "energy_joules_est": 3.44, "sample_count": 1, "duration_seconds": 0.09}, "timestamp": "2026-01-25T20:03:54.171279"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 116.935, "latencies_ms": [116.935], "images_per_second": 8.552, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. A white toilet with a remote control in it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.1, "ram_available_mb": 112053.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10449.7, "ram_available_mb": 112056.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [38.32, 38.32], "power_watts_avg": 38.32, "power_watts_peak": 38.32, "energy_joules_est": 4.49, "sample_count": 2, "duration_seconds": 0.117}, "timestamp": "2026-01-25T20:03:54.376187"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 219.941, "latencies_ms": [219.941], "images_per_second": 4.547, "prompt_tokens": 757, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA person's hand holds a remote control, possibly for an electronic device. The remote appears to be in good condition with no visible damage or wear.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10449.7, "ram_available_mb": 112056.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [38.32, 38.32, 32.8], "power_watts_avg": 36.48, "power_watts_peak": 38.32, "energy_joules_est": 8.04, "sample_count": 3, "duration_seconds": 0.221}, "timestamp": "2026-01-25T20:03:54.681989"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 110.265, "latencies_ms": [110.265], "images_per_second": 9.069, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urn with a white lid and silver handle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10457.8, "ram_available_mb": 112048.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.8, 32.8], "power_watts_avg": 32.8, "power_watts_peak": 32.8, "energy_joules_est": 3.64, "sample_count": 2, "duration_seconds": 0.111}, "timestamp": "2026-01-25T20:03:54.888758"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 325.048, "latencies_ms": [325.048], "images_per_second": 3.076, "prompt_tokens": 744, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nA snowboarder is performing a trick in mid-air, with their board flipping beneath them as they soar above a small white cube on a snowy hillside.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 10457.8, "ram_available_mb": 112048.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10458.5, "ram_available_mb": 112047.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [32.8, 32.8, 29.05, 29.05], "power_watts_avg": 30.92, "power_watts_peak": 32.8, "energy_joules_est": 10.06, "sample_count": 4, "duration_seconds": 0.325}, "timestamp": "2026-01-25T20:03:55.302859"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 259.261, "latencies_ms": [259.261], "images_per_second": 3.857, "prompt_tokens": 759, "response_tokens_est": 42, "n_tiles": 1, "output_text": "\n 1: 0.46 2: 0.5 3: 0.56 4: 0.63 5: 0.69 6: 0.77 7: 0.83 8: 0.9", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.5, "ram_available_mb": 112047.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [29.05, 29.05, 29.05], "power_watts_avg": 29.05, "power_watts_peak": 29.05, "energy_joules_est": 7.55, "sample_count": 3, "duration_seconds": 0.26}, "timestamp": "2026-01-25T20:03:55.608571"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 144.181, "latencies_ms": [144.181], "images_per_second": 6.936, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nThe snowboarder is in midair and about to land on a small box.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10468.2, "ram_available_mb": 112038.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [33.68, 33.68], "power_watts_avg": 33.68, "power_watts_peak": 33.68, "energy_joules_est": 4.87, "sample_count": 2, "duration_seconds": 0.145}, "timestamp": "2026-01-25T20:03:55.814474"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 267.28, "latencies_ms": [267.28], "images_per_second": 3.741, "prompt_tokens": 757, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\nA snowboarder in a red jacket is performing an aerial trick, while another snowboarder in black is flying through the air. A crowd of spectators watches from behind them on a snowy mountain slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.2, "ram_available_mb": 112038.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.68, 33.68, 33.68], "power_watts_avg": 33.68, "power_watts_peak": 33.68, "energy_joules_est": 9.01, "sample_count": 3, "duration_seconds": 0.268}, "timestamp": "2026-01-25T20:03:56.122119"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 323.445, "latencies_ms": [323.445], "images_per_second": 3.092, "prompt_tokens": 756, "response_tokens_est": 55, "n_tiles": 1, "output_text": "\n The image shows a snowboarder performing an aerial trick in the air. The snowboarder is wearing a red and white outfit while riding on their board. There are several spectators watching from behind a fence, which can be seen in the background of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.98, 33.98, 33.98, 33.98], "power_watts_avg": 33.98, "power_watts_peak": 33.98, "energy_joules_est": 11.0, "sample_count": 4, "duration_seconds": 0.324}, "timestamp": "2026-01-25T20:03:56.528777"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 259.819, "latencies_ms": [259.819], "images_per_second": 3.849, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "urn of a plant sits on a table in front of a computer desk, which has two computers and a lamp.", "error": null, "sys_before": {"cpu_percent": 3.4, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.98, 32.5, 32.5], "power_watts_avg": 32.99, "power_watts_peak": 33.98, "energy_joules_est": 8.59, "sample_count": 3, "duration_seconds": 0.26}, "timestamp": "2026-01-25T20:03:56.844626"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 83.027, "latencies_ms": [83.027], "images_per_second": 12.044, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Lamp: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.5], "power_watts_avg": 32.5, "power_watts_peak": 32.5, "energy_joules_est": 2.71, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:03:56.950156"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 154.184, "latencies_ms": [154.184], "images_per_second": 6.486, "prompt_tokens": 763, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\n 1. A black chair with wheels in front of a desk that has a computer and laptop on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10465.8, "ram_available_mb": 112040.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [32.5, 32.5], "power_watts_avg": 32.5, "power_watts_peak": 32.5, "energy_joules_est": 5.02, "sample_count": 2, "duration_seconds": 0.155}, "timestamp": "2026-01-25T20:03:57.156203"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 268.102, "latencies_ms": [268.102], "images_per_second": 3.73, "prompt_tokens": 757, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nThe image shows a corner office with a desk, chair, computer monitor, keyboard, mouse, lamp, and potted plant. The room has white walls and carpeting, creating a clean and organized workspace.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.8, "ram_available_mb": 112040.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [35.63, 35.63, 35.63], "power_watts_avg": 35.63, "power_watts_peak": 35.63, "energy_joules_est": 9.56, "sample_count": 3, "duration_seconds": 0.268}, "timestamp": "2026-01-25T20:03:57.462043"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 104.473, "latencies_ms": [104.473], "images_per_second": 9.572, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n A desk with a computer and laptop on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 10467.8, "ram_available_mb": 112038.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [35.63, 35.81], "power_watts_avg": 35.72, "power_watts_peak": 35.81, "energy_joules_est": 3.75, "sample_count": 2, "duration_seconds": 0.105}, "timestamp": "2026-01-25T20:03:57.668745"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 269.466, "latencies_ms": [269.466], "images_per_second": 3.711, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA motorcyclist in a black helmet and jacket is riding on a dirt road, with another motorcycle parked nearby.", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 10467.8, "ram_available_mb": 112038.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10460.4, "ram_available_mb": 112045.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [35.81, 35.81, 35.81], "power_watts_avg": 35.81, "power_watts_peak": 35.81, "energy_joules_est": 9.66, "sample_count": 3, "duration_seconds": 0.27}, "timestamp": "2026-01-25T20:03:57.983754"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 109.677, "latencies_ms": [109.677], "images_per_second": 9.118, "prompt_tokens": 759, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Person in black helmet and black jacket riding motorcycle", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.6, "ram_available_mb": 112045.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10468.4, "ram_available_mb": 112037.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [35.81, 34.24], "power_watts_avg": 35.02, "power_watts_peak": 35.81, "energy_joules_est": 3.86, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T20:03:58.192040"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 130.731, "latencies_ms": [130.731], "images_per_second": 7.649, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A person riding a motorcycle is looking out over a mountain range.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.4, "ram_available_mb": 112037.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [34.24, 34.24], "power_watts_avg": 34.24, "power_watts_peak": 34.24, "energy_joules_est": 4.49, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T20:03:58.398317"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 226.569, "latencies_ms": [226.569], "images_per_second": 4.414, "prompt_tokens": 757, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\n A motorcyclist in a black helmet rides down a dirt road, with another rider following behind. The riders are on a mountain trail surrounded by trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10483.4, "ram_available_mb": 112022.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [34.24, 34.24, 31.06], "power_watts_avg": 33.18, "power_watts_peak": 34.24, "energy_joules_est": 7.54, "sample_count": 3, "duration_seconds": 0.227}, "timestamp": "2026-01-25T20:03:58.706045"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 92.924, "latencies_ms": [92.924], "images_per_second": 10.762, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The sky is blue and clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.4, "ram_available_mb": 112022.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10481.5, "ram_available_mb": 112024.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [31.06], "power_watts_avg": 31.06, "power_watts_peak": 31.06, "energy_joules_est": 2.9, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T20:03:58.813452"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 265.098, "latencies_ms": [265.098], "images_per_second": 3.772, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urns of orange and green fruit on a wooden table in a kitchen with white appliances, including a refrigerator and oven.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 10481.5, "ram_available_mb": 112024.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10485.3, "ram_available_mb": 112021.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.06, 31.06, 31.06], "power_watts_avg": 31.06, "power_watts_peak": 31.06, "energy_joules_est": 8.24, "sample_count": 3, "duration_seconds": 0.265}, "timestamp": "2026-01-25T20:03:59.120520"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 92.456, "latencies_ms": [92.456], "images_per_second": 10.816, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Orange - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10485.3, "ram_available_mb": 112021.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10485.9, "ram_available_mb": 112020.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.62], "power_watts_avg": 32.62, "power_watts_peak": 32.62, "energy_joules_est": 3.04, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:03:59.225632"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 54.274, "latencies_ms": [54.274], "images_per_second": 18.425, "prompt_tokens": 763, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10485.9, "ram_available_mb": 112020.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10480.0, "ram_available_mb": 112026.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.62], "power_watts_avg": 32.62, "power_watts_peak": 32.62, "energy_joules_est": 1.78, "sample_count": 1, "duration_seconds": 0.054}, "timestamp": "2026-01-25T20:03:59.330568"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 166.15, "latencies_ms": [166.15], "images_per_second": 6.019, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urn of orange flowers sits on a wooden table in a kitchen with white appliances, including a refrigerator and oven.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10480.0, "ram_available_mb": 112026.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10478.7, "ram_available_mb": 112027.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.62, 32.62], "power_watts_avg": 32.62, "power_watts_peak": 32.62, "energy_joules_est": 5.44, "sample_count": 2, "duration_seconds": 0.167}, "timestamp": "2026-01-25T20:03:59.536520"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 93.911, "latencies_ms": [93.911], "images_per_second": 10.648, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "urns on the countertop.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.7, "ram_available_mb": 112027.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10479.6, "ram_available_mb": 112026.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.62], "power_watts_avg": 32.62, "power_watts_peak": 32.62, "energy_joules_est": 3.07, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T20:03:59.641540"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 333.837, "latencies_ms": [333.837], "images_per_second": 2.995, "prompt_tokens": 744, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA female tennis player in a white shirt and black skirt is poised to strike a yellow ball with her racket on a blue court, while spectators watch from behind a blue wall.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10479.6, "ram_available_mb": 112026.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10479.5, "ram_available_mb": 112026.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.01, 32.01, 32.01, 32.01], "power_watts_avg": 32.01, "power_watts_peak": 32.01, "energy_joules_est": 10.69, "sample_count": 4, "duration_seconds": 0.334}, "timestamp": "2026-01-25T20:04:00.053771"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 243.21, "latencies_ms": [243.21], "images_per_second": 4.112, "prompt_tokens": 759, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\n 1. Blue bench 2. Tennis ball 3. Woman in white shirt and black skirt 4. White line 5. Blue wall 6. Person wearing blue jeans 7. Person wearing white shoes", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10479.5, "ram_available_mb": 112026.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10479.6, "ram_available_mb": 112026.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [32.73, 32.73, 32.73], "power_watts_avg": 32.73, "power_watts_peak": 32.73, "energy_joules_est": 7.97, "sample_count": 3, "duration_seconds": 0.244}, "timestamp": "2026-01-25T20:04:00.359427"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 132.249, "latencies_ms": [132.249], "images_per_second": 7.562, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nThe woman is playing tennis and has her racket ready to hit a ball.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10479.6, "ram_available_mb": 112026.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10479.9, "ram_available_mb": 112026.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [32.73, 32.73], "power_watts_avg": 32.73, "power_watts_peak": 32.73, "energy_joules_est": 4.35, "sample_count": 2, "duration_seconds": 0.133}, "timestamp": "2026-01-25T20:04:00.565639"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 198.596, "latencies_ms": [198.596], "images_per_second": 5.035, "prompt_tokens": 757, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA female tennis player in a white outfit is preparing to serve on a blue court, with her right hand holding a tennis racket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.9, "ram_available_mb": 112026.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10479.0, "ram_available_mb": 112027.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [35.16, 35.16], "power_watts_avg": 35.16, "power_watts_peak": 35.16, "energy_joules_est": 7.0, "sample_count": 2, "duration_seconds": 0.199}, "timestamp": "2026-01-25T20:04:00.771388"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 114.364, "latencies_ms": [114.364], "images_per_second": 8.744, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The woman is wearing a white shirt and black skirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.0, "ram_available_mb": 112027.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10476.0, "ram_available_mb": 112030.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [35.16, 35.16], "power_watts_avg": 35.16, "power_watts_peak": 35.16, "energy_joules_est": 4.03, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T20:04:00.977225"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 291.895, "latencies_ms": [291.895], "images_per_second": 3.426, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA red fire hydrant stands on a sidewalk, with a yellow sign and tree in the background. A person is walking past the hydrant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.0, "ram_available_mb": 112030.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10476.9, "ram_available_mb": 112029.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [35.16, 30.35, 30.35], "power_watts_avg": 31.96, "power_watts_peak": 35.16, "energy_joules_est": 9.35, "sample_count": 3, "duration_seconds": 0.292}, "timestamp": "2026-01-25T20:04:01.287580"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.689, "latencies_ms": [82.689], "images_per_second": 12.093, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Fire hydrant - 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.9, "ram_available_mb": 112029.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10480.6, "ram_available_mb": 112025.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.35], "power_watts_avg": 30.35, "power_watts_peak": 30.35, "energy_joules_est": 2.52, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:04:01.392460"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 136.852, "latencies_ms": [136.852], "images_per_second": 7.307, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA red fire hydrant is located on a sidewalk next to a street sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.6, "ram_available_mb": 112025.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.35, 30.35], "power_watts_avg": 30.35, "power_watts_peak": 30.35, "energy_joules_est": 4.16, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T20:04:01.598629"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 297.302, "latencies_ms": [297.302], "images_per_second": 3.364, "prompt_tokens": 757, "response_tokens_est": 48, "n_tiles": 1, "output_text": "\nIn a residential area, there is a red fire hydrant on the sidewalk next to a yellow sign with a picture of two people walking. A woman in a white shirt stands nearby, possibly waiting for someone or just passing by.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 10476.6, "ram_available_mb": 112029.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [34.35, 34.35, 34.35], "power_watts_avg": 34.35, "power_watts_peak": 34.35, "energy_joules_est": 10.23, "sample_count": 3, "duration_seconds": 0.298}, "timestamp": "2026-01-25T20:04:01.904069"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 251.158, "latencies_ms": [251.158], "images_per_second": 3.982, "prompt_tokens": 756, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\n The image features a red fire hydrant on the sidewalk next to a yellow street sign. A person is standing near the fire hydrant and there are two other people in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.6, "ram_available_mb": 112029.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10478.1, "ram_available_mb": 112028.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [34.35, 34.35, 37.83], "power_watts_avg": 35.51, "power_watts_peak": 37.83, "energy_joules_est": 8.93, "sample_count": 3, "duration_seconds": 0.251}, "timestamp": "2026-01-25T20:04:02.209511"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 253.335, "latencies_ms": [253.335], "images_per_second": 3.947, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urns of toilet paper and hand soap are placed on a green carpeted floor in a bathroom stall.\n", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 10478.1, "ram_available_mb": 112028.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [37.83, 37.83, 37.83], "power_watts_avg": 37.83, "power_watts_peak": 37.83, "energy_joules_est": 9.61, "sample_count": 3, "duration_seconds": 0.254}, "timestamp": "2026-01-25T20:04:02.519025"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.926, "latencies_ms": [78.926], "images_per_second": 12.67, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Toilet paper roll", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10475.5, "ram_available_mb": 112030.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [37.83], "power_watts_avg": 37.83, "power_watts_peak": 37.83, "energy_joules_est": 3.01, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T20:04:02.624584"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 104.221, "latencies_ms": [104.221], "images_per_second": 9.595, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nThe toilet is in a small bathroom stall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.5, "ram_available_mb": 112030.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10477.0, "ram_available_mb": 112029.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [36.31, 36.31], "power_watts_avg": 36.31, "power_watts_peak": 36.31, "energy_joules_est": 3.79, "sample_count": 2, "duration_seconds": 0.104}, "timestamp": "2026-01-25T20:04:02.830000"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 276.499, "latencies_ms": [276.499], "images_per_second": 3.617, "prompt_tokens": 757, "response_tokens_est": 47, "n_tiles": 1, "output_text": "\nThe image shows a small bathroom with a white toilet, a green trash can next to it, and a roll of toilet paper on the floor. The room appears to be in a state of disarray or possibly under renovation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.0, "ram_available_mb": 112029.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10470.0, "ram_available_mb": 112036.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.31, 36.31, 36.31], "power_watts_avg": 36.31, "power_watts_peak": 36.31, "energy_joules_est": 10.05, "sample_count": 3, "duration_seconds": 0.277}, "timestamp": "2026-01-25T20:04:03.135342"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 104.65, "latencies_ms": [104.65], "images_per_second": 9.556, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The toilet is white and has a green lid.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.0, "ram_available_mb": 112036.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10469.3, "ram_available_mb": 112037.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.64, 34.64], "power_watts_avg": 34.64, "power_watts_peak": 34.64, "energy_joules_est": 3.64, "sample_count": 2, "duration_seconds": 0.105}, "timestamp": "2026-01-25T20:04:03.340964"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 220.785, "latencies_ms": [220.785], "images_per_second": 4.529, "prompt_tokens": 744, "response_tokens_est": 18, "n_tiles": 1, "output_text": "iced ski slopes with a skier in red and black clothing skiing down them.\n", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10469.3, "ram_available_mb": 112037.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10469.5, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.64, 34.64, 34.64], "power_watts_avg": 34.64, "power_watts_peak": 34.64, "energy_joules_est": 7.66, "sample_count": 3, "duration_seconds": 0.221}, "timestamp": "2026-01-25T20:04:03.653234"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 135.927, "latencies_ms": [135.927], "images_per_second": 7.357, "prompt_tokens": 759, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n [0.39, 0.73, 0.48, 0.82]", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.5, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10470.2, "ram_available_mb": 112036.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.68, 33.68], "power_watts_avg": 33.68, "power_watts_peak": 33.68, "energy_joules_est": 4.59, "sample_count": 2, "duration_seconds": 0.136}, "timestamp": "2026-01-25T20:04:03.860370"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 150.543, "latencies_ms": [150.543], "images_per_second": 6.643, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nThe skier is in the foreground and appears to be skiing down a snowy mountain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.2, "ram_available_mb": 112036.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10467.1, "ram_available_mb": 112039.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [33.68, 33.68], "power_watts_avg": 33.68, "power_watts_peak": 33.68, "energy_joules_est": 5.09, "sample_count": 2, "duration_seconds": 0.151}, "timestamp": "2026-01-25T20:04:04.067974"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 158.426, "latencies_ms": [158.426], "images_per_second": 6.312, "prompt_tokens": 757, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA skier in a red jacket stands on top of a snowy mountain, surrounded by snow-covered slopes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.1, "ram_available_mb": 112039.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [32.04, 32.04], "power_watts_avg": 32.04, "power_watts_peak": 32.04, "energy_joules_est": 5.09, "sample_count": 2, "duration_seconds": 0.159}, "timestamp": "2026-01-25T20:04:04.274146"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 115.363, "latencies_ms": [115.363], "images_per_second": 8.668, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The skier is wearing a red jacket and black pants.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10463.6, "ram_available_mb": 112042.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [32.04, 32.04], "power_watts_avg": 32.04, "power_watts_peak": 32.04, "energy_joules_est": 3.71, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T20:04:04.480225"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 261.148, "latencies_ms": [261.148], "images_per_second": 3.829, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "iced skiers on a snowy mountain trail, with trees in the background and a person wearing number 57.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10463.6, "ram_available_mb": 112042.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [32.04, 26.78, 26.78], "power_watts_avg": 28.53, "power_watts_peak": 32.04, "energy_joules_est": 7.47, "sample_count": 3, "duration_seconds": 0.262}, "timestamp": "2026-01-25T20:04:04.794876"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 157.857, "latencies_ms": [157.857], "images_per_second": 6.335, "prompt_tokens": 759, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n 1: 0.12, 0.13, 0.24, 0.26", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10467.3, "ram_available_mb": 112039.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 86.0}, "power_stats": {"power_watts_samples": [26.78, 26.78], "power_watts_avg": 26.78, "power_watts_peak": 26.78, "energy_joules_est": 4.25, "sample_count": 2, "duration_seconds": 0.159}, "timestamp": "2026-01-25T20:04:05.001717"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 128.346, "latencies_ms": [128.346], "images_per_second": 7.791, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Skier in front of trees on right side.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10462.8, "ram_available_mb": 112043.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [26.78, 31.29], "power_watts_avg": 29.04, "power_watts_peak": 31.29, "energy_joules_est": 3.74, "sample_count": 2, "duration_seconds": 0.129}, "timestamp": "2026-01-25T20:04:05.209230"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 171.755, "latencies_ms": [171.755], "images_per_second": 5.822, "prompt_tokens": 757, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA man in a red and white ski suit is skiing down a snowy trail, surrounded by snow-covered trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.7, "ram_available_mb": 112043.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [31.29, 31.29], "power_watts_avg": 31.29, "power_watts_peak": 31.29, "energy_joules_est": 5.38, "sample_count": 2, "duration_seconds": 0.172}, "timestamp": "2026-01-25T20:04:05.417522"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 115.027, "latencies_ms": [115.027], "images_per_second": 8.694, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The skier is wearing a red and white shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [31.29, 31.29], "power_watts_avg": 31.29, "power_watts_peak": 31.29, "energy_joules_est": 3.62, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T20:04:05.622874"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 243.284, "latencies_ms": [243.284], "images_per_second": 4.11, "prompt_tokens": 744, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nA black and white photograph of a computer monitor, keyboard, and mouse on a desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10464.8, "ram_available_mb": 112041.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [33.64, 33.64, 33.64], "power_watts_avg": 33.64, "power_watts_peak": 33.64, "energy_joules_est": 8.19, "sample_count": 3, "duration_seconds": 0.244}, "timestamp": "2026-01-25T20:04:05.931655"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 85.254, "latencies_ms": [85.254], "images_per_second": 11.73, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n Mouse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.8, "ram_available_mb": 112041.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 42.0}, "power_stats": {"power_watts_samples": [33.64], "power_watts_avg": 33.64, "power_watts_peak": 33.64, "energy_joules_est": 2.89, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T20:04:06.037964"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 167.226, "latencies_ms": [167.226], "images_per_second": 5.98, "prompt_tokens": 763, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\n 1. A computer monitor is on top of a desk with a keyboard and mouse in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [33.64, 31.32], "power_watts_avg": 32.48, "power_watts_peak": 33.64, "energy_joules_est": 5.46, "sample_count": 2, "duration_seconds": 0.168}, "timestamp": "2026-01-25T20:04:06.246546"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 193.372, "latencies_ms": [193.372], "images_per_second": 5.171, "prompt_tokens": 757, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA black and white photo of a computer monitor with a picture of a person on its screen, accompanied by a keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10462.5, "ram_available_mb": 112043.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [31.32, 31.32], "power_watts_avg": 31.32, "power_watts_peak": 31.32, "energy_joules_est": 6.07, "sample_count": 2, "duration_seconds": 0.194}, "timestamp": "2026-01-25T20:04:06.452773"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 223.771, "latencies_ms": [223.771], "images_per_second": 4.469, "prompt_tokens": 756, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nThe image is a black and white photo of an old computer monitor with a keyboard sitting on top. The monitor displays a picture of a person's hand holding a computer mouse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.5, "ram_available_mb": 112043.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [31.32, 31.32, 32.72], "power_watts_avg": 31.79, "power_watts_peak": 32.72, "energy_joules_est": 7.12, "sample_count": 3, "duration_seconds": 0.224}, "timestamp": "2026-01-25T20:04:06.758165"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 282.836, "latencies_ms": [282.836], "images_per_second": 3.536, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA woman in a red shirt holds up a bagel on a plate, smiling at the camera while seated inside a train car.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10457.2, "ram_available_mb": 112049.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.72, 32.72, 32.72], "power_watts_avg": 32.72, "power_watts_peak": 32.72, "energy_joules_est": 9.28, "sample_count": 3, "duration_seconds": 0.284}, "timestamp": "2026-01-25T20:04:07.068656"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.446, "latencies_ms": [79.446], "images_per_second": 12.587, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Sandwich", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.2, "ram_available_mb": 112049.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [36.97], "power_watts_avg": 36.97, "power_watts_peak": 36.97, "energy_joules_est": 2.97, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T20:04:07.174942"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 125.265, "latencies_ms": [125.265], "images_per_second": 7.983, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nA woman holding a plate of food in front of her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [36.97, 36.97], "power_watts_avg": 36.97, "power_watts_peak": 36.97, "energy_joules_est": 4.64, "sample_count": 2, "duration_seconds": 0.126}, "timestamp": "2026-01-25T20:04:07.381089"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 292.316, "latencies_ms": [292.316], "images_per_second": 3.421, "prompt_tokens": 757, "response_tokens_est": 46, "n_tiles": 1, "output_text": "\nA woman in a red shirt holds up a bagel on a plate, smiling at the camera. The background shows another person sitting next to her, possibly enjoying their own meal or waiting for their turn to be served.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10458.9, "ram_available_mb": 112047.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [36.97, 36.97, 34.09], "power_watts_avg": 36.01, "power_watts_peak": 36.97, "energy_joules_est": 10.55, "sample_count": 3, "duration_seconds": 0.293}, "timestamp": "2026-01-25T20:04:07.685641"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 118.093, "latencies_ms": [118.093], "images_per_second": 8.468, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The woman is holding a plate with food on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [34.09, 34.09], "power_watts_avg": 34.09, "power_watts_peak": 34.09, "energy_joules_est": 4.03, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:04:07.890266"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 271.085, "latencies_ms": [271.085], "images_per_second": 3.689, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nTwo zebras are grazing on a lush green field, with their heads lowered to the ground as they eat grass.", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10463.5, "ram_available_mb": 112042.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [34.09, 34.09, 31.8], "power_watts_avg": 33.33, "power_watts_peak": 34.09, "energy_joules_est": 9.06, "sample_count": 3, "duration_seconds": 0.272}, "timestamp": "2026-01-25T20:04:08.204597"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 123.262, "latencies_ms": [123.262], "images_per_second": 8.113, "prompt_tokens": 759, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Zebra head 2. Zebra head", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.5, "ram_available_mb": 112042.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10458.0, "ram_available_mb": 112048.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [31.8, 31.8], "power_watts_avg": 31.8, "power_watts_peak": 31.8, "energy_joules_est": 3.95, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T20:04:08.410626"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 82.801, "latencies_ms": [82.801], "images_per_second": 12.077, "prompt_tokens": 763, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Right zebra.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10461.9, "ram_available_mb": 112044.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [31.8], "power_watts_avg": 31.8, "power_watts_peak": 31.8, "energy_joules_est": 2.64, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:04:08.517956"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 273.98, "latencies_ms": [273.98], "images_per_second": 3.65, "prompt_tokens": 757, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nTwo zebras are grazing on a lush green field, with one zebra bending down to eat grass while the other stands tall. The background features trees and bushes, creating a natural habitat for these animals.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.9, "ram_available_mb": 112044.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [31.8, 32.28, 32.28], "power_watts_avg": 32.12, "power_watts_peak": 32.28, "energy_joules_est": 8.81, "sample_count": 3, "duration_seconds": 0.274}, "timestamp": "2026-01-25T20:04:08.824186"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 315.222, "latencies_ms": [315.222], "images_per_second": 3.172, "prompt_tokens": 756, "response_tokens_est": 52, "n_tiles": 1, "output_text": "\n The image shows two zebras grazing on grass in a field. One zebra is standing closer to the camera with its head down and legs apart, while the other zebra is further away from the camera with its head up and legs together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10466.5, "ram_available_mb": 112039.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.28, 32.28, 32.28, 32.37], "power_watts_avg": 32.3, "power_watts_peak": 32.37, "energy_joules_est": 10.19, "sample_count": 4, "duration_seconds": 0.316}, "timestamp": "2026-01-25T20:04:09.230466"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 248.195, "latencies_ms": [248.195], "images_per_second": 4.029, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nTwo young men ride a green bicycle and motorcycle down a busy street, with other people walking nearby.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10466.5, "ram_available_mb": 112039.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10474.4, "ram_available_mb": 112031.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.37, 32.37, 32.37], "power_watts_avg": 32.37, "power_watts_peak": 32.37, "energy_joules_est": 8.06, "sample_count": 3, "duration_seconds": 0.249}, "timestamp": "2026-01-25T20:04:09.545907"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.179, "latencies_ms": [79.179], "images_per_second": 12.63, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bicycle: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.4, "ram_available_mb": 112031.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10479.0, "ram_available_mb": 112027.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.37], "power_watts_avg": 32.37, "power_watts_peak": 32.37, "energy_joules_est": 2.57, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T20:04:09.651467"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 129.289, "latencies_ms": [129.289], "images_per_second": 7.735, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n 1. A man riding a bike with another man on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.0, "ram_available_mb": 112027.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10477.0, "ram_available_mb": 112029.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [37.52, 37.52], "power_watts_avg": 37.52, "power_watts_peak": 37.52, "energy_joules_est": 4.86, "sample_count": 2, "duration_seconds": 0.13}, "timestamp": "2026-01-25T20:04:09.856806"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 396.26, "latencies_ms": [396.26], "images_per_second": 2.524, "prompt_tokens": 757, "response_tokens_est": 68, "n_tiles": 1, "output_text": "\nTwo boys are riding a green bicycle with one boy sitting on the back of the bike, while another boy sits in front. The street has several motorcycles parked along the side, including a motorcycle that appears to be carrying two people. In the background, there's a building with a Coca Cola sign and some potted plants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.0, "ram_available_mb": 112029.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10477.0, "ram_available_mb": 112029.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [37.52, 37.52, 37.52, 33.55], "power_watts_avg": 36.52, "power_watts_peak": 37.52, "energy_joules_est": 14.49, "sample_count": 4, "duration_seconds": 0.397}, "timestamp": "2026-01-25T20:04:10.262812"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 508.899, "latencies_ms": [508.899], "images_per_second": 1.965, "prompt_tokens": 756, "response_tokens_est": 91, "n_tiles": 1, "output_text": "\n The image shows a group of people riding bicycles and motorcycles on the street. There are two boys sitting on top of each other's bikes, with one boy wearing white clothes and the other in black. A man is also riding a motorcycle behind them. In addition to these vehicles, there are several motorbikes parked nearby. The scene takes place during daytime under bright sunlight, creating a lively atmosphere as people enjoy their ride through the city streets.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.0, "ram_available_mb": 112029.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10470.5, "ram_available_mb": 112035.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [33.55, 33.55, 33.55, 34.92, 34.92, 34.92], "power_watts_avg": 34.24, "power_watts_peak": 34.92, "energy_joules_est": 17.43, "sample_count": 6, "duration_seconds": 0.509}, "timestamp": "2026-01-25T20:04:10.869438"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 273.333, "latencies_ms": [273.333], "images_per_second": 3.659, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA tennis match is taking place on a grass court, with two players in white and black outfits engaged in an intense rally.", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10470.7, "ram_available_mb": 112035.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [34.92, 34.92, 40.48], "power_watts_avg": 36.78, "power_watts_peak": 40.48, "energy_joules_est": 10.06, "sample_count": 3, "duration_seconds": 0.274}, "timestamp": "2026-01-25T20:04:11.183165"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 66.691, "latencies_ms": [66.691], "images_per_second": 14.995, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.7, "ram_available_mb": 112035.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10472.2, "ram_available_mb": 112034.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [40.48], "power_watts_avg": 40.48, "power_watts_peak": 40.48, "energy_joules_est": 2.71, "sample_count": 1, "duration_seconds": 0.067}, "timestamp": "2026-01-25T20:04:11.288715"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 93.466, "latencies_ms": [93.466], "images_per_second": 10.699, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\nThe tennis court is full of people.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.2, "ram_available_mb": 112034.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10476.4, "ram_available_mb": 112029.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [40.48], "power_watts_avg": 40.48, "power_watts_peak": 40.48, "energy_joules_est": 3.8, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T20:04:11.393462"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 240.496, "latencies_ms": [240.496], "images_per_second": 4.158, "prompt_tokens": 757, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\nA tennis match is taking place on a grass court, with two players engaged in an intense rally. The audience is seated in the stands, watching the game intently from various angles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.4, "ram_available_mb": 112029.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10484.5, "ram_available_mb": 112021.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [40.48, 40.48, 36.29], "power_watts_avg": 39.08, "power_watts_peak": 40.48, "energy_joules_est": 9.43, "sample_count": 3, "duration_seconds": 0.241}, "timestamp": "2026-01-25T20:04:11.699223"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 62.74, "latencies_ms": [62.74], "images_per_second": 15.939, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.5, "ram_available_mb": 112021.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10484.8, "ram_available_mb": 112021.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [36.29], "power_watts_avg": 36.29, "power_watts_peak": 36.29, "energy_joules_est": 2.29, "sample_count": 1, "duration_seconds": 0.063}, "timestamp": "2026-01-25T20:04:11.804580"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 259.529, "latencies_ms": [259.529], "images_per_second": 3.853, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urns of plants hang from a white curtain rod, adding greenery to the room's decor.\n", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 10484.8, "ram_available_mb": 112021.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10484.7, "ram_available_mb": 112021.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.29, 36.29, 36.29], "power_watts_avg": 36.29, "power_watts_peak": 36.29, "energy_joules_est": 9.43, "sample_count": 3, "duration_seconds": 0.26}, "timestamp": "2026-01-25T20:04:12.115846"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 96.88, "latencies_ms": [96.88], "images_per_second": 10.322, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Couch (brown)  - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.7, "ram_available_mb": 112021.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10485.9, "ram_available_mb": 112020.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.14], "power_watts_avg": 34.14, "power_watts_peak": 34.14, "energy_joules_est": 3.33, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T20:04:12.220590"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 112.175, "latencies_ms": [112.175], "images_per_second": 8.915, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Couch in front of window with plants on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10485.9, "ram_available_mb": 112020.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10485.9, "ram_available_mb": 112020.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.14, 34.14], "power_watts_avg": 34.14, "power_watts_peak": 34.14, "energy_joules_est": 3.84, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T20:04:12.426250"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 311.686, "latencies_ms": [311.686], "images_per_second": 3.208, "prompt_tokens": 757, "response_tokens_est": 53, "n_tiles": 1, "output_text": "\nA cozy living room with a brown couch, a coffee table, and two potted plants. A television is on in front of the couch, and there are curtains covering the windows. The walls are painted white, creating a bright and airy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10485.9, "ram_available_mb": 112020.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10484.8, "ram_available_mb": 112021.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.14, 34.14, 31.79, 31.79], "power_watts_avg": 32.96, "power_watts_peak": 34.14, "energy_joules_est": 10.29, "sample_count": 4, "duration_seconds": 0.312}, "timestamp": "2026-01-25T20:04:12.832981"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 227.862, "latencies_ms": [227.862], "images_per_second": 4.389, "prompt_tokens": 756, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\n The room is painted white and has a brown couch. There are two potted plants on the wall near the window. A television is placed in front of the couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.8, "ram_available_mb": 112021.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10482.1, "ram_available_mb": 112024.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.79, 31.79, 31.79], "power_watts_avg": 31.79, "power_watts_peak": 31.79, "energy_joules_est": 7.25, "sample_count": 3, "duration_seconds": 0.228}, "timestamp": "2026-01-25T20:04:13.139787"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 258.436, "latencies_ms": [258.436], "images_per_second": 3.869, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA female tennis player in a red dress stands on an orange clay court, holding her racket and smiling.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.1, "ram_available_mb": 112024.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10482.6, "ram_available_mb": 112023.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.47, 31.47, 31.47], "power_watts_avg": 31.47, "power_watts_peak": 31.47, "energy_joules_est": 8.15, "sample_count": 3, "duration_seconds": 0.259}, "timestamp": "2026-01-25T20:04:13.449471"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.892, "latencies_ms": [78.892], "images_per_second": 12.676, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Tennis racket", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.6, "ram_available_mb": 112023.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10488.9, "ram_available_mb": 112017.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.47], "power_watts_avg": 31.47, "power_watts_peak": 31.47, "energy_joules_est": 2.5, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T20:04:13.554880"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 107.821, "latencies_ms": [107.821], "images_per_second": 9.275, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nThe woman is holding a tennis racket and smiling.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10488.7, "ram_available_mb": 112017.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10487.2, "ram_available_mb": 112019.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.47, 35.44], "power_watts_avg": 33.46, "power_watts_peak": 35.44, "energy_joules_est": 3.62, "sample_count": 2, "duration_seconds": 0.108}, "timestamp": "2026-01-25T20:04:13.761965"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 160.612, "latencies_ms": [160.612], "images_per_second": 6.226, "prompt_tokens": 757, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA female tennis player in a red dress stands on an orange clay court, holding her racket high above her head.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10487.2, "ram_available_mb": 112019.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10487.2, "ram_available_mb": 112019.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.44, 35.44], "power_watts_avg": 35.44, "power_watts_peak": 35.44, "energy_joules_est": 5.7, "sample_count": 2, "duration_seconds": 0.161}, "timestamp": "2026-01-25T20:04:13.968143"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 132.983, "latencies_ms": [132.983], "images_per_second": 7.52, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n The woman is wearing a red dress and holding a tennis racket.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10487.2, "ram_available_mb": 112019.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10489.0, "ram_available_mb": 112017.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [35.44, 31.17], "power_watts_avg": 33.31, "power_watts_peak": 35.44, "energy_joules_est": 4.44, "sample_count": 2, "duration_seconds": 0.133}, "timestamp": "2026-01-25T20:04:14.174029"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 357.566, "latencies_ms": [357.566], "images_per_second": 2.797, "prompt_tokens": 744, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nThe image shows a street scene in a small town, with several cars parked along the side of the road and buildings lining both sides. The sky is overcast, casting a soft light on the scene below.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10489.0, "ram_available_mb": 112017.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10477.6, "ram_available_mb": 112028.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [31.17, 31.17, 31.17, 31.17], "power_watts_avg": 31.17, "power_watts_peak": 31.17, "energy_joules_est": 11.16, "sample_count": 4, "duration_seconds": 0.358}, "timestamp": "2026-01-25T20:04:14.587863"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.605, "latencies_ms": [78.605], "images_per_second": 12.722, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Car: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.6, "ram_available_mb": 112028.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10477.6, "ram_available_mb": 112028.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [33.88], "power_watts_avg": 33.88, "power_watts_peak": 33.88, "energy_joules_est": 2.68, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T20:04:14.694019"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 119.939, "latencies_ms": [119.939], "images_per_second": 8.338, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe cars are parked on the side of the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.6, "ram_available_mb": 112028.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10465.4, "ram_available_mb": 112040.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [33.88, 33.88], "power_watts_avg": 33.88, "power_watts_peak": 33.88, "energy_joules_est": 4.08, "sample_count": 2, "duration_seconds": 0.12}, "timestamp": "2026-01-25T20:04:14.900669"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 427.93, "latencies_ms": [427.93], "images_per_second": 2.337, "prompt_tokens": 757, "response_tokens_est": 74, "n_tiles": 1, "output_text": "\nThe image shows a busy city street with cars parked along the side, people walking on the sidewalk, and buildings lining both sides of the road. The sky overhead appears to be overcast, casting a soft light over the scene. In addition to the vehicles and pedestrians, there are several businesses visible in the area, including a pizza shop and a restaurant.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.4, "ram_available_mb": 112040.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10467.1, "ram_available_mb": 112039.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [33.88, 33.88, 34.01, 34.01, 34.01], "power_watts_avg": 33.96, "power_watts_peak": 34.01, "energy_joules_est": 14.54, "sample_count": 5, "duration_seconds": 0.428}, "timestamp": "2026-01-25T20:04:15.409788"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 346.867, "latencies_ms": [346.867], "images_per_second": 2.883, "prompt_tokens": 756, "response_tokens_est": 58, "n_tiles": 1, "output_text": "\n The image shows a street scene with cars parked along the side of the road and people walking on the sidewalk. There are several buildings in the background, including one that has a mural painted on it. The sky is overcast, which gives the scene a somewhat muted color palette.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 10467.1, "ram_available_mb": 112039.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [34.01, 34.01, 30.9, 30.9], "power_watts_avg": 32.46, "power_watts_peak": 34.01, "energy_joules_est": 11.27, "sample_count": 4, "duration_seconds": 0.347}, "timestamp": "2026-01-25T20:04:15.816312"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 302.31, "latencies_ms": [302.31], "images_per_second": 3.308, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA female tennis player in a pink outfit is captured mid-swing, with her racket poised to strike an incoming ball on a green court.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10468.2, "ram_available_mb": 112038.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10472.6, "ram_available_mb": 112033.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [30.9, 30.9, 30.9, 38.85], "power_watts_avg": 32.89, "power_watts_peak": 38.85, "energy_joules_est": 9.95, "sample_count": 4, "duration_seconds": 0.303}, "timestamp": "2026-01-25T20:04:16.231065"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.097, "latencies_ms": [79.097], "images_per_second": 12.643, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Tennis ball", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.6, "ram_available_mb": 112033.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10471.8, "ram_available_mb": 112034.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [38.85], "power_watts_avg": 38.85, "power_watts_peak": 38.85, "energy_joules_est": 3.1, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T20:04:16.336836"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 135.136, "latencies_ms": [135.136], "images_per_second": 7.4, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nThe woman is swinging her tennis racket at a ball on a tennis court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.8, "ram_available_mb": 112034.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10469.7, "ram_available_mb": 112036.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [38.85, 38.85], "power_watts_avg": 38.85, "power_watts_peak": 38.85, "energy_joules_est": 5.26, "sample_count": 2, "duration_seconds": 0.135}, "timestamp": "2026-01-25T20:04:16.542246"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 163.171, "latencies_ms": [163.171], "images_per_second": 6.129, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA female tennis player in a pink outfit swings her racket at an incoming ball on a green tennis court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.7, "ram_available_mb": 112036.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10470.7, "ram_available_mb": 112035.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [38.85, 34.06], "power_watts_avg": 36.46, "power_watts_peak": 38.85, "energy_joules_est": 5.97, "sample_count": 2, "duration_seconds": 0.164}, "timestamp": "2026-01-25T20:04:16.749229"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 452.966, "latencies_ms": [452.966], "images_per_second": 2.208, "prompt_tokens": 756, "response_tokens_est": 77, "n_tiles": 1, "output_text": "\n The image shows a woman playing tennis on a green court. She is wearing a pink outfit and holding a tennis racket in her right hand as she prepares to hit the ball. There are two sports balls visible in the scene - one near the top left corner of the image and another closer to the center-right side, indicating that they have been actively engaged in play.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.7, "ram_available_mb": 112035.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10473.2, "ram_available_mb": 112033.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.06, 34.06, 34.06, 34.06, 30.76], "power_watts_avg": 33.4, "power_watts_peak": 34.06, "energy_joules_est": 15.15, "sample_count": 5, "duration_seconds": 0.454}, "timestamp": "2026-01-25T20:04:17.256958"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 237.722, "latencies_ms": [237.722], "images_per_second": 4.207, "prompt_tokens": 744, "response_tokens_est": 20, "n_tiles": 1, "output_text": "urns of water are on a train track, with trees in the background and bushes nearby.", "error": null, "sys_before": {"cpu_percent": 5.4, "ram_used_mb": 10473.2, "ram_available_mb": 112033.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10465.8, "ram_available_mb": 112040.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [30.76, 30.76, 30.76], "power_watts_avg": 30.76, "power_watts_peak": 30.76, "energy_joules_est": 7.33, "sample_count": 3, "duration_seconds": 0.238}, "timestamp": "2026-01-25T20:04:17.578463"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.209, "latencies_ms": [82.209], "images_per_second": 12.164, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10462.6, "ram_available_mb": 112043.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.86], "power_watts_avg": 37.86, "power_watts_peak": 37.86, "energy_joules_est": 3.14, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:04:17.684207"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 137.897, "latencies_ms": [137.897], "images_per_second": 7.252, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. Train on tracks 2. Bushes and trees 3. Grass", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.6, "ram_available_mb": 112043.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10460.6, "ram_available_mb": 112045.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.86, 37.86], "power_watts_avg": 37.86, "power_watts_peak": 37.86, "energy_joules_est": 5.23, "sample_count": 2, "duration_seconds": 0.138}, "timestamp": "2026-01-25T20:04:17.888918"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 239.416, "latencies_ms": [239.416], "images_per_second": 4.177, "prompt_tokens": 757, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA blue, white, and orange train travels down a track surrounded by tall grass. The train appears to be moving through a wooded area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.6, "ram_available_mb": 112045.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10455.9, "ram_available_mb": 112050.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [37.86, 37.86, 33.65], "power_watts_avg": 36.46, "power_watts_peak": 37.86, "energy_joules_est": 8.75, "sample_count": 3, "duration_seconds": 0.24}, "timestamp": "2026-01-25T20:04:18.195739"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 104.68, "latencies_ms": [104.68], "images_per_second": 9.553, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nThe train is silver and blue with orange stripes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.9, "ram_available_mb": 112050.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10456.0, "ram_available_mb": 112050.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [33.65, 33.65], "power_watts_avg": 33.65, "power_watts_peak": 33.65, "energy_joules_est": 3.53, "sample_count": 2, "duration_seconds": 0.105}, "timestamp": "2026-01-25T20:04:18.402350"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 245.426, "latencies_ms": [245.426], "images_per_second": 4.075, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nTwo cats, one brown and one gray, are sleeping on a pink blanket atop a couch.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10456.0, "ram_available_mb": 112050.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [33.65, 33.65, 30.16], "power_watts_avg": 32.49, "power_watts_peak": 33.65, "energy_joules_est": 7.98, "sample_count": 3, "duration_seconds": 0.246}, "timestamp": "2026-01-25T20:04:18.716322"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 93.14, "latencies_ms": [93.14], "images_per_second": 10.737, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Remote control - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.1, "ram_available_mb": 112054.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10451.6, "ram_available_mb": 112054.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [30.16], "power_watts_avg": 30.16, "power_watts_peak": 30.16, "energy_joules_est": 2.83, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T20:04:18.821484"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 124.202, "latencies_ms": [124.202], "images_per_second": 8.051, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n 1. Remote control on couch cushion next to cat laying down.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.6, "ram_available_mb": 112054.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10452.7, "ram_available_mb": 112053.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [30.16, 30.16], "power_watts_avg": 30.16, "power_watts_peak": 30.16, "energy_joules_est": 3.75, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T20:04:19.026950"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 164.785, "latencies_ms": [164.785], "images_per_second": 6.069, "prompt_tokens": 757, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nTwo cats are sleeping on a pink blanket, one cat is laying down while the other cat is sitting up.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10452.7, "ram_available_mb": 112053.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10452.4, "ram_available_mb": 112053.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 36.0}, "power_stats": {"power_watts_samples": [30.16, 30.81], "power_watts_avg": 30.49, "power_watts_peak": 30.81, "energy_joules_est": 5.04, "sample_count": 2, "duration_seconds": 0.165}, "timestamp": "2026-01-25T20:04:19.233830"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 92.481, "latencies_ms": [92.481], "images_per_second": 10.813, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The cat is brown and gray.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.4, "ram_available_mb": 112053.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 36.0}, "power_stats": {"power_watts_samples": [30.81], "power_watts_avg": 30.81, "power_watts_peak": 30.81, "energy_joules_est": 2.86, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:04:19.339546"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 267.172, "latencies_ms": [267.172], "images_per_second": 3.743, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA surfer in a black wetsuit stands on their board, riding a wave near a bridge and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 36.0}, "power_stats": {"power_watts_samples": [30.81, 30.81, 30.81], "power_watts_avg": 30.81, "power_watts_peak": 30.81, "energy_joules_est": 8.24, "sample_count": 3, "duration_seconds": 0.267}, "timestamp": "2026-01-25T20:04:19.651341"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 241.105, "latencies_ms": [241.105], "images_per_second": 4.148, "prompt_tokens": 759, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\n 1. Bridge  2. Tree  3. River  4. Person in black wet suit  5. Surfer  6. Surfboard  7. Person in red shirt", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.7, "ram_available_mb": 112052.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10457.8, "ram_available_mb": 112048.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 36.0}, "power_stats": {"power_watts_samples": [33.25, 33.25, 33.25], "power_watts_avg": 33.25, "power_watts_peak": 33.25, "energy_joules_est": 8.05, "sample_count": 3, "duration_seconds": 0.242}, "timestamp": "2026-01-25T20:04:19.958909"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 174.961, "latencies_ms": [174.961], "images_per_second": 5.716, "prompt_tokens": 763, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA man in a black wetsuit is surfing on a river while another surfer stands nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.8, "ram_available_mb": 112048.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10457.0, "ram_available_mb": 112049.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [33.25, 33.25], "power_watts_avg": 33.25, "power_watts_peak": 33.25, "energy_joules_est": 5.84, "sample_count": 2, "duration_seconds": 0.176}, "timestamp": "2026-01-25T20:04:20.165852"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 170.345, "latencies_ms": [170.345], "images_per_second": 5.87, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA man in a black wetsuit stands on his surfboard, riding a wave in a river.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.0, "ram_available_mb": 112049.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [35.19, 35.19], "power_watts_avg": 35.19, "power_watts_peak": 35.19, "energy_joules_est": 6.02, "sample_count": 2, "duration_seconds": 0.171}, "timestamp": "2026-01-25T20:04:20.370999"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 440.736, "latencies_ms": [440.736], "images_per_second": 2.269, "prompt_tokens": 756, "response_tokens_est": 77, "n_tiles": 1, "output_text": "\n The image shows a man surfing on a river. He is wearing a black wetsuit and standing on his surfboard in the water. The river has white foam coming off of it, indicating that there might be strong currents or waves. In the background, there are trees and buildings visible along with other people present who appear to be watching the surfer's performance.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [35.19, 35.19, 32.84, 32.84, 32.84], "power_watts_avg": 33.78, "power_watts_peak": 35.19, "energy_joules_est": 14.9, "sample_count": 5, "duration_seconds": 0.441}, "timestamp": "2026-01-25T20:04:20.877184"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 272.797, "latencies_ms": [272.797], "images_per_second": 3.666, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA woman and a child are flying a large, colorful kite in an open field with trees in the background.", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 10460.0, "ram_available_mb": 112046.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [32.84, 32.84, 37.82], "power_watts_avg": 34.5, "power_watts_peak": 37.82, "energy_joules_est": 9.43, "sample_count": 3, "duration_seconds": 0.273}, "timestamp": "2026-01-25T20:04:21.192121"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 226.621, "latencies_ms": [226.621], "images_per_second": 4.413, "prompt_tokens": 759, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\n 1. Tree  2. Tree  3. Tree  4. Tree  5. Tree  6. Tree  7. Tree  8. Tree", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [37.82, 37.82, 37.82], "power_watts_avg": 37.82, "power_watts_peak": 37.82, "energy_joules_est": 8.58, "sample_count": 3, "duration_seconds": 0.227}, "timestamp": "2026-01-25T20:04:21.498977"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 148.07, "latencies_ms": [148.07], "images_per_second": 6.754, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nA woman and a child are flying a kite in an open field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10451.9, "ram_available_mb": 112054.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [37.82, 36.41], "power_watts_avg": 37.12, "power_watts_peak": 37.82, "energy_joules_est": 5.52, "sample_count": 2, "duration_seconds": 0.149}, "timestamp": "2026-01-25T20:04:21.706022"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 232.155, "latencies_ms": [232.155], "images_per_second": 4.307, "prompt_tokens": 757, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA woman in a black jacket holds a colorful kite while standing on a grassy field. A child stands next to her, also holding onto the kite string.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.9, "ram_available_mb": 112054.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10452.2, "ram_available_mb": 112054.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [36.41, 36.41, 36.41], "power_watts_avg": 36.41, "power_watts_peak": 36.41, "energy_joules_est": 8.46, "sample_count": 3, "duration_seconds": 0.232}, "timestamp": "2026-01-25T20:04:22.014637"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 213.028, "latencies_ms": [213.028], "images_per_second": 4.694, "prompt_tokens": 756, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\n The woman is wearing a black jacket and blue jeans. She has her arms outstretched as she flies the kite with her daughter in front of her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.2, "ram_available_mb": 112054.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10456.4, "ram_available_mb": 112049.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [36.41, 30.94, 30.94], "power_watts_avg": 32.76, "power_watts_peak": 36.41, "energy_joules_est": 7.0, "sample_count": 3, "duration_seconds": 0.214}, "timestamp": "2026-01-25T20:04:22.321613"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 328.483, "latencies_ms": [328.483], "images_per_second": 3.044, "prompt_tokens": 744, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nThe image features a young man playing tennis on a court, holding a yellow racket and preparing to hit an incoming ball. The player is wearing a white shirt and black shorts.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10456.4, "ram_available_mb": 112049.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [30.94, 30.94, 30.94, 35.16], "power_watts_avg": 32.0, "power_watts_peak": 35.16, "energy_joules_est": 10.53, "sample_count": 4, "duration_seconds": 0.329}, "timestamp": "2026-01-25T20:04:22.734909"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 148.689, "latencies_ms": [148.689], "images_per_second": 6.725, "prompt_tokens": 759, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n [0.39, 0.13, 0.63, 0.69]", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10457.8, "ram_available_mb": 112048.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [35.16, 35.16], "power_watts_avg": 35.16, "power_watts_peak": 35.16, "energy_joules_est": 5.24, "sample_count": 2, "duration_seconds": 0.149}, "timestamp": "2026-01-25T20:04:22.942587"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 121.038, "latencies_ms": [121.038], "images_per_second": 8.262, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nThe boy is playing tennis and swinging his racket at a ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.8, "ram_available_mb": 112048.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 22.0}, "power_stats": {"power_watts_samples": [35.16, 35.16], "power_watts_avg": 35.16, "power_watts_peak": 35.16, "energy_joules_est": 4.27, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T20:04:23.147758"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 401.547, "latencies_ms": [401.547], "images_per_second": 2.49, "prompt_tokens": 757, "response_tokens_est": 68, "n_tiles": 1, "output_text": "\nIn this image, a young man wearing a white shirt and black shorts is playing tennis on a court. He is holding a yellow tennis racket in his right hand as he prepares to hit an incoming ball with his left hand. The background features a green wall adorned with advertisements for the National Junior Masters and Nike Junior Athletes.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 22.0}, "power_stats": {"power_watts_samples": [35.02, 35.02, 35.02, 35.02, 35.02], "power_watts_avg": 35.02, "power_watts_peak": 35.02, "energy_joules_est": 14.08, "sample_count": 5, "duration_seconds": 0.402}, "timestamp": "2026-01-25T20:04:23.653860"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 246.028, "latencies_ms": [246.028], "images_per_second": 4.065, "prompt_tokens": 756, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\n The image shows a young man playing tennis on the court. He is wearing a white shirt and black shorts while holding a yellow tennis racket in his right hand as he prepares to hit the ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.2, "ram_available_mb": 112048.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 22.0}, "power_stats": {"power_watts_samples": [32.93, 32.93, 32.93], "power_watts_avg": 32.93, "power_watts_peak": 32.93, "energy_joules_est": 8.12, "sample_count": 3, "duration_seconds": 0.246}, "timestamp": "2026-01-25T20:04:23.960388"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 303.274, "latencies_ms": [303.274], "images_per_second": 3.297, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "urns of water and a candle sit on a wooden table in a cluttered bedroom, with a bed covered in blankets and a chair nearby.\n", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10462.2, "ram_available_mb": 112044.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [32.93, 32.88, 32.88, 32.88], "power_watts_avg": 32.89, "power_watts_peak": 32.93, "energy_joules_est": 9.98, "sample_count": 4, "duration_seconds": 0.304}, "timestamp": "2026-01-25T20:04:24.374235"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 94.407, "latencies_ms": [94.407], "images_per_second": 10.592, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bed: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.2, "ram_available_mb": 112044.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10462.0, "ram_available_mb": 112044.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [32.88], "power_watts_avg": 32.88, "power_watts_peak": 32.88, "energy_joules_est": 3.12, "sample_count": 1, "duration_seconds": 0.095}, "timestamp": "2026-01-25T20:04:24.479971"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 126.525, "latencies_ms": [126.525], "images_per_second": 7.904, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A bed with a brown blanket on it is in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.0, "ram_available_mb": 112044.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10461.4, "ram_available_mb": 112044.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [32.88, 32.62], "power_watts_avg": 32.75, "power_watts_peak": 32.88, "energy_joules_est": 4.15, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:04:24.684991"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 667.714, "latencies_ms": [667.714], "images_per_second": 1.498, "prompt_tokens": 757, "response_tokens_est": 120, "n_tiles": 1, "output_text": "\nThe image shows a bedroom with a bed, chair, and various items scattered around. The room has white walls and a brick wall on one side. On top of the bed are two pillows, and there are several books and bags in the room as well. A door can be seen in the background, suggesting that it leads to another part of the house or apartment.\n\nThe bedroom appears to be messy with clothes and other items strewn about, indicating a disorganized living situation. The presence of a chair suggests that this space is used for relaxation and possibly work as well.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.4, "ram_available_mb": 112044.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.62, 32.62, 32.62, 32.62, 35.02, 35.02, 35.02], "power_watts_avg": 33.65, "power_watts_peak": 35.02, "energy_joules_est": 22.48, "sample_count": 7, "duration_seconds": 0.668}, "timestamp": "2026-01-25T20:04:25.392989"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 117.2, "latencies_ms": [117.2], "images_per_second": 8.532, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The room is dark and has a brick wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.02, 35.02], "power_watts_avg": 35.02, "power_watts_peak": 35.02, "energy_joules_est": 4.11, "sample_count": 2, "duration_seconds": 0.117}, "timestamp": "2026-01-25T20:04:25.598518"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 287.841, "latencies_ms": [287.841], "images_per_second": 3.474, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA person in a red, green, and yellow jacket rides a brown horse over a wooden fence obstacle during an equestrian event.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10459.6, "ram_available_mb": 112046.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.77, 35.77, 35.77], "power_watts_avg": 35.77, "power_watts_peak": 35.77, "energy_joules_est": 10.32, "sample_count": 3, "duration_seconds": 0.288}, "timestamp": "2026-01-25T20:04:25.909171"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 88.642, "latencies_ms": [88.642], "images_per_second": 11.281, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Horse: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.6, "ram_available_mb": 112046.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10459.2, "ram_available_mb": 112047.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.77], "power_watts_avg": 35.77, "power_watts_peak": 35.77, "energy_joules_est": 3.19, "sample_count": 1, "duration_seconds": 0.089}, "timestamp": "2026-01-25T20:04:26.013173"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 100.581, "latencies_ms": [100.581], "images_per_second": 9.942, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nThe horse is jumping over a wooden fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.2, "ram_available_mb": 112047.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10457.3, "ram_available_mb": 112049.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [35.77, 36.01], "power_watts_avg": 35.89, "power_watts_peak": 36.01, "energy_joules_est": 3.62, "sample_count": 2, "duration_seconds": 0.101}, "timestamp": "2026-01-25T20:04:26.219441"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 253.734, "latencies_ms": [253.734], "images_per_second": 3.941, "prompt_tokens": 757, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nA person wearing a red, green, and yellow jacket rides a brown horse over a wooden fence. The rider appears to be in mid-jump as they navigate the obstacle course.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.3, "ram_available_mb": 112049.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10460.8, "ram_available_mb": 112045.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [36.01, 36.01, 36.01], "power_watts_avg": 36.01, "power_watts_peak": 36.01, "energy_joules_est": 9.15, "sample_count": 3, "duration_seconds": 0.254}, "timestamp": "2026-01-25T20:04:26.523908"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 263.878, "latencies_ms": [263.878], "images_per_second": 3.79, "prompt_tokens": 756, "response_tokens_est": 42, "n_tiles": 1, "output_text": "\n The image shows a person riding on the back of a brown horse. The rider is wearing a red and green jacket with a white helmet for safety while they jump over an obstacle in front of them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.8, "ram_available_mb": 112045.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 10463.7, "ram_available_mb": 112042.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [36.01, 32.28, 32.28], "power_watts_avg": 33.53, "power_watts_peak": 36.01, "energy_joules_est": 8.86, "sample_count": 3, "duration_seconds": 0.264}, "timestamp": "2026-01-25T20:04:26.830606"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 287.844, "latencies_ms": [287.844], "images_per_second": 3.474, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nTwo men sit under a large umbrella, with one man holding a pair of shoes and the other sitting next to him on a sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.7, "ram_available_mb": 112042.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [32.28, 32.28, 32.28], "power_watts_avg": 32.28, "power_watts_peak": 32.28, "energy_joules_est": 9.33, "sample_count": 3, "duration_seconds": 0.289}, "timestamp": "2026-01-25T20:04:27.139593"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 66.597, "latencies_ms": [66.597], "images_per_second": 15.016, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Umbrella: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10479.4, "ram_available_mb": 112026.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [35.71], "power_watts_avg": 35.71, "power_watts_peak": 35.71, "energy_joules_est": 2.38, "sample_count": 1, "duration_seconds": 0.067}, "timestamp": "2026-01-25T20:04:27.244747"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 377.329, "latencies_ms": [377.329], "images_per_second": 2.65, "prompt_tokens": 763, "response_tokens_est": 63, "n_tiles": 1, "output_text": "\nThe man sitting under the umbrella is on the left side of the image. The other man is seated further to his right and appears to be looking at something in front of him. There are two bicycles parked nearby, one closer to the center of the scene and another towards the right edge of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.4, "ram_available_mb": 112026.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10474.2, "ram_available_mb": 112032.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [35.71, 35.71, 35.71, 35.71], "power_watts_avg": 35.71, "power_watts_peak": 35.71, "energy_joules_est": 13.49, "sample_count": 4, "duration_seconds": 0.378}, "timestamp": "2026-01-25T20:04:27.653784"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 347.455, "latencies_ms": [347.455], "images_per_second": 2.878, "prompt_tokens": 757, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\nTwo men are sitting under a large umbrella, one of them wearing white pants. They have their feet propped up on a table with various tools around them. A car can be seen in the background, indicating that they might be in an urban setting or near a street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.2, "ram_available_mb": 112032.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [38.8, 38.8, 38.8, 38.8], "power_watts_avg": 38.8, "power_watts_peak": 38.8, "energy_joules_est": 13.5, "sample_count": 4, "duration_seconds": 0.348}, "timestamp": "2026-01-25T20:04:28.060876"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 124.397, "latencies_ms": [124.397], "images_per_second": 8.039, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. The umbrella is white and black with Chinese characters on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [38.8, 35.7], "power_watts_avg": 37.25, "power_watts_peak": 38.8, "energy_joules_est": 4.66, "sample_count": 2, "duration_seconds": 0.125}, "timestamp": "2026-01-25T20:04:28.266767"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 273.633, "latencies_ms": [273.633], "images_per_second": 3.655, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "urns and other knick-knacks are displayed on a window sill, with a white dishwasher in the background.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [35.7, 35.7, 35.7], "power_watts_avg": 35.7, "power_watts_peak": 35.7, "energy_joules_est": 9.8, "sample_count": 3, "duration_seconds": 0.274}, "timestamp": "2026-01-25T20:04:28.581165"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 90.552, "latencies_ms": [90.552], "images_per_second": 11.043, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Ceiling fan: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10464.3, "ram_available_mb": 112042.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [36.99], "power_watts_avg": 36.99, "power_watts_peak": 36.99, "energy_joules_est": 3.36, "sample_count": 1, "duration_seconds": 0.091}, "timestamp": "2026-01-25T20:04:28.687243"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 97.76, "latencies_ms": [97.76], "images_per_second": 10.229, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. White dishwasher in kitchen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.3, "ram_available_mb": 112042.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 87.0}, "power_stats": {"power_watts_samples": [36.99], "power_watts_avg": 36.99, "power_watts_peak": 36.99, "energy_joules_est": 3.63, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T20:04:28.791514"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 391.618, "latencies_ms": [391.618], "images_per_second": 2.554, "prompt_tokens": 757, "response_tokens_est": 68, "n_tiles": 1, "output_text": "\nThe image shows a small, white kitchen with black countertops. The kitchen features a sink, stove, dishwasher, and a ceiling fan. There are several items on the counters, including multiple bottles, a knife, and a bowl. A window above the sink allows natural light to enter the room, creating a bright atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10462.2, "ram_available_mb": 112044.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.99, 36.99, 36.99, 35.32], "power_watts_avg": 36.57, "power_watts_peak": 36.99, "energy_joules_est": 14.33, "sample_count": 4, "duration_seconds": 0.392}, "timestamp": "2026-01-25T20:04:29.197262"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 225.975, "latencies_ms": [225.975], "images_per_second": 4.425, "prompt_tokens": 756, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\n The kitchen is white with black countertops and a brown ceiling. There are two windows in the room that let in natural light. A ceiling fan hangs from the ceiling.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10462.2, "ram_available_mb": 112044.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10465.7, "ram_available_mb": 112040.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.32, 35.32, 35.32], "power_watts_avg": 35.32, "power_watts_peak": 35.32, "energy_joules_est": 7.99, "sample_count": 3, "duration_seconds": 0.226}, "timestamp": "2026-01-25T20:04:29.502800"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 250.757, "latencies_ms": [250.757], "images_per_second": 3.988, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urn of water on a bed, with a child sleeping next to it and smiling at the camera.\n", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10465.7, "ram_available_mb": 112040.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.32, 33.14, 33.14], "power_watts_avg": 33.87, "power_watts_peak": 35.32, "energy_joules_est": 8.5, "sample_count": 3, "duration_seconds": 0.251}, "timestamp": "2026-01-25T20:04:29.812218"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 91.328, "latencies_ms": [91.328], "images_per_second": 10.95, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Lamp - 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.14], "power_watts_avg": 33.14, "power_watts_peak": 33.14, "energy_joules_est": 3.04, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T20:04:29.918307"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 141.38, "latencies_ms": [141.38], "images_per_second": 7.073, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA young child is sleeping on a bed with a blue and white floral blanket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10457.6, "ram_available_mb": 112048.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [33.14, 33.14], "power_watts_avg": 33.14, "power_watts_peak": 33.14, "energy_joules_est": 4.69, "sample_count": 2, "duration_seconds": 0.142}, "timestamp": "2026-01-25T20:04:30.123974"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 225.681, "latencies_ms": [225.681], "images_per_second": 4.431, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA young child is lying in bed, smiling and holding a pacifier. The bed has a blue and white floral comforter, and there are trains on the sheets.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 10457.6, "ram_available_mb": 112048.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10458.4, "ram_available_mb": 112047.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [36.13, 36.13, 36.13], "power_watts_avg": 36.13, "power_watts_peak": 36.13, "energy_joules_est": 8.17, "sample_count": 3, "duration_seconds": 0.226}, "timestamp": "2026-01-25T20:04:30.429440"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 123.333, "latencies_ms": [123.333], "images_per_second": 8.108, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A child is laying in bed with a white and blue blanket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.4, "ram_available_mb": 112047.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10461.8, "ram_available_mb": 112044.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [36.13, 36.13], "power_watts_avg": 36.13, "power_watts_peak": 36.13, "energy_joules_est": 4.48, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T20:04:30.637546"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 345.973, "latencies_ms": [345.973], "images_per_second": 2.89, "prompt_tokens": 744, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\nA green highway sign with white text reading \"NO TRUCKS\" and \"EAST QUEENS BRONX\" is displayed on a metal structure, possibly an overpass or bridge.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10461.8, "ram_available_mb": 112044.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [34.69, 34.69, 34.69, 34.69], "power_watts_avg": 34.69, "power_watts_peak": 34.69, "energy_joules_est": 12.03, "sample_count": 4, "duration_seconds": 0.347}, "timestamp": "2026-01-25T20:04:31.049645"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 298.723, "latencies_ms": [298.723], "images_per_second": 3.348, "prompt_tokens": 759, "response_tokens_est": 47, "n_tiles": 1, "output_text": "\n 1. No Trucks sign 2. East bound sign 3. Queens bound sign 4. Bronx bound sign 5. Eastbound transportation sign 6. Queens bound sign 7. Bronx bound sign 8. Eastbound transportation sign", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 76.0}, "power_stats": {"power_watts_samples": [34.69, 34.33, 34.33], "power_watts_avg": 34.45, "power_watts_peak": 34.69, "energy_joules_est": 10.3, "sample_count": 3, "duration_seconds": 0.299}, "timestamp": "2026-01-25T20:04:31.357816"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 124.782, "latencies_ms": [124.782], "images_per_second": 8.014, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. No trucks sign on top of green highway sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 76.0}, "power_stats": {"power_watts_samples": [34.33, 34.33], "power_watts_avg": 34.33, "power_watts_peak": 34.33, "energy_joules_est": 4.3, "sample_count": 2, "duration_seconds": 0.125}, "timestamp": "2026-01-25T20:04:31.563580"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 289.86, "latencies_ms": [289.86], "images_per_second": 3.45, "prompt_tokens": 757, "response_tokens_est": 50, "n_tiles": 1, "output_text": "\nThe image shows a green highway sign that says \"No Trucks\" with the number \"270\", indicating that trucks are not allowed to pass through this area. The sign is located on East Queensboro Road in Queens, New York City.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 76.0}, "power_stats": {"power_watts_samples": [35.62, 35.62, 35.62], "power_watts_avg": 35.62, "power_watts_peak": 35.62, "energy_joules_est": 10.34, "sample_count": 3, "duration_seconds": 0.29}, "timestamp": "2026-01-25T20:04:31.869009"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 340.779, "latencies_ms": [340.779], "images_per_second": 2.934, "prompt_tokens": 756, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\n The image shows a green highway sign that says \"No Trucks\" and \"Queens Bronx\". There is also a white sign with the number \"270\" on it. The scene takes place in front of a bridge or overpass, which suggests an urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10471.0, "ram_available_mb": 112035.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [35.62, 35.62, 34.84, 34.84], "power_watts_avg": 35.23, "power_watts_peak": 35.62, "energy_joules_est": 12.01, "sample_count": 4, "duration_seconds": 0.341}, "timestamp": "2026-01-25T20:04:32.275729"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 349.181, "latencies_ms": [349.181], "images_per_second": 2.864, "prompt_tokens": 744, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\nA red Chevrolet truck is parked in a parking lot, facing away from the camera and displaying its rear lights on. The truck has a large bed and chrome accents that add to its vintage appeal.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10471.0, "ram_available_mb": 112035.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10471.0, "ram_available_mb": 112035.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [34.84, 34.84, 34.84, 39.63], "power_watts_avg": 36.04, "power_watts_peak": 39.63, "energy_joules_est": 12.61, "sample_count": 4, "duration_seconds": 0.35}, "timestamp": "2026-01-25T20:04:32.689394"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 74.257, "latencies_ms": [74.257], "images_per_second": 13.467, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Truck", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.0, "ram_available_mb": 112035.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10474.0, "ram_available_mb": 112032.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [39.63], "power_watts_avg": 39.63, "power_watts_peak": 39.63, "energy_joules_est": 2.95, "sample_count": 1, "duration_seconds": 0.074}, "timestamp": "2026-01-25T20:04:32.794553"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 127.327, "latencies_ms": [127.327], "images_per_second": 7.854, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nThe red truck is parked in a parking lot with other cars.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.0, "ram_available_mb": 112032.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [39.63, 39.63], "power_watts_avg": 39.63, "power_watts_peak": 39.63, "energy_joules_est": 5.07, "sample_count": 2, "duration_seconds": 0.128}, "timestamp": "2026-01-25T20:04:33.001342"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 437.908, "latencies_ms": [437.908], "images_per_second": 2.284, "prompt_tokens": 757, "response_tokens_est": 76, "n_tiles": 1, "output_text": "\nA red Chevrolet truck with chrome accents is parked in a parking lot, facing away from the camera. The truck has a large rear bumper and a small front bumper, giving it an old-fashioned appearance. In the background, there are other cars visible, indicating that this might be a busy location or a popular spot for car enthusiasts to gather and admire classic vehicles.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10474.9, "ram_available_mb": 112031.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [39.63, 33.95, 33.95, 33.95, 33.95], "power_watts_avg": 35.09, "power_watts_peak": 39.63, "energy_joules_est": 15.39, "sample_count": 5, "duration_seconds": 0.439}, "timestamp": "2026-01-25T20:04:33.509877"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 135.073, "latencies_ms": [135.073], "images_per_second": 7.403, "prompt_tokens": 756, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n The red truck is parked in a parking lot with the sky above it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.9, "ram_available_mb": 112031.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [33.95, 31.48], "power_watts_avg": 32.72, "power_watts_peak": 33.95, "energy_joules_est": 4.43, "sample_count": 2, "duration_seconds": 0.135}, "timestamp": "2026-01-25T20:04:33.716164"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 285.491, "latencies_ms": [285.491], "images_per_second": 3.503, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nThree cows are standing behind a barbed wire fence in a field, with their heads peeking over the top of the fence.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10468.3, "ram_available_mb": 112038.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [31.48, 31.48, 31.48], "power_watts_avg": 31.48, "power_watts_peak": 31.48, "energy_joules_est": 9.01, "sample_count": 3, "duration_seconds": 0.286}, "timestamp": "2026-01-25T20:04:34.029297"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.155, "latencies_ms": [78.155], "images_per_second": 12.795, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Cow in foreground", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.3, "ram_available_mb": 112038.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10468.3, "ram_available_mb": 112038.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.48], "power_watts_avg": 31.48, "power_watts_peak": 31.48, "energy_joules_est": 2.47, "sample_count": 1, "duration_seconds": 0.078}, "timestamp": "2026-01-25T20:04:34.134175"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 108.706, "latencies_ms": [108.706], "images_per_second": 9.199, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Cow in front of other cows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.3, "ram_available_mb": 112038.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [37.58, 37.58], "power_watts_avg": 37.58, "power_watts_peak": 37.58, "energy_joules_est": 4.1, "sample_count": 2, "duration_seconds": 0.109}, "timestamp": "2026-01-25T20:04:34.341943"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 253.51, "latencies_ms": [253.51], "images_per_second": 3.945, "prompt_tokens": 757, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\nA group of cows, including a white cow with horns, are standing behind a barbed wire fence in a field. The cows appear to be looking at something or someone outside the frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [37.58, 37.58, 37.58], "power_watts_avg": 37.58, "power_watts_peak": 37.58, "energy_joules_est": 9.55, "sample_count": 3, "duration_seconds": 0.254}, "timestamp": "2026-01-25T20:04:34.649166"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 124.421, "latencies_ms": [124.421], "images_per_second": 8.037, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. The cows are black and white in color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.98, 33.98], "power_watts_avg": 33.98, "power_watts_peak": 33.98, "energy_joules_est": 4.25, "sample_count": 2, "duration_seconds": 0.125}, "timestamp": "2026-01-25T20:04:34.854924"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 253.329, "latencies_ms": [253.329], "images_per_second": 3.947, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "urns of water are on a table in front of a fireplace, creating an inviting atmosphere for relaxation and warmth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [33.98, 33.98, 33.98], "power_watts_avg": 33.98, "power_watts_peak": 33.98, "energy_joules_est": 8.62, "sample_count": 3, "duration_seconds": 0.254}, "timestamp": "2026-01-25T20:04:35.167178"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.905, "latencies_ms": [79.905], "images_per_second": 12.515, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Bed", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [32.16], "power_watts_avg": 32.16, "power_watts_peak": 32.16, "energy_joules_est": 2.59, "sample_count": 1, "duration_seconds": 0.081}, "timestamp": "2026-01-25T20:04:35.273701"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 81.144, "latencies_ms": [81.144], "images_per_second": 12.324, "prompt_tokens": 763, "response_tokens_est": 6, "n_tiles": 1, "output_text": "urn on fireplace mantel", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [32.16], "power_watts_avg": 32.16, "power_watts_peak": 32.16, "energy_joules_est": 2.63, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T20:04:35.379883"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 328.498, "latencies_ms": [328.498], "images_per_second": 3.044, "prompt_tokens": 757, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\nThe image shows a cozy bedroom with a large bed, a fireplace, and a television. The room has wooden walls and ceiling, giving it a warm and inviting atmosphere. A clock can be seen on one of the walls, providing both function and decoration to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10470.2, "ram_available_mb": 112036.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [32.16, 32.16, 32.45, 32.45], "power_watts_avg": 32.3, "power_watts_peak": 32.45, "energy_joules_est": 10.63, "sample_count": 4, "duration_seconds": 0.329}, "timestamp": "2026-01-25T20:04:35.788205"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 69.839, "latencies_ms": [69.839], "images_per_second": 14.319, "prompt_tokens": 756, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urns on the fireplace.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.2, "ram_available_mb": 112036.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [32.45], "power_watts_avg": 32.45, "power_watts_peak": 32.45, "energy_joules_est": 2.27, "sample_count": 1, "duration_seconds": 0.07}, "timestamp": "2026-01-25T20:04:35.893611"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 258.296, "latencies_ms": [258.296], "images_per_second": 3.872, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urns of water are visible in the background, with a group of three birds standing on dry grass and shrubs.", "error": null, "sys_before": {"cpu_percent": 6.9, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [32.45, 32.45, 33.66], "power_watts_avg": 32.85, "power_watts_peak": 33.66, "energy_joules_est": 8.51, "sample_count": 3, "duration_seconds": 0.259}, "timestamp": "2026-01-25T20:04:36.210255"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 87.909, "latencies_ms": [87.909], "images_per_second": 11.375, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Bird: 0.5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10470.1, "ram_available_mb": 112036.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [33.66], "power_watts_avg": 33.66, "power_watts_peak": 33.66, "energy_joules_est": 2.98, "sample_count": 1, "duration_seconds": 0.089}, "timestamp": "2026-01-25T20:04:36.316993"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 129.165, "latencies_ms": [129.165], "images_per_second": 7.742, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nTwo birds on a hillside with one bird in front of another.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.1, "ram_available_mb": 112036.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10472.6, "ram_available_mb": 112033.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [33.66, 33.66], "power_watts_avg": 33.66, "power_watts_peak": 33.66, "energy_joules_est": 4.37, "sample_count": 2, "duration_seconds": 0.13}, "timestamp": "2026-01-25T20:04:36.523486"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 118.275, "latencies_ms": [118.275], "images_per_second": 8.455, "prompt_tokens": 757, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n Three black and white birds are walking on a dry grassy field.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 10472.6, "ram_available_mb": 112033.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [33.66, 33.52], "power_watts_avg": 33.59, "power_watts_peak": 33.66, "energy_joules_est": 3.98, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:04:36.730021"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 497.913, "latencies_ms": [497.913], "images_per_second": 2.008, "prompt_tokens": 756, "response_tokens_est": 88, "n_tiles": 1, "output_text": "\n The image shows a group of three birds standing on the ground. One bird is black and white with a red beak, another is gray and white with a blue head, and the third one is brown and white with a black head. They are all facing different directions, possibly observing something in their surroundings or simply enjoying each other's company. The background features some bushes and trees, adding to the natural setting of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10481.3, "ram_available_mb": 112025.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.52, 33.52, 33.52, 33.52, 32.3], "power_watts_avg": 33.27, "power_watts_peak": 33.52, "energy_joules_est": 16.57, "sample_count": 5, "duration_seconds": 0.498}, "timestamp": "2026-01-25T20:04:37.237027"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 261.462, "latencies_ms": [261.462], "images_per_second": 3.825, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "iced tea and a brown bag are present in the scene, with three people standing on skis in the snow.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10481.3, "ram_available_mb": 112025.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10477.4, "ram_available_mb": 112028.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.3, 32.3, 32.3], "power_watts_avg": 32.3, "power_watts_peak": 32.3, "energy_joules_est": 8.45, "sample_count": 3, "duration_seconds": 0.262}, "timestamp": "2026-01-25T20:04:37.550115"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 194.389, "latencies_ms": [194.389], "images_per_second": 5.144, "prompt_tokens": 759, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\n 1. Person in white and orange jacket 2. Person in black and red jacket 3. Person in blue and white jacket", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.4, "ram_available_mb": 112028.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10477.7, "ram_available_mb": 112028.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.3, 38.66], "power_watts_avg": 35.48, "power_watts_peak": 38.66, "energy_joules_est": 6.92, "sample_count": 2, "duration_seconds": 0.195}, "timestamp": "2026-01-25T20:04:37.755372"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 99.123, "latencies_ms": [99.123], "images_per_second": 10.089, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\nTwo people in front of a forest.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.7, "ram_available_mb": 112028.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10474.2, "ram_available_mb": 112032.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [38.66], "power_watts_avg": 38.66, "power_watts_peak": 38.66, "energy_joules_est": 3.84, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T20:04:37.860703"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 205.991, "latencies_ms": [205.991], "images_per_second": 4.855, "prompt_tokens": 757, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nThree people are standing in a snowy field, each holding ski poles. They appear to be posing for a photo with their skis on.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.2, "ram_available_mb": 112032.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10475.7, "ram_available_mb": 112030.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [38.66, 38.66, 38.66], "power_watts_avg": 38.66, "power_watts_peak": 38.66, "energy_joules_est": 7.98, "sample_count": 3, "duration_seconds": 0.206}, "timestamp": "2026-01-25T20:04:38.167538"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 341.331, "latencies_ms": [341.331], "images_per_second": 2.93, "prompt_tokens": 756, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\n The image shows three people standing in the snow with their skis on. One person is wearing a red jacket and holding ski poles, while another person is wearing an orange jacket and also has ski poles. A third person is wearing a black jacket and has no ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.7, "ram_available_mb": 112030.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10476.4, "ram_available_mb": 112029.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [38.48, 38.48, 38.48, 38.48], "power_watts_avg": 38.48, "power_watts_peak": 38.48, "energy_joules_est": 13.15, "sample_count": 4, "duration_seconds": 0.342}, "timestamp": "2026-01-25T20:04:38.573407"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 299.218, "latencies_ms": [299.218], "images_per_second": 3.342, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA white and blue bus with a black roof is parked on the side of a street, displaying \"51\" in red letters on its front.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10476.4, "ram_available_mb": 112029.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10476.4, "ram_available_mb": 112029.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [32.92, 32.92, 32.92], "power_watts_avg": 32.92, "power_watts_peak": 32.92, "energy_joules_est": 9.87, "sample_count": 3, "duration_seconds": 0.3}, "timestamp": "2026-01-25T20:04:38.883949"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.509, "latencies_ms": [82.509], "images_per_second": 12.12, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bus: 51", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.4, "ram_available_mb": 112029.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10476.4, "ram_available_mb": 112029.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [32.92], "power_watts_avg": 32.92, "power_watts_peak": 32.92, "energy_joules_est": 2.72, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:04:38.988088"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 124.238, "latencies_ms": [124.238], "images_per_second": 8.049, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. Bus number 51 is displayed on the front of the bus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.4, "ram_available_mb": 112029.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10469.5, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [32.92, 35.39], "power_watts_avg": 34.15, "power_watts_peak": 35.39, "energy_joules_est": 4.25, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T20:04:39.194036"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 342.54, "latencies_ms": [342.54], "images_per_second": 2.919, "prompt_tokens": 757, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\nA white bus with a blue stripe on its front, displaying \"51\" in red letters. The bus has a license plate that reads \"L82J\". It is parked next to a building, possibly at a bus stop or designated parking area for public transit vehicles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.5, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [35.39, 35.39, 35.39, 35.39], "power_watts_avg": 35.39, "power_watts_peak": 35.39, "energy_joules_est": 12.14, "sample_count": 4, "duration_seconds": 0.343}, "timestamp": "2026-01-25T20:04:39.601028"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 115.221, "latencies_ms": [115.221], "images_per_second": 8.679, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n The front of the bus is white with blue and green accents.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [36.56, 36.56], "power_watts_avg": 36.56, "power_watts_peak": 36.56, "energy_joules_est": 4.22, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T20:04:39.806896"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 181.808, "latencies_ms": [181.808], "images_per_second": 5.5, "prompt_tokens": 744, "response_tokens_est": 8, "n_tiles": 1, "output_text": "urn of water on a stove top", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10465.2, "ram_available_mb": 112041.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [36.56, 36.56], "power_watts_avg": 36.56, "power_watts_peak": 36.56, "energy_joules_est": 6.67, "sample_count": 2, "duration_seconds": 0.183}, "timestamp": "2026-01-25T20:04:40.017917"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 64.148, "latencies_ms": [64.148], "images_per_second": 15.589, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Wall", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10465.2, "ram_available_mb": 112041.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10465.2, "ram_available_mb": 112041.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [36.56], "power_watts_avg": 36.56, "power_watts_peak": 36.56, "energy_joules_est": 2.35, "sample_count": 1, "duration_seconds": 0.064}, "timestamp": "2026-01-25T20:04:40.123639"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 108.787, "latencies_ms": [108.787], "images_per_second": 9.192, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe man is standing in front of a white wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10459.3, "ram_available_mb": 112047.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [33.75, 33.75], "power_watts_avg": 33.75, "power_watts_peak": 33.75, "energy_joules_est": 3.69, "sample_count": 2, "duration_seconds": 0.109}, "timestamp": "2026-01-25T20:04:40.329701"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 274.035, "latencies_ms": [274.035], "images_per_second": 3.649, "prompt_tokens": 757, "response_tokens_est": 46, "n_tiles": 1, "output_text": "\nA young man in a blue blazer, tie, and skirt stands against a white wall. He holds a black bag in his left hand and wears tights with black tights peeking out from under his skirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.3, "ram_available_mb": 112047.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10467.1, "ram_available_mb": 112039.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [33.75, 33.75, 33.75], "power_watts_avg": 33.75, "power_watts_peak": 33.75, "energy_joules_est": 9.27, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T20:04:40.636924"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 112.666, "latencies_ms": [112.666], "images_per_second": 8.876, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The man is wearing a blue blazer and tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.1, "ram_available_mb": 112039.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 80.0}, "power_stats": {"power_watts_samples": [31.33, 31.33], "power_watts_avg": 31.33, "power_watts_peak": 31.33, "energy_joules_est": 3.54, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T20:04:40.843448"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 277.989, "latencies_ms": [277.989], "images_per_second": 3.597, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA large metal fence runs along a road, with several train cars parked on it and a sign indicating \"No Parking\".", "error": null, "sys_before": {"cpu_percent": 3.4, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.33, 31.33, 31.33], "power_watts_avg": 31.33, "power_watts_peak": 31.33, "energy_joules_est": 8.73, "sample_count": 3, "duration_seconds": 0.279}, "timestamp": "2026-01-25T20:04:41.161222"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 83.888, "latencies_ms": [83.888], "images_per_second": 11.921, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Train car 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.9], "power_watts_avg": 32.9, "power_watts_peak": 32.9, "energy_joules_est": 2.77, "sample_count": 1, "duration_seconds": 0.084}, "timestamp": "2026-01-25T20:04:41.266702"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 119.691, "latencies_ms": [119.691], "images_per_second": 8.355, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n 1. A train station with a lot of trains on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.9, 32.9], "power_watts_avg": 32.9, "power_watts_peak": 32.9, "energy_joules_est": 3.95, "sample_count": 2, "duration_seconds": 0.12}, "timestamp": "2026-01-25T20:04:41.471691"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 226.576, "latencies_ms": [226.576], "images_per_second": 4.414, "prompt_tokens": 757, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA row of train cars sits on a track, with a fence running along the side. The sky above appears hazy, creating an atmospheric backdrop for the scene below.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.9, 31.74, 31.74], "power_watts_avg": 32.12, "power_watts_peak": 32.9, "energy_joules_est": 7.3, "sample_count": 3, "duration_seconds": 0.227}, "timestamp": "2026-01-25T20:04:41.778807"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 240.878, "latencies_ms": [240.878], "images_per_second": 4.151, "prompt_tokens": 756, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\n The image shows a train track with multiple trains on it. The sky is hazy and the ground appears to be wet, giving an impression of foggy or misty conditions.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10467.3, "ram_available_mb": 112039.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.74, 31.74, 31.74], "power_watts_avg": 31.74, "power_watts_peak": 31.74, "energy_joules_est": 7.66, "sample_count": 3, "duration_seconds": 0.241}, "timestamp": "2026-01-25T20:04:42.085445"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 287.717, "latencies_ms": [287.717], "images_per_second": 3.476, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "urns of water and a potted plant sit on the floor in front of a white toilet, with various items scattered around the room.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.3, "ram_available_mb": 112039.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10467.3, "ram_available_mb": 112039.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.58, 31.58, 31.58], "power_watts_avg": 31.58, "power_watts_peak": 31.58, "energy_joules_est": 9.09, "sample_count": 3, "duration_seconds": 0.288}, "timestamp": "2026-01-25T20:04:42.396391"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.394, "latencies_ms": [79.394], "images_per_second": 12.595, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Toilet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.3, "ram_available_mb": 112039.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10466.7, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.58], "power_watts_avg": 31.58, "power_watts_peak": 31.58, "energy_joules_est": 2.51, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T20:04:42.502020"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 127.625, "latencies_ms": [127.625], "images_per_second": 7.835, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Toilet - 2. Plant - 3. Shoes", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.7, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10466.7, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.58, 35.59], "power_watts_avg": 33.59, "power_watts_peak": 35.59, "energy_joules_est": 4.3, "sample_count": 2, "duration_seconds": 0.128}, "timestamp": "2026-01-25T20:04:42.707500"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 152.77, "latencies_ms": [152.77], "images_per_second": 6.546, "prompt_tokens": 757, "response_tokens_est": 20, "n_tiles": 1, "output_text": "urns of water on floor, a toilet, and shoes are scattered around in a bathroom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.7, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10466.7, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.59, 35.59], "power_watts_avg": 35.59, "power_watts_peak": 35.59, "energy_joules_est": 5.44, "sample_count": 2, "duration_seconds": 0.153}, "timestamp": "2026-01-25T20:04:42.912998"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 93.838, "latencies_ms": [93.838], "images_per_second": 10.657, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "urns of water and a plant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10462.3, "ram_available_mb": 112044.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.59], "power_watts_avg": 35.59, "power_watts_peak": 35.59, "energy_joules_est": 3.35, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T20:04:43.018867"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 245.13, "latencies_ms": [245.13], "images_per_second": 4.079, "prompt_tokens": 744, "response_tokens_est": 20, "n_tiles": 1, "output_text": "urn of water with a polar bear in it, holding a green ball and looking at it.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10462.3, "ram_available_mb": 112044.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10457.1, "ram_available_mb": 112049.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [35.59, 31.84, 31.84], "power_watts_avg": 33.09, "power_watts_peak": 35.59, "energy_joules_est": 8.12, "sample_count": 3, "duration_seconds": 0.245}, "timestamp": "2026-01-25T20:04:43.333399"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 80.871, "latencies_ms": [80.871], "images_per_second": 12.365, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Polar bear in water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.1, "ram_available_mb": 112049.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [31.84], "power_watts_avg": 31.84, "power_watts_peak": 31.84, "energy_joules_est": 2.58, "sample_count": 1, "duration_seconds": 0.081}, "timestamp": "2026-01-25T20:04:43.439281"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 120.551, "latencies_ms": [120.551], "images_per_second": 8.295, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nA polar bear is playing with a green ball in water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.7, "ram_available_mb": 112045.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [31.84, 31.84], "power_watts_avg": 31.84, "power_watts_peak": 31.84, "energy_joules_est": 3.84, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T20:04:43.646127"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 142.09, "latencies_ms": [142.09], "images_per_second": 7.038, "prompt_tokens": 757, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA polar bear cub is playing with a green ball in a pool of water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [32.43, 32.43], "power_watts_avg": 32.43, "power_watts_peak": 32.43, "energy_joules_est": 4.64, "sample_count": 2, "duration_seconds": 0.143}, "timestamp": "2026-01-25T20:04:43.852319"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 216.036, "latencies_ms": [216.036], "images_per_second": 4.629, "prompt_tokens": 756, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\n A polar bear is playing with a green ball in the water. The bear has its paw raised and appears to be enjoying itself as it interacts with the ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [32.43, 32.43, 32.43], "power_watts_avg": 32.43, "power_watts_peak": 32.43, "energy_joules_est": 7.02, "sample_count": 3, "duration_seconds": 0.216}, "timestamp": "2026-01-25T20:04:44.158875"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 330.586, "latencies_ms": [330.586], "images_per_second": 3.025, "prompt_tokens": 744, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nThe image is a collage of two photos, with the top photo showing a person's feet wearing blue slippers and the bottom photo featuring a hand holding a cell phone.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10474.2, "ram_available_mb": 112032.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [33.68, 33.68, 33.68, 33.68], "power_watts_avg": 33.68, "power_watts_peak": 33.68, "energy_joules_est": 11.15, "sample_count": 4, "duration_seconds": 0.331}, "timestamp": "2026-01-25T20:04:44.570595"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 81.586, "latencies_ms": [81.586], "images_per_second": 12.257, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Chair - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.2, "ram_available_mb": 112032.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10475.5, "ram_available_mb": 112030.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [33.73], "power_watts_avg": 33.73, "power_watts_peak": 33.73, "energy_joules_est": 2.77, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T20:04:44.674754"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 140.676, "latencies_ms": [140.676], "images_per_second": 7.109, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n 1. A person's feet in blue slippers and black socks on a wooden floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.5, "ram_available_mb": 112030.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10475.5, "ram_available_mb": 112030.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [33.73, 33.73], "power_watts_avg": 33.73, "power_watts_peak": 33.73, "energy_joules_est": 4.76, "sample_count": 2, "duration_seconds": 0.141}, "timestamp": "2026-01-25T20:04:44.879918"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 380.408, "latencies_ms": [380.408], "images_per_second": 2.629, "prompt_tokens": 757, "response_tokens_est": 65, "n_tiles": 1, "output_text": "\nThe image shows a person's feet wearing blue slippers, with their legs crossed in front of them. The person is holding a cell phone to their ear while standing on a wooden floor. In another part of the image, there are two chairs - one with a book resting on it and the other empty.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.5, "ram_available_mb": 112030.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 75.0}, "power_stats": {"power_watts_samples": [33.73, 33.73, 33.81, 33.81], "power_watts_avg": 33.77, "power_watts_peak": 33.81, "energy_joules_est": 12.87, "sample_count": 4, "duration_seconds": 0.381}, "timestamp": "2026-01-25T20:04:45.286769"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 146.103, "latencies_ms": [146.103], "images_per_second": 6.844, "prompt_tokens": 756, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n The image features a person's feet wearing blue slippers and holding a cell phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 75.0}, "power_stats": {"power_watts_samples": [33.81, 33.81], "power_watts_avg": 33.81, "power_watts_peak": 33.81, "energy_joules_est": 4.95, "sample_count": 2, "duration_seconds": 0.147}, "timestamp": "2026-01-25T20:04:45.491787"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 273.892, "latencies_ms": [273.892], "images_per_second": 3.651, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA yellow train is traveling down a snowy track, surrounded by trees and bushes on either side of the tracks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 75.0}, "power_stats": {"power_watts_samples": [33.81, 30.65, 30.65], "power_watts_avg": 31.71, "power_watts_peak": 33.81, "energy_joules_est": 8.7, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T20:04:45.802987"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 77.877, "latencies_ms": [77.877], "images_per_second": 12.841, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Train", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 75.0}, "power_stats": {"power_watts_samples": [30.65], "power_watts_avg": 30.65, "power_watts_peak": 30.65, "energy_joules_est": 2.4, "sample_count": 1, "duration_seconds": 0.078}, "timestamp": "2026-01-25T20:04:45.908380"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 142.96, "latencies_ms": [142.96], "images_per_second": 6.995, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n 1. Train on tracks 2. Trees in background 3. Snowy sky above", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10472.4, "ram_available_mb": 112033.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [30.65, 30.65], "power_watts_avg": 30.65, "power_watts_peak": 30.65, "energy_joules_est": 4.39, "sample_count": 2, "duration_seconds": 0.143}, "timestamp": "2026-01-25T20:04:46.114178"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 239.682, "latencies_ms": [239.682], "images_per_second": 4.172, "prompt_tokens": 757, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nA yellow train travels down a snowy track, surrounded by trees. The sky above is overcast, creating a serene atmosphere as the train moves through the wintry landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.4, "ram_available_mb": 112033.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [36.14, 36.14, 36.14], "power_watts_avg": 36.14, "power_watts_peak": 36.14, "energy_joules_est": 8.67, "sample_count": 3, "duration_seconds": 0.24}, "timestamp": "2026-01-25T20:04:46.420514"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 358.653, "latencies_ms": [358.653], "images_per_second": 2.788, "prompt_tokens": 756, "response_tokens_est": 62, "n_tiles": 1, "output_text": "\n A yellow train is traveling down the tracks in a snowy landscape. The sky above appears to be overcast and gray, which creates a moody atmosphere. The snow-covered ground adds to the wintry scene, while the trees lining the tracks provide some contrast against the monochrome backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10479.3, "ram_available_mb": 112027.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [36.14, 36.14, 35.42, 35.42], "power_watts_avg": 35.78, "power_watts_peak": 36.14, "energy_joules_est": 12.84, "sample_count": 4, "duration_seconds": 0.359}, "timestamp": "2026-01-25T20:04:46.826676"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 284.611, "latencies_ms": [284.611], "images_per_second": 3.514, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA group of people are walking through a snowy street, with snow covering the ground and piles of snow on the side of the road.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 10479.3, "ram_available_mb": 112027.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10477.4, "ram_available_mb": 112028.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [35.42, 35.42, 35.42], "power_watts_avg": 35.42, "power_watts_peak": 35.42, "energy_joules_est": 10.11, "sample_count": 3, "duration_seconds": 0.286}, "timestamp": "2026-01-25T20:04:47.137966"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 101.942, "latencies_ms": [101.942], "images_per_second": 9.81, "prompt_tokens": 759, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Person walking in snow - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.4, "ram_available_mb": 112028.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [37.42, 37.42], "power_watts_avg": 37.42, "power_watts_peak": 37.42, "energy_joules_est": 3.84, "sample_count": 2, "duration_seconds": 0.103}, "timestamp": "2026-01-25T20:04:47.343770"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 128.322, "latencies_ms": [128.322], "images_per_second": 7.793, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. Person walking in front of a snow pile on the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10479.9, "ram_available_mb": 112026.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [37.42, 37.42], "power_watts_avg": 37.42, "power_watts_peak": 37.42, "energy_joules_est": 4.82, "sample_count": 2, "duration_seconds": 0.129}, "timestamp": "2026-01-25T20:04:47.549726"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 166.258, "latencies_ms": [166.258], "images_per_second": 6.015, "prompt_tokens": 757, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA group of people are walking through a snowy street, with snow piled up on the side of the road.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10479.9, "ram_available_mb": 112026.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [37.42, 34.33], "power_watts_avg": 35.88, "power_watts_peak": 37.42, "energy_joules_est": 5.98, "sample_count": 2, "duration_seconds": 0.167}, "timestamp": "2026-01-25T20:04:47.755948"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 459.13, "latencies_ms": [459.13], "images_per_second": 2.178, "prompt_tokens": 756, "response_tokens_est": 83, "n_tiles": 1, "output_text": "\n The image shows a group of people walking through snow in the city. There are at least six people visible in the scene, with some wearing winter clothing and carrying backpacks or handbags. The snow is piled up on the ground, creating a snowy landscape that contrasts with the urban setting. The sky appears to be overcast, which might affect visibility for the pedestrians as they walk through the city streets.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10478.5, "ram_available_mb": 112027.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.33, 34.33, 34.33, 34.33, 31.48], "power_watts_avg": 33.76, "power_watts_peak": 34.33, "energy_joules_est": 15.52, "sample_count": 5, "duration_seconds": 0.46}, "timestamp": "2026-01-25T20:04:48.261527"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 277.144, "latencies_ms": [277.144], "images_per_second": 3.608, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA \"No Parking\" sign is attached to a pole, with a yellow and black monster-like figure on top of it.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10478.5, "ram_available_mb": 112027.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.48, 31.48, 31.48], "power_watts_avg": 31.48, "power_watts_peak": 31.48, "energy_joules_est": 8.76, "sample_count": 3, "duration_seconds": 0.278}, "timestamp": "2026-01-25T20:04:48.571488"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 83.656, "latencies_ms": [83.656], "images_per_second": 11.954, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. No parking sign", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10470.4, "ram_available_mb": 112035.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [39.6], "power_watts_avg": 39.6, "power_watts_peak": 39.6, "energy_joules_est": 3.34, "sample_count": 1, "duration_seconds": 0.084}, "timestamp": "2026-01-25T20:04:48.676024"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 139.794, "latencies_ms": [139.794], "images_per_second": 7.153, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA sign with a no parking zone and a yellow monster on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.4, "ram_available_mb": 112035.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10467.9, "ram_available_mb": 112038.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [39.6, 39.6], "power_watts_avg": 39.6, "power_watts_peak": 39.6, "energy_joules_est": 5.54, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T20:04:48.881055"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 205.154, "latencies_ms": [205.154], "images_per_second": 4.874, "prompt_tokens": 757, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA \"No Parking\" sign with a \"Tow Zone\" sign attached to it, along with a yellow monster face on one of them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.9, "ram_available_mb": 112038.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_watts_samples": [39.6, 39.6, 36.39], "power_watts_avg": 38.53, "power_watts_peak": 39.6, "energy_joules_est": 7.93, "sample_count": 3, "duration_seconds": 0.206}, "timestamp": "2026-01-25T20:04:49.187623"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 132.642, "latencies_ms": [132.642], "images_per_second": 7.539, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n The sign is red and white with a yellow cartoon face on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10463.6, "ram_available_mb": 112042.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_watts_samples": [36.39, 36.39], "power_watts_avg": 36.39, "power_watts_peak": 36.39, "energy_joules_est": 4.85, "sample_count": 2, "duration_seconds": 0.133}, "timestamp": "2026-01-25T20:04:49.393020"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 230.684, "latencies_ms": [230.684], "images_per_second": 4.335, "prompt_tokens": 744, "response_tokens_est": 17, "n_tiles": 1, "output_text": "urn of bears sits on a red table, with a microphone and headphones nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.5, "ram_available_mb": 112042.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_watts_samples": [36.39, 36.39, 31.16], "power_watts_avg": 34.65, "power_watts_peak": 36.39, "energy_joules_est": 8.0, "sample_count": 3, "duration_seconds": 0.231}, "timestamp": "2026-01-25T20:04:49.701483"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 72.91, "latencies_ms": [72.91], "images_per_second": 13.716, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Bear", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10472.6, "ram_available_mb": 112033.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_watts_samples": [31.16], "power_watts_avg": 31.16, "power_watts_peak": 31.16, "energy_joules_est": 2.28, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T20:04:49.806615"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 162.363, "latencies_ms": [162.363], "images_per_second": 6.159, "prompt_tokens": 763, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n 1. A teddy bear with glasses on its ears and a microphone in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.6, "ram_available_mb": 112033.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 73.0}, "power_stats": {"power_watts_samples": [31.16, 31.16], "power_watts_avg": 31.16, "power_watts_peak": 31.16, "energy_joules_est": 5.07, "sample_count": 2, "duration_seconds": 0.163}, "timestamp": "2026-01-25T20:04:50.012026"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 193.208, "latencies_ms": [193.208], "images_per_second": 5.176, "prompt_tokens": 757, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\n A teddy bear wearing glasses sits on a red table, with various electronic devices including an iPod, microphone, and keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [31.16, 29.64], "power_watts_avg": 30.4, "power_watts_peak": 31.16, "energy_joules_est": 5.88, "sample_count": 2, "duration_seconds": 0.194}, "timestamp": "2026-01-25T20:04:50.216870"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 123.496, "latencies_ms": [123.496], "images_per_second": 8.097, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n A teddy bear wearing glasses and holding a microphone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [29.64, 29.64], "power_watts_avg": 29.64, "power_watts_peak": 29.64, "energy_joules_est": 3.67, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T20:04:50.422584"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 277.545, "latencies_ms": [277.545], "images_per_second": 3.603, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "iced skis and ski poles are in use by a person on a snowy mountain, with a sign for a skiing event nearby.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10474.0, "ram_available_mb": 112032.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [29.64, 29.64, 32.01], "power_watts_avg": 30.43, "power_watts_peak": 32.01, "energy_joules_est": 8.46, "sample_count": 3, "duration_seconds": 0.278}, "timestamp": "2026-01-25T20:04:50.732866"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 185.601, "latencies_ms": [185.601], "images_per_second": 5.388, "prompt_tokens": 759, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\n 1. Skier in orange pants and white shirt falling over backwards, holding ski poles upside down, snow flying around them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10474.7, "ram_available_mb": 112031.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [32.01, 32.01], "power_watts_avg": 32.01, "power_watts_peak": 32.01, "energy_joules_est": 5.96, "sample_count": 2, "duration_seconds": 0.186}, "timestamp": "2026-01-25T20:04:50.938566"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 132.793, "latencies_ms": [132.793], "images_per_second": 7.531, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nThe skier is in midair and has his skis crossed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.7, "ram_available_mb": 112031.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10474.7, "ram_available_mb": 112031.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [32.01, 32.01], "power_watts_avg": 32.01, "power_watts_peak": 32.01, "energy_joules_est": 4.26, "sample_count": 2, "duration_seconds": 0.133}, "timestamp": "2026-01-25T20:04:51.144125"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 255.227, "latencies_ms": [255.227], "images_per_second": 3.918, "prompt_tokens": 757, "response_tokens_est": 42, "n_tiles": 1, "output_text": "\nA skier in an orange jacket is performing a trick on their skis, with snow flying up behind them. The skier is surrounded by trees and mountains, creating a picturesque winter landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.7, "ram_available_mb": 112031.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10475.2, "ram_available_mb": 112031.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [36.44, 36.44, 36.44], "power_watts_avg": 36.44, "power_watts_peak": 36.44, "energy_joules_est": 9.32, "sample_count": 3, "duration_seconds": 0.256}, "timestamp": "2026-01-25T20:04:51.450131"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 593.665, "latencies_ms": [593.665], "images_per_second": 1.684, "prompt_tokens": 756, "response_tokens_est": 107, "n_tiles": 1, "output_text": "\nThe image shows a person skiing down a snow-covered slope. The skier is wearing an orange jacket and white pants while holding ski poles in their hands. They are captured mid-air as they perform a jump or trick on the slopes. The background features a mountain with trees and rocks, providing a picturesque setting for this winter sport scene.\n\nThe image also includes text that reads \"24 febraio 2010 Tour de Sas\", which suggests that the skiing event is taking place in February of 2010.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.2, "ram_available_mb": 112031.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10477.2, "ram_available_mb": 112029.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 32.0}, "power_stats": {"power_watts_samples": [36.44, 36.44, 33.23, 33.23, 33.23, 33.23], "power_watts_avg": 34.3, "power_watts_peak": 36.44, "energy_joules_est": 20.38, "sample_count": 6, "duration_seconds": 0.594}, "timestamp": "2026-01-25T20:04:52.057010"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 273.093, "latencies_ms": [273.093], "images_per_second": 3.662, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nTwo surfers, one on a yellow board and another on a white board, are riding waves in the ocean.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10477.2, "ram_available_mb": 112029.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10477.2, "ram_available_mb": 112029.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.23, 36.78, 36.78], "power_watts_avg": 35.6, "power_watts_peak": 36.78, "energy_joules_est": 9.73, "sample_count": 3, "duration_seconds": 0.273}, "timestamp": "2026-01-25T20:04:52.368990"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 97.138, "latencies_ms": [97.138], "images_per_second": 10.295, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Person surfing a wave in the ocean", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.2, "ram_available_mb": 112029.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10479.0, "ram_available_mb": 112027.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.78], "power_watts_avg": 36.78, "power_watts_peak": 36.78, "energy_joules_est": 3.59, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T20:04:52.473690"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 107.822, "latencies_ms": [107.822], "images_per_second": 9.275, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Surfer on left side of image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.0, "ram_available_mb": 112027.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10482.7, "ram_available_mb": 112023.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.78, 40.89], "power_watts_avg": 38.84, "power_watts_peak": 40.89, "energy_joules_est": 4.21, "sample_count": 2, "duration_seconds": 0.108}, "timestamp": "2026-01-25T20:04:52.679158"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 172.228, "latencies_ms": [172.228], "images_per_second": 5.806, "prompt_tokens": 757, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nTwo surfers, one on a yellow board and another on a white board, are riding waves in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.7, "ram_available_mb": 112023.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10483.0, "ram_available_mb": 112023.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [40.89, 40.89], "power_watts_avg": 40.89, "power_watts_peak": 40.89, "energy_joules_est": 7.05, "sample_count": 2, "duration_seconds": 0.172}, "timestamp": "2026-01-25T20:04:52.883956"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 373.472, "latencies_ms": [373.472], "images_per_second": 2.678, "prompt_tokens": 756, "response_tokens_est": 64, "n_tiles": 1, "output_text": "\n The image shows two surfers riding waves in the ocean. One surfer is on a green surfboard and appears to be crouched down while surfing a small wave. The other surfer is standing upright on their white surfboard and seems to be waiting for a larger wave or taking a break from surfing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.9, "ram_available_mb": 112023.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10474.9, "ram_available_mb": 112031.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [40.89, 40.89, 34.97, 34.97], "power_watts_avg": 37.93, "power_watts_peak": 40.89, "energy_joules_est": 14.18, "sample_count": 4, "duration_seconds": 0.374}, "timestamp": "2026-01-25T20:04:53.290927"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 250.207, "latencies_ms": [250.207], "images_per_second": 3.997, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "iced tea and a jar of marinara sauce are on a table with a pizza, ready to be served.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.9, "ram_available_mb": 112031.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10477.3, "ram_available_mb": 112029.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [34.97, 34.97, 34.97], "power_watts_avg": 34.97, "power_watts_peak": 34.97, "energy_joules_est": 8.76, "sample_count": 3, "duration_seconds": 0.25}, "timestamp": "2026-01-25T20:04:53.599928"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 70.012, "latencies_ms": [70.012], "images_per_second": 14.283, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Pizza pan", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.3, "ram_available_mb": 112029.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10477.3, "ram_available_mb": 112029.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [37.28], "power_watts_avg": 37.28, "power_watts_peak": 37.28, "energy_joules_est": 2.63, "sample_count": 1, "duration_seconds": 0.071}, "timestamp": "2026-01-25T20:04:53.705315"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 108.603, "latencies_ms": [108.603], "images_per_second": 9.208, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nA pizza sits on a silver tray atop a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.3, "ram_available_mb": 112029.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10478.0, "ram_available_mb": 112028.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 71.0}, "power_stats": {"power_watts_samples": [37.28, 37.28], "power_watts_avg": 37.28, "power_watts_peak": 37.28, "energy_joules_est": 4.06, "sample_count": 2, "duration_seconds": 0.109}, "timestamp": "2026-01-25T20:04:53.910840"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 211.894, "latencies_ms": [211.894], "images_per_second": 4.719, "prompt_tokens": 757, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\n A pizza with various toppings sits on a silver tray atop a blue table. Next to the pizza, there is a jar of sauce and a napkin.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.0, "ram_available_mb": 112028.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10483.5, "ram_available_mb": 112022.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [37.28, 37.28, 35.71], "power_watts_avg": 36.76, "power_watts_peak": 37.28, "energy_joules_est": 7.8, "sample_count": 3, "duration_seconds": 0.212}, "timestamp": "2026-01-25T20:04:54.216404"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 96.184, "latencies_ms": [96.184], "images_per_second": 10.397, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urn of sauce on table next to pizza.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.5, "ram_available_mb": 112022.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10491.3, "ram_available_mb": 112015.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [35.71], "power_watts_avg": 35.71, "power_watts_peak": 35.71, "energy_joules_est": 3.45, "sample_count": 1, "duration_seconds": 0.096}, "timestamp": "2026-01-25T20:04:54.319976"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 265.761, "latencies_ms": [265.761], "images_per_second": 3.763, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA clock tower stands on a pole in front of a row of buildings, with snow covering the ground and trees nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10491.3, "ram_available_mb": 112015.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10490.8, "ram_available_mb": 112015.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [35.71, 35.71, 35.71], "power_watts_avg": 35.71, "power_watts_peak": 35.71, "energy_joules_est": 9.5, "sample_count": 3, "duration_seconds": 0.266}, "timestamp": "2026-01-25T20:04:54.628945"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 74.769, "latencies_ms": [74.769], "images_per_second": 13.374, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Clock", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10490.8, "ram_available_mb": 112015.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10491.3, "ram_available_mb": 112015.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [32.73], "power_watts_avg": 32.73, "power_watts_peak": 32.73, "energy_joules_est": 2.46, "sample_count": 1, "duration_seconds": 0.075}, "timestamp": "2026-01-25T20:04:54.734592"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 104.231, "latencies_ms": [104.231], "images_per_second": 9.594, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nA clock on a pole in front of a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10491.3, "ram_available_mb": 112015.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10492.9, "ram_available_mb": 112013.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 56.0}, "power_stats": {"power_watts_samples": [32.73, 32.73], "power_watts_avg": 32.73, "power_watts_peak": 32.73, "energy_joules_est": 3.42, "sample_count": 2, "duration_seconds": 0.104}, "timestamp": "2026-01-25T20:04:54.939728"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 230.964, "latencies_ms": [230.964], "images_per_second": 4.33, "prompt_tokens": 757, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA clock tower stands on a pole in front of a row of buildings, with snow covering the ground. The clock face shows that it's almost 4 o'clock.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10492.9, "ram_available_mb": 112013.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10482.8, "ram_available_mb": 112023.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [32.73, 32.73, 32.32], "power_watts_avg": 32.59, "power_watts_peak": 32.73, "energy_joules_est": 7.54, "sample_count": 3, "duration_seconds": 0.231}, "timestamp": "2026-01-25T20:04:55.246605"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 113.019, "latencies_ms": [113.019], "images_per_second": 8.848, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n A clock on a pole in the middle of a sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.8, "ram_available_mb": 112023.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [32.32, 32.32], "power_watts_avg": 32.32, "power_watts_peak": 32.32, "energy_joules_est": 3.66, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T20:04:55.451859"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 311.767, "latencies_ms": [311.767], "images_per_second": 3.208, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA baseball player in a white and blue uniform swings his bat, attempting to hit an incoming ball during a game on a field with green grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [32.32, 32.32, 28.57, 28.57], "power_watts_avg": 30.45, "power_watts_peak": 32.32, "energy_joules_est": 9.51, "sample_count": 4, "duration_seconds": 0.312}, "timestamp": "2026-01-25T20:04:55.862348"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 73.428, "latencies_ms": [73.428], "images_per_second": 13.619, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Pitcher", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10483.7, "ram_available_mb": 112022.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [28.57], "power_watts_avg": 28.57, "power_watts_peak": 28.57, "energy_joules_est": 2.11, "sample_count": 1, "duration_seconds": 0.074}, "timestamp": "2026-01-25T20:04:55.967606"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 151.64, "latencies_ms": [151.64], "images_per_second": 6.595, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nThe baseball player is swinging his bat at a ball that has been thrown to him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.7, "ram_available_mb": 112022.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10483.9, "ram_available_mb": 112022.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [28.57, 31.93], "power_watts_avg": 30.25, "power_watts_peak": 31.93, "energy_joules_est": 4.6, "sample_count": 2, "duration_seconds": 0.152}, "timestamp": "2026-01-25T20:04:56.173411"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 393.806, "latencies_ms": [393.806], "images_per_second": 2.539, "prompt_tokens": 757, "response_tokens_est": 69, "n_tiles": 1, "output_text": "\nIn the image, a baseball player in a white uniform with blue accents is swinging his bat at an incoming ball. Behind him, another player wearing red is crouched down behind home plate, ready to catch the ball if necessary. The scene takes place on a baseball field, and there are several people present, including players and spectators.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.9, "ram_available_mb": 112022.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10483.9, "ram_available_mb": 112022.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [31.93, 31.93, 31.93, 31.93], "power_watts_avg": 31.93, "power_watts_peak": 31.93, "energy_joules_est": 12.58, "sample_count": 4, "duration_seconds": 0.394}, "timestamp": "2026-01-25T20:04:56.579549"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 284.952, "latencies_ms": [284.952], "images_per_second": 3.509, "prompt_tokens": 756, "response_tokens_est": 47, "n_tiles": 1, "output_text": "\n The image shows a baseball player swinging his bat at an incoming ball. He is wearing a blue and white uniform with the Cubs logo on it. The catcher is in position behind him, ready to catch the ball if needed.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10483.9, "ram_available_mb": 112022.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10486.8, "ram_available_mb": 112019.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [35.64, 35.64, 35.64], "power_watts_avg": 35.64, "power_watts_peak": 35.64, "energy_joules_est": 10.16, "sample_count": 3, "duration_seconds": 0.285}, "timestamp": "2026-01-25T20:04:56.884284"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 243.063, "latencies_ms": [243.063], "images_per_second": 4.114, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "urn with a teddy bear sitting on it, wearing a red and white plaid bow tie.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10486.8, "ram_available_mb": 112019.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10486.5, "ram_available_mb": 112019.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [35.64, 35.64, 40.08], "power_watts_avg": 37.12, "power_watts_peak": 40.08, "energy_joules_est": 9.03, "sample_count": 3, "duration_seconds": 0.243}, "timestamp": "2026-01-25T20:04:57.196554"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 91.307, "latencies_ms": [91.307], "images_per_second": 10.952, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Chair cushion - 1", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10486.5, "ram_available_mb": 112019.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [40.08], "power_watts_avg": 40.08, "power_watts_peak": 40.08, "energy_joules_est": 3.69, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T20:04:57.301447"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 155.082, "latencies_ms": [155.082], "images_per_second": 6.448, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n A brown teddy bear sits on a red cushion in front of a striped curtain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10484.3, "ram_available_mb": 112022.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [40.08, 40.08], "power_watts_avg": 40.08, "power_watts_peak": 40.08, "energy_joules_est": 6.23, "sample_count": 2, "duration_seconds": 0.155}, "timestamp": "2026-01-25T20:04:57.507685"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 116.201, "latencies_ms": [116.201], "images_per_second": 8.606, "prompt_tokens": 757, "response_tokens_est": 13, "n_tiles": 1, "output_text": "urn with a teddy bear sitting on top of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.3, "ram_available_mb": 112022.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10488.0, "ram_available_mb": 112018.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [40.08, 34.5], "power_watts_avg": 37.29, "power_watts_peak": 40.08, "energy_joules_est": 4.35, "sample_count": 2, "duration_seconds": 0.117}, "timestamp": "2026-01-25T20:04:57.713692"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 96.103, "latencies_ms": [96.103], "images_per_second": 10.405, "prompt_tokens": 756, "response_tokens_est": 8, "n_tiles": 1, "output_text": "urn made of wood and metal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.0, "ram_available_mb": 112018.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10488.0, "ram_available_mb": 112018.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [34.5], "power_watts_avg": 34.5, "power_watts_peak": 34.5, "energy_joules_est": 3.32, "sample_count": 1, "duration_seconds": 0.096}, "timestamp": "2026-01-25T20:04:57.818172"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 259.933, "latencies_ms": [259.933], "images_per_second": 3.847, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nTwo people, dressed in red and black jackets, stand on a snowy mountain top with their backpacks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.0, "ram_available_mb": 112018.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10493.3, "ram_available_mb": 112013.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [34.5, 34.5, 34.5], "power_watts_avg": 34.5, "power_watts_peak": 34.5, "energy_joules_est": 8.97, "sample_count": 3, "duration_seconds": 0.26}, "timestamp": "2026-01-25T20:04:58.126101"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.365, "latencies_ms": [82.365], "images_per_second": 12.141, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Sun - 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10493.3, "ram_available_mb": 112013.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10493.1, "ram_available_mb": 112013.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [32.05], "power_watts_avg": 32.05, "power_watts_peak": 32.05, "energy_joules_est": 2.66, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:04:58.231164"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 237.623, "latencies_ms": [237.623], "images_per_second": 4.208, "prompt_tokens": 763, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nThe man in a red jacket is standing on top of a snowy mountain with his snowboard. A woman wearing a red jacket and holding her own snowboard is also present nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10493.1, "ram_available_mb": 112013.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10492.3, "ram_available_mb": 112014.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [32.05, 32.05, 32.05], "power_watts_avg": 32.05, "power_watts_peak": 32.05, "energy_joules_est": 7.62, "sample_count": 3, "duration_seconds": 0.238}, "timestamp": "2026-01-25T20:04:58.537830"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 239.576, "latencies_ms": [239.576], "images_per_second": 4.174, "prompt_tokens": 757, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\nA man in a red jacket and black pants stands on top of a snowy mountain, holding his snowboard. A woman wearing a red jacket also stands nearby with her own snowboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10492.3, "ram_available_mb": 112014.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10492.6, "ram_available_mb": 112013.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [32.05, 33.0, 33.0], "power_watts_avg": 32.69, "power_watts_peak": 33.0, "energy_joules_est": 7.85, "sample_count": 3, "duration_seconds": 0.24}, "timestamp": "2026-01-25T20:04:58.843323"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 121.901, "latencies_ms": [121.901], "images_per_second": 8.203, "prompt_tokens": 756, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n The sun is shining brightly in the sky and creating a lens flare effect.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10492.6, "ram_available_mb": 112013.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10489.3, "ram_available_mb": 112017.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [33.0, 33.0], "power_watts_avg": 33.0, "power_watts_peak": 33.0, "energy_joules_est": 4.03, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:04:59.048627"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 241.988, "latencies_ms": [241.988], "images_per_second": 4.132, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "urn of red apples hanging from a tree branch, with some leaves and bark visible on the branches.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10489.3, "ram_available_mb": 112017.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10488.5, "ram_available_mb": 112017.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [33.0, 30.29, 30.29], "power_watts_avg": 31.19, "power_watts_peak": 33.0, "energy_joules_est": 7.55, "sample_count": 3, "duration_seconds": 0.242}, "timestamp": "2026-01-25T20:04:59.359763"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 71.871, "latencies_ms": [71.871], "images_per_second": 13.914, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Apple: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.5, "ram_available_mb": 112017.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10488.1, "ram_available_mb": 112018.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [30.29], "power_watts_avg": 30.29, "power_watts_peak": 30.29, "energy_joules_est": 2.19, "sample_count": 1, "duration_seconds": 0.072}, "timestamp": "2026-01-25T20:04:59.464120"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 151.991, "latencies_ms": [151.991], "images_per_second": 6.579, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n 1. A tree with a hole in it and red apples hanging from its branches.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.1, "ram_available_mb": 112018.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10489.3, "ram_available_mb": 112017.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [30.29, 30.29], "power_watts_avg": 30.29, "power_watts_peak": 30.29, "energy_joules_est": 4.63, "sample_count": 2, "duration_seconds": 0.153}, "timestamp": "2026-01-25T20:04:59.669615"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 131.15, "latencies_ms": [131.15], "images_per_second": 7.625, "prompt_tokens": 757, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A tree with a hole in its trunk, surrounded by red apples.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10489.3, "ram_available_mb": 112017.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10491.8, "ram_available_mb": 112014.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 54.0}, "power_stats": {"power_watts_samples": [34.05, 34.05], "power_watts_avg": 34.05, "power_watts_peak": 34.05, "energy_joules_est": 4.48, "sample_count": 2, "duration_seconds": 0.132}, "timestamp": "2026-01-25T20:04:59.875485"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 136.23, "latencies_ms": [136.23], "images_per_second": 7.341, "prompt_tokens": 756, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n A tree with a hole in it and red apples hanging from the branches.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10491.8, "ram_available_mb": 112014.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10489.5, "ram_available_mb": 112016.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 76.0}, "power_stats": {"power_watts_samples": [34.05, 34.05], "power_watts_avg": 34.05, "power_watts_peak": 34.05, "energy_joules_est": 4.65, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T20:05:00.080605"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 314.145, "latencies_ms": [314.145], "images_per_second": 3.183, "prompt_tokens": 744, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nTwo men in white shirts are working together to prepare food in a large commercial kitchen, with one man stirring something on the stove and another holding a pot.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10489.5, "ram_available_mb": 112016.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10488.5, "ram_available_mb": 112017.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 76.0}, "power_stats": {"power_watts_samples": [31.98, 31.98, 31.98, 31.98], "power_watts_avg": 31.98, "power_watts_peak": 31.98, "energy_joules_est": 10.05, "sample_count": 4, "duration_seconds": 0.314}, "timestamp": "2026-01-25T20:05:00.490407"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 255.232, "latencies_ms": [255.232], "images_per_second": 3.918, "prompt_tokens": 759, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\n 1. Oven door  2. Pan  3. Pot holder  4. Apron  5. Tray  6. Bucket  7. Kettle  8. Bowl", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.5, "ram_available_mb": 112017.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10488.5, "ram_available_mb": 112017.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 76.0}, "power_stats": {"power_watts_samples": [31.98, 31.76, 31.76], "power_watts_avg": 31.83, "power_watts_peak": 31.98, "energy_joules_est": 8.13, "sample_count": 3, "duration_seconds": 0.255}, "timestamp": "2026-01-25T20:05:00.795837"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 165.901, "latencies_ms": [165.901], "images_per_second": 6.028, "prompt_tokens": 763, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n 1. A man in a white shirt and blue cap is cooking food on a stove top.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.5, "ram_available_mb": 112017.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10482.6, "ram_available_mb": 112023.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 76.0}, "power_stats": {"power_watts_samples": [31.76, 31.76], "power_watts_avg": 31.76, "power_watts_peak": 31.76, "energy_joules_est": 5.28, "sample_count": 2, "duration_seconds": 0.166}, "timestamp": "2026-01-25T20:05:01.002089"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 219.097, "latencies_ms": [219.097], "images_per_second": 4.564, "prompt_tokens": 757, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nTwo men are working in a kitchen, one of them stirring something on the stove. The other man is standing next to him, possibly preparing food as well.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.6, "ram_available_mb": 112023.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10481.6, "ram_available_mb": 112024.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [31.76, 34.33, 34.33], "power_watts_avg": 33.48, "power_watts_peak": 34.33, "energy_joules_est": 7.36, "sample_count": 3, "duration_seconds": 0.22}, "timestamp": "2026-01-25T20:05:01.309105"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 90.21, "latencies_ms": [90.21], "images_per_second": 11.085, "prompt_tokens": 756, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urns on the counter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.6, "ram_available_mb": 112024.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10481.3, "ram_available_mb": 112025.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [34.33], "power_watts_avg": 34.33, "power_watts_peak": 34.33, "energy_joules_est": 3.11, "sample_count": 1, "duration_seconds": 0.091}, "timestamp": "2026-01-25T20:05:01.413631"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 334.447, "latencies_ms": [334.447], "images_per_second": 2.99, "prompt_tokens": 744, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nA group of motorcyclists are gathered on a road, with some standing and others sitting on their bikes. The motorcycles vary in size and color, creating an interesting visual display.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.3, "ram_available_mb": 112025.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10484.2, "ram_available_mb": 112022.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [34.33, 34.33, 34.88, 34.88], "power_watts_avg": 34.61, "power_watts_peak": 34.88, "energy_joules_est": 11.58, "sample_count": 4, "duration_seconds": 0.335}, "timestamp": "2026-01-25T20:05:01.818986"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 100.729, "latencies_ms": [100.729], "images_per_second": 9.928, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Motorcycle: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.2, "ram_available_mb": 112022.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10480.0, "ram_available_mb": 112026.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [34.88, 34.88], "power_watts_avg": 34.88, "power_watts_peak": 34.88, "energy_joules_est": 3.52, "sample_count": 2, "duration_seconds": 0.101}, "timestamp": "2026-01-25T20:05:02.023791"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 122.199, "latencies_ms": [122.199], "images_per_second": 8.183, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Motorcycle in front of motorcycle on right side.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10480.0, "ram_available_mb": 112026.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10479.6, "ram_available_mb": 112026.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 12.0}, "power_stats": {"power_watts_samples": [34.88, 32.46], "power_watts_avg": 33.67, "power_watts_peak": 34.88, "energy_joules_est": 4.12, "sample_count": 2, "duration_seconds": 0.123}, "timestamp": "2026-01-25T20:05:02.228658"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 332.649, "latencies_ms": [332.649], "images_per_second": 3.006, "prompt_tokens": 757, "response_tokens_est": 55, "n_tiles": 1, "output_text": "\nA group of motorcyclists are gathered on a road, with some standing around their bikes. The motorcycles include various colors such as red, black, blue, and white. A few people can be seen in the background, possibly admiring or discussing the motorcycles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.6, "ram_available_mb": 112026.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10479.2, "ram_available_mb": 112027.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 12.0}, "power_stats": {"power_watts_samples": [32.46, 32.46, 32.46, 32.46], "power_watts_avg": 32.46, "power_watts_peak": 32.46, "energy_joules_est": 10.81, "sample_count": 4, "duration_seconds": 0.333}, "timestamp": "2026-01-25T20:05:02.634768"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 441.52, "latencies_ms": [441.52], "images_per_second": 2.265, "prompt_tokens": 756, "response_tokens_est": 78, "n_tiles": 1, "output_text": "\n The image shows a group of people standing on the side of a road with their motorcycles. There are at least ten motorcycles in various colors and sizes, including black, red, blue, green, and white. Some riders have helmets on while others do not. The sky is cloudy, suggesting an overcast day or possibly rain. The scene appears to be outdoors during daytime hours.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.2, "ram_available_mb": 112027.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10478.2, "ram_available_mb": 112028.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.52, 31.52, 31.52, 31.52, 31.52], "power_watts_avg": 31.52, "power_watts_peak": 31.52, "energy_joules_est": 13.94, "sample_count": 5, "duration_seconds": 0.442}, "timestamp": "2026-01-25T20:05:03.141268"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 151.378, "latencies_ms": [151.378], "images_per_second": 6.606, "prompt_tokens": 744, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.2, "ram_available_mb": 112028.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10478.3, "ram_available_mb": 112028.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.05, 36.05], "power_watts_avg": 36.05, "power_watts_peak": 36.05, "energy_joules_est": 5.48, "sample_count": 2, "duration_seconds": 0.152}, "timestamp": "2026-01-25T20:05:03.349527"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.308, "latencies_ms": [76.308], "images_per_second": 13.105, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Plane", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.3, "ram_available_mb": 112028.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10475.4, "ram_available_mb": 112030.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.05], "power_watts_avg": 36.05, "power_watts_peak": 36.05, "energy_joules_est": 2.76, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T20:05:03.454732"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 118.649, "latencies_ms": [118.649], "images_per_second": 8.428, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. The plane is flying in front of a cloud.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.4, "ram_available_mb": 112030.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.05, 36.05], "power_watts_avg": 36.05, "power_watts_peak": 36.05, "energy_joules_est": 4.28, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T20:05:03.661959"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 60.26, "latencies_ms": [60.26], "images_per_second": 16.595, "prompt_tokens": 757, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.52], "power_watts_avg": 34.52, "power_watts_peak": 34.52, "energy_joules_est": 2.1, "sample_count": 1, "duration_seconds": 0.061}, "timestamp": "2026-01-25T20:05:03.767646"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 80.854, "latencies_ms": [80.854], "images_per_second": 12.368, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The airplane is black and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.1, "ram_available_mb": 112031.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10476.4, "ram_available_mb": 112029.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.52], "power_watts_avg": 34.52, "power_watts_peak": 34.52, "energy_joules_est": 2.81, "sample_count": 1, "duration_seconds": 0.081}, "timestamp": "2026-01-25T20:05:03.873239"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 301.013, "latencies_ms": [301.013], "images_per_second": 3.322, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA group of four sheep are standing on a grassy hill, with two looking directly at the camera and two others facing away from it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.4, "ram_available_mb": 112029.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10478.8, "ram_available_mb": 112027.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [34.52, 34.52, 31.85], "power_watts_avg": 33.63, "power_watts_peak": 34.52, "energy_joules_est": 10.15, "sample_count": 3, "duration_seconds": 0.302}, "timestamp": "2026-01-25T20:05:04.184660"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 191.991, "latencies_ms": [191.991], "images_per_second": 5.209, "prompt_tokens": 759, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\n 1. Sheep standing alone 2. Sheep standing next to sheep 3. Sheep standing next to sheep 4. Sheep standing next to sheep", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.8, "ram_available_mb": 112027.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10480.5, "ram_available_mb": 112025.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [31.85, 31.85], "power_watts_avg": 31.85, "power_watts_peak": 31.85, "energy_joules_est": 6.12, "sample_count": 2, "duration_seconds": 0.192}, "timestamp": "2026-01-25T20:05:04.388744"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 114.435, "latencies_ms": [114.435], "images_per_second": 8.739, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Sheep on left side of photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.5, "ram_available_mb": 112025.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [31.85, 31.85], "power_watts_avg": 31.85, "power_watts_peak": 31.85, "energy_joules_est": 3.65, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T20:05:04.594784"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 218.479, "latencies_ms": [218.479], "images_per_second": 4.577, "prompt_tokens": 757, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA group of four sheep are standing on a grassy hill, overlooking a lake. The mountains in the background add to the picturesque scenery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.7, "ram_available_mb": 112025.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10476.8, "ram_available_mb": 112029.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 67.0}, "power_stats": {"power_watts_samples": [32.35, 32.35, 32.35], "power_watts_avg": 32.35, "power_watts_peak": 32.35, "energy_joules_est": 7.08, "sample_count": 3, "duration_seconds": 0.219}, "timestamp": "2026-01-25T20:05:04.901498"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 139.955, "latencies_ms": [139.955], "images_per_second": 7.145, "prompt_tokens": 756, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n A group of sheep standing on a grassy hill with mountains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.8, "ram_available_mb": 112029.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [32.35, 32.35], "power_watts_avg": 32.35, "power_watts_peak": 32.35, "energy_joules_est": 4.54, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T20:05:05.107024"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 281.713, "latencies_ms": [281.713], "images_per_second": 3.55, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA woman in a wheelchair is holding a tennis racket, ready to play, while another person sits nearby with their own racket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [30.98, 30.98, 30.98], "power_watts_avg": 30.98, "power_watts_peak": 30.98, "energy_joules_est": 8.76, "sample_count": 3, "duration_seconds": 0.283}, "timestamp": "2026-01-25T20:05:05.416116"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 89.358, "latencies_ms": [89.358], "images_per_second": 11.191, "prompt_tokens": 759, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Woman in wheelchair holding tennis racket", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10472.4, "ram_available_mb": 112033.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [30.98], "power_watts_avg": 30.98, "power_watts_peak": 30.98, "energy_joules_est": 2.79, "sample_count": 1, "duration_seconds": 0.09}, "timestamp": "2026-01-25T20:05:05.521192"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 119.005, "latencies_ms": [119.005], "images_per_second": 8.403, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe woman in a wheelchair is holding a tennis racket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.4, "ram_available_mb": 112033.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [30.98, 32.74], "power_watts_avg": 31.86, "power_watts_peak": 32.74, "energy_joules_est": 3.8, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T20:05:05.725845"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 270.283, "latencies_ms": [270.283], "images_per_second": 3.7, "prompt_tokens": 757, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nA woman in a wheelchair is holding a tennis racket, while another person sits next to her. They are both seated on chairs within an indoor sports facility with white walls and a basketball hoop visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.5, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [32.74, 32.74, 32.74], "power_watts_avg": 32.74, "power_watts_peak": 32.74, "energy_joules_est": 8.85, "sample_count": 3, "duration_seconds": 0.27}, "timestamp": "2026-01-25T20:05:06.031752"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 45.445, "latencies_ms": [45.445], "images_per_second": 22.005, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 10469.8, "ram_available_mb": 112036.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [32.74], "power_watts_avg": 32.74, "power_watts_peak": 32.74, "energy_joules_est": 1.5, "sample_count": 1, "duration_seconds": 0.046}, "timestamp": "2026-01-25T20:05:06.136182"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 279.658, "latencies_ms": [279.658], "images_per_second": 3.576, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA young child in a pink and white plaid shirt sits atop a brown horse, wearing a black helmet for safety.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.8, "ram_available_mb": 112036.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10472.4, "ram_available_mb": 112033.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [33.69, 33.69, 33.69], "power_watts_avg": 33.69, "power_watts_peak": 33.69, "energy_joules_est": 9.44, "sample_count": 3, "duration_seconds": 0.28}, "timestamp": "2026-01-25T20:05:06.447527"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 77.881, "latencies_ms": [77.881], "images_per_second": 12.84, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Horse saddle pad", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.3, "ram_available_mb": 112034.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [33.69], "power_watts_avg": 33.69, "power_watts_peak": 33.69, "energy_joules_est": 2.64, "sample_count": 1, "duration_seconds": 0.078}, "timestamp": "2026-01-25T20:05:06.553125"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 126.49, "latencies_ms": [126.49], "images_per_second": 7.906, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nThe young girl is sitting on a horse in front of trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10462.6, "ram_available_mb": 112043.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [33.69, 33.31], "power_watts_avg": 33.5, "power_watts_peak": 33.69, "energy_joules_est": 4.24, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:05:06.759256"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 238.763, "latencies_ms": [238.763], "images_per_second": 4.188, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA young girl in a pink and white plaid shirt sits on top of a brown horse, wearing a black helmet. The horse stands still as they enjoy their time together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.6, "ram_available_mb": 112043.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10462.1, "ram_available_mb": 112044.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 82.0}, "power_stats": {"power_watts_samples": [33.31, 33.31, 33.31], "power_watts_avg": 33.31, "power_watts_peak": 33.31, "energy_joules_est": 7.97, "sample_count": 3, "duration_seconds": 0.239}, "timestamp": "2026-01-25T20:05:07.065376"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 299.404, "latencies_ms": [299.404], "images_per_second": 3.34, "prompt_tokens": 756, "response_tokens_est": 49, "n_tiles": 1, "output_text": "\n The image shows a young girl wearing a helmet and riding on the back of a horse. She is sitting in a brown saddle with purple coverings. The background features trees and a forest setting, creating an outdoor atmosphere for this scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.4, "ram_available_mb": 112043.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10463.7, "ram_available_mb": 112042.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [33.31, 32.56, 32.56], "power_watts_avg": 32.81, "power_watts_peak": 33.31, "energy_joules_est": 9.84, "sample_count": 3, "duration_seconds": 0.3}, "timestamp": "2026-01-25T20:05:07.370938"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 393.633, "latencies_ms": [393.633], "images_per_second": 2.54, "prompt_tokens": 744, "response_tokens_est": 50, "n_tiles": 1, "output_text": "\nIn this image, two surfers are riding a wave in the ocean near the coast of New Zealand. The surfer on the left is skillfully navigating the wave while their companion on the right is paddling out to catch another wave.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.7, "ram_available_mb": 112042.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [32.56, 32.56, 36.8, 36.8], "power_watts_avg": 34.68, "power_watts_peak": 36.8, "energy_joules_est": 13.66, "sample_count": 4, "duration_seconds": 0.394}, "timestamp": "2026-01-25T20:05:07.778890"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 121.503, "latencies_ms": [121.503], "images_per_second": 8.23, "prompt_tokens": 759, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. Surfboarder in black wetsuit riding a wave.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.1, "ram_available_mb": 112040.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10466.5, "ram_available_mb": 112039.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [36.8, 36.8], "power_watts_avg": 36.8, "power_watts_peak": 36.8, "energy_joules_est": 4.48, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:05:07.983137"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 337.542, "latencies_ms": [337.542], "images_per_second": 2.963, "prompt_tokens": 763, "response_tokens_est": 58, "n_tiles": 1, "output_text": "\nIn this image, two people are surfing on a wave in the ocean. One person is standing on their surfboard and riding the wave, while another person is paddling out to catch the next wave. The surfers appear to be enjoying themselves as they navigate the waves together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.5, "ram_available_mb": 112039.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [36.8, 38.61, 38.61, 38.61], "power_watts_avg": 38.16, "power_watts_peak": 38.61, "energy_joules_est": 12.91, "sample_count": 4, "duration_seconds": 0.338}, "timestamp": "2026-01-25T20:05:08.390791"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 306.587, "latencies_ms": [306.587], "images_per_second": 3.262, "prompt_tokens": 757, "response_tokens_est": 50, "n_tiles": 1, "output_text": "\nIn this image, two surfers are riding a wave in the ocean. One surfer is standing on their board while the other is lying down on their board. The surfers appear to be enjoying themselves as they navigate the waves together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [38.61, 38.61, 35.21, 35.21], "power_watts_avg": 36.91, "power_watts_peak": 38.61, "energy_joules_est": 11.34, "sample_count": 4, "duration_seconds": 0.307}, "timestamp": "2026-01-25T20:05:08.797726"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 323.443, "latencies_ms": [323.443], "images_per_second": 3.092, "prompt_tokens": 756, "response_tokens_est": 55, "n_tiles": 1, "output_text": "\n The image features two people surfing on a wave in the ocean. One person is standing upright on their surfboard while the other one is lying down with their surfboard extended behind them. The water appears to be calm and blue, creating an ideal environment for surfing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.21, 35.21, 35.21, 35.93], "power_watts_avg": 35.39, "power_watts_peak": 35.93, "energy_joules_est": 11.46, "sample_count": 4, "duration_seconds": 0.324}, "timestamp": "2026-01-25T20:05:09.205785"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 254.068, "latencies_ms": [254.068], "images_per_second": 3.936, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urns of water sit on a windowsill, with a small plant and a picture hanging above them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.93, 35.93, 35.93], "power_watts_avg": 35.93, "power_watts_peak": 35.93, "energy_joules_est": 9.15, "sample_count": 3, "duration_seconds": 0.255}, "timestamp": "2026-01-25T20:05:09.516816"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 96.202, "latencies_ms": [96.202], "images_per_second": 10.395, "prompt_tokens": 759, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Window sill with plants and pictures - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.93], "power_watts_avg": 35.93, "power_watts_peak": 35.93, "energy_joules_est": 3.48, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T20:05:09.621679"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 145.374, "latencies_ms": [145.374], "images_per_second": 6.879, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n A window with a picture hanging on it is located in front of an open cabinet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10465.4, "ram_available_mb": 112040.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.7, 35.7], "power_watts_avg": 35.7, "power_watts_peak": 35.7, "energy_joules_est": 5.2, "sample_count": 2, "duration_seconds": 0.146}, "timestamp": "2026-01-25T20:05:09.826478"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 120.064, "latencies_ms": [120.064], "images_per_second": 8.329, "prompt_tokens": 757, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA small kitchen with a window that has a picture hanging on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.4, "ram_available_mb": 112040.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.7, 35.7], "power_watts_avg": 35.7, "power_watts_peak": 35.7, "energy_joules_est": 4.29, "sample_count": 2, "duration_seconds": 0.12}, "timestamp": "2026-01-25T20:05:10.032466"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 117.806, "latencies_ms": [117.806], "images_per_second": 8.489, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n The kitchen window is open and has a picture hanging on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.8, "ram_available_mb": 112040.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10465.0, "ram_available_mb": 112041.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [35.7, 31.93], "power_watts_avg": 33.82, "power_watts_peak": 35.7, "energy_joules_est": 4.0, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:05:10.238519"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 279.584, "latencies_ms": [279.584], "images_per_second": 3.577, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "urn of incense, a red and white sign with Chinese characters, pineapples, oranges, and cups on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.0, "ram_available_mb": 112041.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [31.93, 31.93, 31.93], "power_watts_avg": 31.93, "power_watts_peak": 31.93, "energy_joules_est": 8.95, "sample_count": 3, "duration_seconds": 0.28}, "timestamp": "2026-01-25T20:05:10.572784"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 99.414, "latencies_ms": [99.414], "images_per_second": 10.059, "prompt_tokens": 759, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Pineapple: 0.17", "error": null, "sys_before": {"cpu_percent": 5.5, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [32.93], "power_watts_avg": 32.93, "power_watts_peak": 32.93, "energy_joules_est": 3.29, "sample_count": 1, "duration_seconds": 0.1}, "timestamp": "2026-01-25T20:05:10.678388"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 139.661, "latencies_ms": [139.661], "images_per_second": 7.16, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n - A red sign with Chinese characters is in front of a pineapple.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [32.93, 32.93], "power_watts_avg": 32.93, "power_watts_peak": 32.93, "energy_joules_est": 4.62, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T20:05:10.884719"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 375.927, "latencies_ms": [375.927], "images_per_second": 2.66, "prompt_tokens": 757, "response_tokens_est": 61, "n_tiles": 1, "output_text": "\nIn a room, there are five red cups on a table next to a pineapple. The cups appear to be filled with incense or candles. A small red sign with Chinese characters is also present in the scene. In front of these objects, there are four oranges arranged neatly on a plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10470.5, "ram_available_mb": 112035.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [32.93, 32.93, 33.64, 33.64], "power_watts_avg": 33.28, "power_watts_peak": 33.64, "energy_joules_est": 12.52, "sample_count": 4, "duration_seconds": 0.376}, "timestamp": "2026-01-25T20:05:11.290628"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 112.394, "latencies_ms": [112.394], "images_per_second": 8.897, "prompt_tokens": 756, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n A red and white sign with Chinese characters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.5, "ram_available_mb": 112035.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10470.1, "ram_available_mb": 112036.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [33.64, 33.64], "power_watts_avg": 33.64, "power_watts_peak": 33.64, "energy_joules_est": 3.79, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T20:05:11.496496"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 253.193, "latencies_ms": [253.193], "images_per_second": 3.95, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urn of sauce on a plate, with a man holding it in front of him and smiling for the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.1, "ram_available_mb": 112036.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10466.8, "ram_available_mb": 112039.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [33.64, 30.24, 30.24], "power_watts_avg": 31.37, "power_watts_peak": 33.64, "energy_joules_est": 7.95, "sample_count": 3, "duration_seconds": 0.254}, "timestamp": "2026-01-25T20:05:11.804022"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.072, "latencies_ms": [76.072], "images_per_second": 13.145, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Sandwich plate", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10466.8, "ram_available_mb": 112039.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10468.3, "ram_available_mb": 112038.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [30.24], "power_watts_avg": 30.24, "power_watts_peak": 30.24, "energy_joules_est": 2.32, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T20:05:11.908995"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 111.511, "latencies_ms": [111.511], "images_per_second": 8.968, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n A man holding a plate of food and wearing glasses.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [30.24, 30.24], "power_watts_avg": 30.24, "power_watts_peak": 30.24, "energy_joules_est": 3.38, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T20:05:12.114749"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 165.421, "latencies_ms": [165.421], "images_per_second": 6.045, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA man in a black shirt holds up his plate of food, which includes a sandwich and some fries.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10470.2, "ram_available_mb": 112036.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [33.83, 33.83], "power_watts_avg": 33.83, "power_watts_peak": 33.83, "energy_joules_est": 5.6, "sample_count": 2, "duration_seconds": 0.166}, "timestamp": "2026-01-25T20:05:12.320557"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 100.934, "latencies_ms": [100.934], "images_per_second": 9.907, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urns of water on the table.\n", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10470.2, "ram_available_mb": 112036.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10469.3, "ram_available_mb": 112037.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [33.83, 33.83], "power_watts_avg": 33.83, "power_watts_peak": 33.83, "energy_joules_est": 3.42, "sample_count": 2, "duration_seconds": 0.101}, "timestamp": "2026-01-25T20:05:12.525319"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 327.814, "latencies_ms": [327.814], "images_per_second": 3.051, "prompt_tokens": 744, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nThe image shows a large building with multiple levels, including a spiral staircase and a circular tower. A group of people can be seen walking on a sidewalk in front of the building.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 10469.3, "ram_available_mb": 112037.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10473.7, "ram_available_mb": 112032.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 51.0}, "power_stats": {"power_watts_samples": [33.83, 30.66, 30.66, 30.66], "power_watts_avg": 31.45, "power_watts_peak": 33.83, "energy_joules_est": 10.33, "sample_count": 4, "duration_seconds": 0.328}, "timestamp": "2026-01-25T20:05:12.935891"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 219.531, "latencies_ms": [219.531], "images_per_second": 4.555, "prompt_tokens": 759, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\n 1. Building  2. Building  3. Building  4. Building  5. Building  6. Building  7. Building  8. Building", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10473.7, "ram_available_mb": 112032.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10479.8, "ram_available_mb": 112026.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [30.66, 30.66, 32.79], "power_watts_avg": 31.37, "power_watts_peak": 32.79, "energy_joules_est": 6.9, "sample_count": 3, "duration_seconds": 0.22}, "timestamp": "2026-01-25T20:05:13.241414"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 135.363, "latencies_ms": [135.363], "images_per_second": 7.388, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA group of people walking on a sidewalk in front of tall buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.8, "ram_available_mb": 112026.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10479.6, "ram_available_mb": 112026.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [32.79, 32.79], "power_watts_avg": 32.79, "power_watts_peak": 32.79, "energy_joules_est": 4.45, "sample_count": 2, "duration_seconds": 0.136}, "timestamp": "2026-01-25T20:05:13.447135"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 315.366, "latencies_ms": [315.366], "images_per_second": 3.171, "prompt_tokens": 757, "response_tokens_est": 53, "n_tiles": 1, "output_text": "\nIn a city, there are two people walking on a sidewalk with umbrellas. The sky above them appears to be gray and cloudy, suggesting that it might rain soon. On the street below, several bicycles are parked or being ridden by their owners.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.6, "ram_available_mb": 112026.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10477.2, "ram_available_mb": 112029.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 23.0}, "power_stats": {"power_watts_samples": [32.79, 32.79, 35.09, 35.09], "power_watts_avg": 33.94, "power_watts_peak": 35.09, "energy_joules_est": 10.72, "sample_count": 4, "duration_seconds": 0.316}, "timestamp": "2026-01-25T20:05:13.854102"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 324.609, "latencies_ms": [324.609], "images_per_second": 3.081, "prompt_tokens": 756, "response_tokens_est": 53, "n_tiles": 1, "output_text": "\n The image shows a large building with a metal tower and a walkway. There are two people walking on the walkway in front of the building. One person is holding an umbrella while they walk down the street. A bicycle can be seen parked nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.2, "ram_available_mb": 112029.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [35.09, 35.09, 35.09, 31.83], "power_watts_avg": 34.27, "power_watts_peak": 35.09, "energy_joules_est": 11.14, "sample_count": 4, "duration_seconds": 0.325}, "timestamp": "2026-01-25T20:05:14.259563"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 236.729, "latencies_ms": [236.729], "images_per_second": 4.224, "prompt_tokens": 744, "response_tokens_est": 18, "n_tiles": 1, "output_text": "iced coffee with a spoon in it, and a fork on top of the plate.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [31.83, 31.83, 31.83], "power_watts_avg": 31.83, "power_watts_peak": 31.83, "energy_joules_est": 7.56, "sample_count": 3, "duration_seconds": 0.237}, "timestamp": "2026-01-25T20:05:14.568770"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 74.873, "latencies_ms": [74.873], "images_per_second": 13.356, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Fork and knife", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [36.36], "power_watts_avg": 36.36, "power_watts_peak": 36.36, "energy_joules_est": 2.74, "sample_count": 1, "duration_seconds": 0.075}, "timestamp": "2026-01-25T20:05:14.674625"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 111.082, "latencies_ms": [111.082], "images_per_second": 9.002, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n A close up of a plate with food on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10470.0, "ram_available_mb": 112036.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 90.0}, "power_stats": {"power_watts_samples": [36.36, 36.36], "power_watts_avg": 36.36, "power_watts_peak": 36.36, "energy_joules_est": 4.05, "sample_count": 2, "duration_seconds": 0.111}, "timestamp": "2026-01-25T20:05:14.880532"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 173.255, "latencies_ms": [173.255], "images_per_second": 5.772, "prompt_tokens": 757, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA close-up view of a plate with a dish covered in yellow cheese sauce, accompanied by a fork on top.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10470.0, "ram_available_mb": 112036.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [36.36, 36.36], "power_watts_avg": 36.36, "power_watts_peak": 36.36, "energy_joules_est": 6.32, "sample_count": 2, "duration_seconds": 0.174}, "timestamp": "2026-01-25T20:05:15.086851"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 120.226, "latencies_ms": [120.226], "images_per_second": 8.318, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A close up of a plate with food and utensils.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10469.4, "ram_available_mb": 112036.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [33.06, 33.06], "power_watts_avg": 33.06, "power_watts_peak": 33.06, "energy_joules_est": 3.98, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T20:05:15.293400"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 288.705, "latencies_ms": [288.705], "images_per_second": 3.464, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA silver laptop sits on a desk, with a mouse and keyboard nearby. The laptop's screen displays a green leafy plant design.", "error": null, "sys_before": {"cpu_percent": 3.6, "ram_used_mb": 10469.4, "ram_available_mb": 112036.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 7.5, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [33.06, 33.06, 33.06], "power_watts_avg": 33.06, "power_watts_peak": 33.06, "energy_joules_est": 9.58, "sample_count": 3, "duration_seconds": 0.29}, "timestamp": "2026-01-25T20:05:15.608878"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 110.064, "latencies_ms": [110.064], "images_per_second": 9.086, "prompt_tokens": 759, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Laptop screen with green leaves - 2", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [32.68, 32.68], "power_watts_avg": 32.68, "power_watts_peak": 32.68, "energy_joules_est": 3.61, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T20:05:15.815555"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 158.782, "latencies_ms": [158.782], "images_per_second": 6.298, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n 1. A laptop with a green leaf design on its screen is open and turned on.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [32.68, 32.68], "power_watts_avg": 32.68, "power_watts_peak": 32.68, "energy_joules_est": 5.2, "sample_count": 2, "duration_seconds": 0.159}, "timestamp": "2026-01-25T20:05:16.021952"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 407.458, "latencies_ms": [407.458], "images_per_second": 2.454, "prompt_tokens": 757, "response_tokens_est": 69, "n_tiles": 1, "output_text": "\nThe image shows a desk with an open laptop, a mouse, and a keyboard. The laptop screen displays a green leafy design, possibly from a computer wallpaper or a photo on the screen. A window can be seen in the background behind the desk, suggesting that it might be located near a window or a view of nature outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 63.0}, "power_stats": {"power_watts_samples": [32.68, 31.17, 31.17, 31.17, 31.17], "power_watts_avg": 31.47, "power_watts_peak": 32.68, "energy_joules_est": 12.84, "sample_count": 5, "duration_seconds": 0.408}, "timestamp": "2026-01-25T20:05:16.530412"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 122.29, "latencies_ms": [122.29], "images_per_second": 8.177, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n A laptop with a green leafy background on the screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10481.4, "ram_available_mb": 112024.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 63.0}, "power_stats": {"power_watts_samples": [31.17, 30.5], "power_watts_avg": 30.84, "power_watts_peak": 31.17, "energy_joules_est": 3.78, "sample_count": 2, "duration_seconds": 0.123}, "timestamp": "2026-01-25T20:05:16.737182"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 257.491, "latencies_ms": [257.491], "images_per_second": 3.884, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "urn of flowers sits on a nightstand in a bedroom with an orange wall and white bedspread.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10481.4, "ram_available_mb": 112024.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10480.2, "ram_available_mb": 112026.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 63.0}, "power_stats": {"power_watts_samples": [30.5, 30.5, 30.5], "power_watts_avg": 30.5, "power_watts_peak": 30.5, "energy_joules_est": 7.87, "sample_count": 3, "duration_seconds": 0.258}, "timestamp": "2026-01-25T20:05:17.050551"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 81.668, "latencies_ms": [81.668], "images_per_second": 12.245, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Lamp: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.2, "ram_available_mb": 112026.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10476.6, "ram_available_mb": 112029.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [30.5], "power_watts_avg": 30.5, "power_watts_peak": 30.5, "energy_joules_est": 2.5, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T20:05:17.156877"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 91.488, "latencies_ms": [91.488], "images_per_second": 10.93, "prompt_tokens": 763, "response_tokens_est": 8, "n_tiles": 1, "output_text": "urn on a table next to bed", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.6, "ram_available_mb": 112029.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10475.3, "ram_available_mb": 112031.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [35.46], "power_watts_avg": 35.46, "power_watts_peak": 35.46, "energy_joules_est": 3.25, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T20:05:17.261445"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 148.116, "latencies_ms": [148.116], "images_per_second": 6.751, "prompt_tokens": 757, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA young girl in a green shirt sits on her bed, surrounded by toys.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10475.9, "ram_available_mb": 112030.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [35.46, 35.46], "power_watts_avg": 35.46, "power_watts_peak": 35.46, "energy_joules_est": 5.27, "sample_count": 2, "duration_seconds": 0.149}, "timestamp": "2026-01-25T20:05:17.467893"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 133.709, "latencies_ms": [133.709], "images_per_second": 7.479, "prompt_tokens": 756, "response_tokens_est": 17, "n_tiles": 1, "output_text": "urns of water on the floor and a lamp with an orange shade.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.9, "ram_available_mb": 112030.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10477.9, "ram_available_mb": 112028.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 88.0}, "power_stats": {"power_watts_samples": [35.46, 31.75], "power_watts_avg": 33.6, "power_watts_peak": 35.46, "energy_joules_est": 4.51, "sample_count": 2, "duration_seconds": 0.134}, "timestamp": "2026-01-25T20:05:17.673612"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 435.803, "latencies_ms": [435.803], "images_per_second": 2.295, "prompt_tokens": 744, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\nA baseball game is in progress, with a batter at home plate holding his bat and waiting for the pitch. Behind him, a catcher crouches behind home plate, ready to catch any balls that come their way. An umpire stands nearby, observing the play closely.", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 10477.9, "ram_available_mb": 112028.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10459.2, "ram_available_mb": 112047.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [31.75, 31.75, 31.75, 31.75, 34.48], "power_watts_avg": 32.29, "power_watts_peak": 34.48, "energy_joules_est": 14.08, "sample_count": 5, "duration_seconds": 0.436}, "timestamp": "2026-01-25T20:05:18.185373"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 71.934, "latencies_ms": [71.934], "images_per_second": 13.902, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Pitcher", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.2, "ram_available_mb": 112047.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.48], "power_watts_avg": 34.48, "power_watts_peak": 34.48, "energy_joules_est": 2.5, "sample_count": 1, "duration_seconds": 0.072}, "timestamp": "2026-01-25T20:05:18.291211"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 98.22, "latencies_ms": [98.22], "images_per_second": 10.181, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nThe umpire is standing behind the catcher.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.48], "power_watts_avg": 34.48, "power_watts_peak": 34.48, "energy_joules_est": 3.4, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T20:05:18.395802"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 299.274, "latencies_ms": [299.274], "images_per_second": 3.341, "prompt_tokens": 757, "response_tokens_est": 51, "n_tiles": 1, "output_text": "\nIn this image, a baseball game is taking place on a field. The batter is standing at home plate holding his bat, ready to swing. Behind him, the catcher and umpire are crouched down, attentively watching the play unfold.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10465.4, "ram_available_mb": 112040.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.48, 34.48, 36.03], "power_watts_avg": 34.99, "power_watts_peak": 36.03, "energy_joules_est": 10.49, "sample_count": 3, "duration_seconds": 0.3}, "timestamp": "2026-01-25T20:05:18.701740"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 450.868, "latencies_ms": [450.868], "images_per_second": 2.218, "prompt_tokens": 756, "response_tokens_est": 81, "n_tiles": 1, "output_text": "\n The image shows a baseball game in progress. A batter is standing at home plate holding his bat and waiting for the pitch. Behind him, there are three people present - one person wearing a red shirt and another person wearing a blue shirt. Additionally, an umpire can be seen behind the catcher, observing the play closely. The scene takes place on a baseball field with green grass underfoot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.4, "ram_available_mb": 112040.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10465.2, "ram_available_mb": 112041.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.03, 36.03, 36.03, 36.03, 35.78], "power_watts_avg": 35.98, "power_watts_peak": 36.03, "energy_joules_est": 16.23, "sample_count": 5, "duration_seconds": 0.451}, "timestamp": "2026-01-25T20:05:19.208242"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 155.766, "latencies_ms": [155.766], "images_per_second": 6.42, "prompt_tokens": 744, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.2, "ram_available_mb": 112041.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10468.5, "ram_available_mb": 112037.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.78, 35.78], "power_watts_avg": 35.78, "power_watts_peak": 35.78, "energy_joules_est": 5.59, "sample_count": 2, "duration_seconds": 0.156}, "timestamp": "2026-01-25T20:05:19.417564"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 89.864, "latencies_ms": [89.864], "images_per_second": 11.128, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bird: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10468.5, "ram_available_mb": 112037.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.78], "power_watts_avg": 35.78, "power_watts_peak": 35.78, "energy_joules_est": 3.23, "sample_count": 1, "duration_seconds": 0.09}, "timestamp": "2026-01-25T20:05:19.522716"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 91.687, "latencies_ms": [91.687], "images_per_second": 10.907, "prompt_tokens": 763, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\nA cat is eating a bird.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10468.8, "ram_available_mb": 112037.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.78], "power_watts_avg": 35.78, "power_watts_peak": 35.78, "energy_joules_est": 3.29, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T20:05:19.627137"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 252.261, "latencies_ms": [252.261], "images_per_second": 3.964, "prompt_tokens": 757, "response_tokens_est": 42, "n_tiles": 1, "output_text": "\nA cat is sitting on a wooden surface, with its head resting on a bird that has been caught. The bird appears to be dead or injured, as evidenced by the blood stains on its body.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.5, "ram_available_mb": 112037.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10467.9, "ram_available_mb": 112038.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [37.33, 37.33, 37.33], "power_watts_avg": 37.33, "power_watts_peak": 37.33, "energy_joules_est": 9.42, "sample_count": 3, "duration_seconds": 0.252}, "timestamp": "2026-01-25T20:05:19.932629"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 194.486, "latencies_ms": [194.486], "images_per_second": 5.142, "prompt_tokens": 756, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\n The image shows a cat with white fur and brown spots on its face. It is sitting on a wooden surface outdoors in the sun.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.9, "ram_available_mb": 112038.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10471.8, "ram_available_mb": 112034.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [37.33, 37.33], "power_watts_avg": 37.33, "power_watts_peak": 37.33, "energy_joules_est": 7.28, "sample_count": 2, "duration_seconds": 0.195}, "timestamp": "2026-01-25T20:05:20.138025"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 342.46, "latencies_ms": [342.46], "images_per_second": 2.92, "prompt_tokens": 744, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nA hand is holding a sandwich with lettuce, tomato, and cheese on a white plate. The sandwich appears to be made of bread and has been cut in half for easier consumption.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.8, "ram_available_mb": 112034.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [34.35, 34.35, 34.35, 34.35], "power_watts_avg": 34.35, "power_watts_peak": 34.35, "energy_joules_est": 11.77, "sample_count": 4, "duration_seconds": 0.343}, "timestamp": "2026-01-25T20:05:20.546028"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 97.17, "latencies_ms": [97.17], "images_per_second": 10.291, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Sandwich: 0.5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [34.35], "power_watts_avg": 34.35, "power_watts_peak": 34.35, "energy_joules_est": 3.35, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T20:05:20.650335"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 125.626, "latencies_ms": [125.626], "images_per_second": 7.96, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nA hand holding a sandwich on top of a white plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [37.68, 37.68], "power_watts_avg": 37.68, "power_watts_peak": 37.68, "energy_joules_est": 4.75, "sample_count": 2, "duration_seconds": 0.126}, "timestamp": "2026-01-25T20:05:20.856147"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 204.244, "latencies_ms": [204.244], "images_per_second": 4.896, "prompt_tokens": 757, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA hand holds a sandwich with lettuce, tomato, and cheese on top of a white plate. The sandwich appears to be partially eaten or prepared for consumption.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [37.68, 37.68, 37.68], "power_watts_avg": 37.68, "power_watts_peak": 37.68, "energy_joules_est": 7.73, "sample_count": 3, "duration_seconds": 0.205}, "timestamp": "2026-01-25T20:05:21.164014"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 302.513, "latencies_ms": [302.513], "images_per_second": 3.306, "prompt_tokens": 756, "response_tokens_est": 48, "n_tiles": 1, "output_text": "\n The image shows a hand holding up a sandwich on top of a white plate. The sandwich is made with bread and tomatoes, and it appears to be quite large in size. There are also some crumbs visible around the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10466.4, "ram_available_mb": 112039.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.12, 36.12, 36.12, 36.12], "power_watts_avg": 36.12, "power_watts_peak": 36.12, "energy_joules_est": 10.94, "sample_count": 4, "duration_seconds": 0.303}, "timestamp": "2026-01-25T20:05:21.570085"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 149.329, "latencies_ms": [149.329], "images_per_second": 6.697, "prompt_tokens": 744, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10466.4, "ram_available_mb": 112039.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10466.7, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [29.66, 29.66], "power_watts_avg": 29.66, "power_watts_peak": 29.66, "energy_joules_est": 4.45, "sample_count": 2, "duration_seconds": 0.15}, "timestamp": "2026-01-25T20:05:21.781164"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 166.383, "latencies_ms": [166.383], "images_per_second": 6.01, "prompt_tokens": 759, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n 1. Hat  2. Water  3. Boat  4. Person  5. Person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.7, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [29.66, 29.66], "power_watts_avg": 29.66, "power_watts_peak": 29.66, "energy_joules_est": 4.96, "sample_count": 2, "duration_seconds": 0.167}, "timestamp": "2026-01-25T20:05:21.986698"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 172.603, "latencies_ms": [172.603], "images_per_second": 5.794, "prompt_tokens": 763, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\n 1. A girl in a white shirt and pink pants sits on top of another girl wearing a hat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [29.66, 31.24], "power_watts_avg": 30.45, "power_watts_peak": 31.24, "energy_joules_est": 5.26, "sample_count": 2, "duration_seconds": 0.173}, "timestamp": "2026-01-25T20:05:22.192848"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 121.605, "latencies_ms": [121.605], "images_per_second": 8.223, "prompt_tokens": 757, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nTwo young girls are sitting on a boat, one wearing a hat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10468.3, "ram_available_mb": 112038.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [31.24, 31.24], "power_watts_avg": 31.24, "power_watts_peak": 31.24, "energy_joules_est": 3.82, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:05:22.399342"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 277.62, "latencies_ms": [277.62], "images_per_second": 3.602, "prompt_tokens": 756, "response_tokens_est": 46, "n_tiles": 1, "output_text": "\n The image shows two girls sitting on a boat. One girl is wearing a white shirt and blue jeans, while the other girl has pink pants and an orange bib. They are both looking out at the ocean together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.3, "ram_available_mb": 112038.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10469.5, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [31.24, 31.24, 32.85], "power_watts_avg": 31.78, "power_watts_peak": 32.85, "energy_joules_est": 8.84, "sample_count": 3, "duration_seconds": 0.278}, "timestamp": "2026-01-25T20:05:22.704382"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 312.749, "latencies_ms": [312.749], "images_per_second": 3.197, "prompt_tokens": 744, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA white sheep stands in a grassy field, facing the camera with its head turned to the left. The background features a stone wall and some plants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.5, "ram_available_mb": 112036.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.85, 32.85, 32.85, 32.85], "power_watts_avg": 32.85, "power_watts_peak": 32.85, "energy_joules_est": 10.28, "sample_count": 4, "duration_seconds": 0.313}, "timestamp": "2026-01-25T20:05:23.115804"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 68.316, "latencies_ms": [68.316], "images_per_second": 14.638, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Sheep", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.95], "power_watts_avg": 36.95, "power_watts_peak": 36.95, "energy_joules_est": 2.54, "sample_count": 1, "duration_seconds": 0.069}, "timestamp": "2026-01-25T20:05:23.220913"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 111.771, "latencies_ms": [111.771], "images_per_second": 8.947, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nA white sheep standing in a grassy field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10472.5, "ram_available_mb": 112033.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.95, 36.95], "power_watts_avg": 36.95, "power_watts_peak": 36.95, "energy_joules_est": 4.14, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T20:05:23.426599"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 197.704, "latencies_ms": [197.704], "images_per_second": 5.058, "prompt_tokens": 757, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA white sheep stands in a grassy field, facing the camera. The background features a stone wall with yellow moss growing on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.5, "ram_available_mb": 112033.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.95, 36.95], "power_watts_avg": 36.95, "power_watts_peak": 36.95, "energy_joules_est": 7.32, "sample_count": 2, "duration_seconds": 0.198}, "timestamp": "2026-01-25T20:05:23.631872"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 296.545, "latencies_ms": [296.545], "images_per_second": 3.372, "prompt_tokens": 756, "response_tokens_est": 48, "n_tiles": 1, "output_text": "\n The image features a white sheep standing in the grass. The sheep is facing towards the camera and appears to be looking at it as well. The background of the image consists of a stone wall with patches of yellow moss on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.3, "ram_available_mb": 112040.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.38, 33.38, 33.38], "power_watts_avg": 33.38, "power_watts_peak": 33.38, "energy_joules_est": 9.91, "sample_count": 3, "duration_seconds": 0.297}, "timestamp": "2026-01-25T20:05:23.937176"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 328.864, "latencies_ms": [328.864], "images_per_second": 3.041, "prompt_tokens": 744, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA man in a white shirt and blue jeans stands next to a truck, watching as another person operates it. The truck is carrying large black pipes on its flatbed trailer.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [33.38, 33.38, 33.22, 33.22], "power_watts_avg": 33.3, "power_watts_peak": 33.38, "energy_joules_est": 10.96, "sample_count": 4, "duration_seconds": 0.329}, "timestamp": "2026-01-25T20:05:24.347893"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 68.707, "latencies_ms": [68.707], "images_per_second": 14.555, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Truck", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [33.22], "power_watts_avg": 33.22, "power_watts_peak": 33.22, "energy_joules_est": 2.3, "sample_count": 1, "duration_seconds": 0.069}, "timestamp": "2026-01-25T20:05:24.453133"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 171.978, "latencies_ms": [171.978], "images_per_second": 5.815, "prompt_tokens": 763, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\n 1. A man in a white shirt and blue jeans standing next to a truck with a yellow cover on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.8, "ram_available_mb": 112047.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10462.3, "ram_available_mb": 112044.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [33.22, 33.22], "power_watts_avg": 33.22, "power_watts_peak": 33.22, "energy_joules_est": 5.72, "sample_count": 2, "duration_seconds": 0.172}, "timestamp": "2026-01-25T20:05:24.659069"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 201.971, "latencies_ms": [201.971], "images_per_second": 4.951, "prompt_tokens": 757, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA man in a white shirt stands next to a truck, watching as another man lifts a large black pipe onto the back of the truck.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.3, "ram_available_mb": 112044.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [37.28, 37.28, 37.28], "power_watts_avg": 37.28, "power_watts_peak": 37.28, "energy_joules_est": 7.54, "sample_count": 3, "duration_seconds": 0.202}, "timestamp": "2026-01-25T20:05:24.964967"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 94.836, "latencies_ms": [94.836], "images_per_second": 10.544, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urns on the back of a truck.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10459.4, "ram_available_mb": 112046.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10463.0, "ram_available_mb": 112043.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [37.28], "power_watts_avg": 37.28, "power_watts_peak": 37.28, "energy_joules_est": 3.54, "sample_count": 1, "duration_seconds": 0.095}, "timestamp": "2026-01-25T20:05:25.068912"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 271.202, "latencies_ms": [271.202], "images_per_second": 3.687, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nIn a zoo enclosure, three giraffes are walking along a dirt path near a small pond and some trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.0, "ram_available_mb": 112043.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10457.6, "ram_available_mb": 112048.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 1.0}, "power_stats": {"power_watts_samples": [32.24, 32.24, 32.24], "power_watts_avg": 32.24, "power_watts_peak": 32.24, "energy_joules_est": 8.75, "sample_count": 3, "duration_seconds": 0.272}, "timestamp": "2026-01-25T20:05:25.380503"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 121.987, "latencies_ms": [121.987], "images_per_second": 8.198, "prompt_tokens": 759, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Giraffe walking away from water hole - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.6, "ram_available_mb": 112048.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10451.1, "ram_available_mb": 112055.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 1.0}, "power_stats": {"power_watts_samples": [32.24, 32.24], "power_watts_avg": 32.24, "power_watts_peak": 32.24, "energy_joules_est": 3.94, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:05:25.586111"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 174.483, "latencies_ms": [174.483], "images_per_second": 5.731, "prompt_tokens": 763, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n 1. A giraffe walking in front of another giraffe and a third giraffe behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.1, "ram_available_mb": 112055.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10450.8, "ram_available_mb": 112055.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 1.0}, "power_stats": {"power_watts_samples": [33.62, 33.62], "power_watts_avg": 33.62, "power_watts_peak": 33.62, "energy_joules_est": 5.88, "sample_count": 2, "duration_seconds": 0.175}, "timestamp": "2026-01-25T20:05:25.791622"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 180.912, "latencies_ms": [180.912], "images_per_second": 5.528, "prompt_tokens": 757, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\n Three giraffes are walking across a dirt road in their zoo enclosure, with one of them looking back over its shoulder.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.0, "ram_available_mb": 112055.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10454.3, "ram_available_mb": 112052.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 1.0}, "power_stats": {"power_watts_samples": [33.62, 33.62], "power_watts_avg": 33.62, "power_watts_peak": 33.62, "energy_joules_est": 6.1, "sample_count": 2, "duration_seconds": 0.181}, "timestamp": "2026-01-25T20:05:25.998157"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 126.489, "latencies_ms": [126.489], "images_per_second": 7.906, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A giraffe walking in front of two other giraffes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.3, "ram_available_mb": 112052.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10455.8, "ram_available_mb": 112050.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [33.62, 33.69], "power_watts_avg": 33.66, "power_watts_peak": 33.69, "energy_joules_est": 4.27, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:05:26.203755"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 308.906, "latencies_ms": [308.906], "images_per_second": 3.237, "prompt_tokens": 744, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA pizza with mushrooms, cheese and ham sits on a white plate atop a table in a restaurant. Two glasses of beer are placed next to the pizza.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10459.6, "ram_available_mb": 112046.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [33.69, 33.69, 33.69, 33.69], "power_watts_avg": 33.69, "power_watts_peak": 33.69, "energy_joules_est": 10.43, "sample_count": 4, "duration_seconds": 0.31}, "timestamp": "2026-01-25T20:05:26.614233"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 121.603, "latencies_ms": [121.603], "images_per_second": 8.223, "prompt_tokens": 759, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Pizza with mushrooms, cheese and ham - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.6, "ram_available_mb": 112046.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [34.09, 34.09], "power_watts_avg": 34.09, "power_watts_peak": 34.09, "energy_joules_est": 4.16, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:05:26.819515"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 142.562, "latencies_ms": [142.562], "images_per_second": 7.014, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n A plate of pizza with a glass of beer and two wine glasses on it.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 62.0}, "power_stats": {"power_watts_samples": [34.09, 34.09], "power_watts_avg": 34.09, "power_watts_peak": 34.09, "energy_joules_est": 4.87, "sample_count": 2, "duration_seconds": 0.143}, "timestamp": "2026-01-25T20:05:27.026547"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 173.688, "latencies_ms": [173.688], "images_per_second": 5.757, "prompt_tokens": 757, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\n A table with a pizza on top of a white plate, two glasses of beer, and a bowl of food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [34.09, 31.31], "power_watts_avg": 32.7, "power_watts_peak": 34.09, "energy_joules_est": 5.7, "sample_count": 2, "duration_seconds": 0.174}, "timestamp": "2026-01-25T20:05:27.232289"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 92.612, "latencies_ms": [92.612], "images_per_second": 10.798, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urns of water on the table.\n", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [31.31], "power_watts_avg": 31.31, "power_watts_peak": 31.31, "energy_joules_est": 2.91, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:05:27.337179"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 293.822, "latencies_ms": [293.822], "images_per_second": 3.403, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA black cat is drinking water from a silver faucet in a bathroom sink, with its head tilted down and mouth open wide.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [31.31, 31.31, 31.31], "power_watts_avg": 31.31, "power_watts_peak": 31.31, "energy_joules_est": 9.21, "sample_count": 3, "duration_seconds": 0.294}, "timestamp": "2026-01-25T20:05:27.644638"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 91.844, "latencies_ms": [91.844], "images_per_second": 10.888, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Soap bottle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10466.5, "ram_available_mb": 112039.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [32.22], "power_watts_avg": 32.22, "power_watts_peak": 32.22, "energy_joules_est": 2.96, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T20:05:27.749083"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 172.998, "latencies_ms": [172.998], "images_per_second": 5.78, "prompt_tokens": 763, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\n 1. A cat drinking water from a faucet in front of a sink and soap dispenser.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.5, "ram_available_mb": 112039.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [32.22, 32.22], "power_watts_avg": 32.22, "power_watts_peak": 32.22, "energy_joules_est": 5.58, "sample_count": 2, "duration_seconds": 0.173}, "timestamp": "2026-01-25T20:05:27.955318"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 128.585, "latencies_ms": [128.585], "images_per_second": 7.777, "prompt_tokens": 757, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA black cat drinks from a silver faucet in a bathroom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [32.22, 32.22], "power_watts_avg": 32.22, "power_watts_peak": 32.22, "energy_joules_est": 4.16, "sample_count": 2, "duration_seconds": 0.129}, "timestamp": "2026-01-25T20:05:28.161724"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 92.256, "latencies_ms": [92.256], "images_per_second": 10.839, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The cat is black and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.5, "ram_available_mb": 112041.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [35.99], "power_watts_avg": 35.99, "power_watts_peak": 35.99, "energy_joules_est": 3.34, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:05:28.267159"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 259.892, "latencies_ms": [259.892], "images_per_second": 3.848, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nTwo people are riding a horse-drawn carriage through a muddy field, with their reflections visible in the water below.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [35.99, 35.99, 35.99], "power_watts_avg": 35.99, "power_watts_peak": 35.99, "energy_joules_est": 9.36, "sample_count": 3, "duration_seconds": 0.26}, "timestamp": "2026-01-25T20:05:28.579436"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 72.628, "latencies_ms": [72.628], "images_per_second": 13.769, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Barn", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [34.57], "power_watts_avg": 34.57, "power_watts_peak": 34.57, "energy_joules_est": 2.52, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T20:05:28.684520"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 120.617, "latencies_ms": [120.617], "images_per_second": 8.291, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Horse pulling a buggy on muddy ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10461.9, "ram_available_mb": 112044.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [34.57, 34.57], "power_watts_avg": 34.57, "power_watts_peak": 34.57, "energy_joules_est": 4.18, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T20:05:28.890342"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 167.022, "latencies_ms": [167.022], "images_per_second": 5.987, "prompt_tokens": 757, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nIn a muddy field, two people are riding in an old-fashioned wooden carriage pulled by a brown horse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.9, "ram_available_mb": 112044.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10460.9, "ram_available_mb": 112045.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [34.57, 34.57], "power_watts_avg": 34.57, "power_watts_peak": 34.57, "energy_joules_est": 5.79, "sample_count": 2, "duration_seconds": 0.168}, "timestamp": "2026-01-25T20:05:29.095627"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 459.942, "latencies_ms": [459.942], "images_per_second": 2.174, "prompt_tokens": 756, "response_tokens_est": 78, "n_tiles": 1, "output_text": "\n The image shows a horse pulling a buggy on muddy ground. There are two people riding in the buggy and one person is standing next to it. The horse appears to be brown with black spots, and its reflection can be seen in the water below. The sky above them is blue, suggesting that they might be in an area where there's not much vegetation or trees nearby.\n", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10460.9, "ram_available_mb": 112045.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10460.9, "ram_available_mb": 112045.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [32.04, 32.04, 32.04, 32.04, 32.04], "power_watts_avg": 32.04, "power_watts_peak": 32.04, "energy_joules_est": 14.74, "sample_count": 5, "duration_seconds": 0.46}, "timestamp": "2026-01-25T20:05:29.603241"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 316.461, "latencies_ms": [316.461], "images_per_second": 3.16, "prompt_tokens": 744, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA man in a black suit and tie holds an umbrella over his head, while a woman in a white dress stands next to him on a grassy lawn.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.9, "ram_available_mb": 112045.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10462.0, "ram_available_mb": 112044.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 66.0}, "power_stats": {"power_watts_samples": [32.31, 32.31, 32.31, 32.31], "power_watts_avg": 32.31, "power_watts_peak": 32.31, "energy_joules_est": 10.24, "sample_count": 4, "duration_seconds": 0.317}, "timestamp": "2026-01-25T20:05:30.013748"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 85.487, "latencies_ms": [85.487], "images_per_second": 11.698, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n Umbrella: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.0, "ram_available_mb": 112044.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 17.0}, "power_stats": {"power_watts_samples": [32.31], "power_watts_avg": 32.31, "power_watts_peak": 32.31, "energy_joules_est": 2.78, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T20:05:30.118648"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 147.84, "latencies_ms": [147.84], "images_per_second": 6.764, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nThe bride and groom are standing in a grassy area with an umbrella.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10458.7, "ram_available_mb": 112047.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 17.0}, "power_stats": {"power_watts_samples": [36.67, 36.67], "power_watts_avg": 36.67, "power_watts_peak": 36.67, "energy_joules_est": 5.43, "sample_count": 2, "duration_seconds": 0.148}, "timestamp": "2026-01-25T20:05:30.323387"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 253.664, "latencies_ms": [253.664], "images_per_second": 3.942, "prompt_tokens": 757, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\nA man in a suit holds an umbrella over a woman wearing a white dress, who has her hand on her face. They are standing outside of a stone building with a red roof.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.9, "ram_available_mb": 112047.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 17.0}, "power_stats": {"power_watts_samples": [36.67, 36.67, 36.67], "power_watts_avg": 36.67, "power_watts_peak": 36.67, "energy_joules_est": 9.31, "sample_count": 3, "duration_seconds": 0.254}, "timestamp": "2026-01-25T20:05:30.629181"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 120.78, "latencies_ms": [120.78], "images_per_second": 8.28, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The bride is wearing a white dress and holding an umbrella.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 17.0}, "power_stats": {"power_watts_samples": [34.08, 34.08], "power_watts_avg": 34.08, "power_watts_peak": 34.08, "energy_joules_est": 4.13, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T20:05:30.834582"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 305.565, "latencies_ms": [305.565], "images_per_second": 3.273, "prompt_tokens": 744, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA man is lying on a sandy beach, with his legs crossed and his feet in the air, while another person stands nearby holding a kite string.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [34.08, 34.08, 34.08, 33.97], "power_watts_avg": 34.05, "power_watts_peak": 34.08, "energy_joules_est": 10.44, "sample_count": 4, "duration_seconds": 0.307}, "timestamp": "2026-01-25T20:05:31.246918"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.949, "latencies_ms": [79.949], "images_per_second": 12.508, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Kite - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [33.97], "power_watts_avg": 33.97, "power_watts_peak": 33.97, "energy_joules_est": 2.73, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T20:05:31.351951"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 144.178, "latencies_ms": [144.178], "images_per_second": 6.936, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n A man is laying on a beach with his legs crossed and feet up.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [33.97, 33.97], "power_watts_avg": 33.97, "power_watts_peak": 33.97, "energy_joules_est": 4.91, "sample_count": 2, "duration_seconds": 0.144}, "timestamp": "2026-01-25T20:05:31.557154"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 229.012, "latencies_ms": [229.012], "images_per_second": 4.367, "prompt_tokens": 757, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\n A man in a white shirt lies on the beach, with his legs crossed. A colorful kite is flying above him, adding to the relaxing atmosphere of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [33.97, 31.26, 31.26], "power_watts_avg": 32.16, "power_watts_peak": 33.97, "energy_joules_est": 7.39, "sample_count": 3, "duration_seconds": 0.23}, "timestamp": "2026-01-25T20:05:31.863101"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 121.273, "latencies_ms": [121.273], "images_per_second": 8.246, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A colorful kite is flying in the sky above a sandy beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [31.26, 31.26], "power_watts_avg": 31.26, "power_watts_peak": 31.26, "energy_joules_est": 3.8, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:05:32.067586"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 238.485, "latencies_ms": [238.485], "images_per_second": 4.193, "prompt_tokens": 744, "response_tokens_est": 20, "n_tiles": 1, "output_text": "urns of water on a table, with a lamp and television nearby in a living room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 68.0}, "power_stats": {"power_watts_samples": [28.59, 28.59, 28.59], "power_watts_avg": 28.59, "power_watts_peak": 28.59, "energy_joules_est": 6.85, "sample_count": 3, "duration_seconds": 0.24}, "timestamp": "2026-01-25T20:05:32.375894"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.269, "latencies_ms": [76.269], "images_per_second": 13.111, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Couch - 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 68.0}, "power_stats": {"power_watts_samples": [28.59], "power_watts_avg": 28.59, "power_watts_peak": 28.59, "energy_joules_est": 2.19, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T20:05:32.481599"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 134.471, "latencies_ms": [134.471], "images_per_second": 7.437, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n A brown couch sits in front of a table with a lamp on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10465.2, "ram_available_mb": 112041.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 68.0}, "power_stats": {"power_watts_samples": [28.59, 33.0], "power_watts_avg": 30.79, "power_watts_peak": 33.0, "energy_joules_est": 4.15, "sample_count": 2, "duration_seconds": 0.135}, "timestamp": "2026-01-25T20:05:32.685589"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 435.687, "latencies_ms": [435.687], "images_per_second": 2.295, "prompt_tokens": 757, "response_tokens_est": 76, "n_tiles": 1, "output_text": "\nThe image shows a cozy living room with beige walls, a brown couch, a black chair, and a television. The room has two windows that let in natural light, creating a warm atmosphere. A lamp on one of the tables provides additional lighting for reading or relaxing. There are also some books scattered around the room, suggesting that someone may have been reading recently.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.2, "ram_available_mb": 112041.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10465.7, "ram_available_mb": 112040.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.0, 33.0, 33.0, 33.0, 36.46], "power_watts_avg": 33.69, "power_watts_peak": 36.46, "energy_joules_est": 14.69, "sample_count": 5, "duration_seconds": 0.436}, "timestamp": "2026-01-25T20:05:33.191310"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 247.506, "latencies_ms": [247.506], "images_per_second": 4.04, "prompt_tokens": 756, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\n The room is painted white and has a brown couch. There are two windows with wooden shutters that let in natural light. A lamp on the table next to the window provides additional lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.7, "ram_available_mb": 112040.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.46, 36.46, 36.46], "power_watts_avg": 36.46, "power_watts_peak": 36.46, "energy_joules_est": 9.05, "sample_count": 3, "duration_seconds": 0.248}, "timestamp": "2026-01-25T20:05:33.498163"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 221.584, "latencies_ms": [221.584], "images_per_second": 4.513, "prompt_tokens": 744, "response_tokens_est": 16, "n_tiles": 1, "output_text": "urn of a man eating cake in front of trees and grass, wearing glasses", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10468.7, "ram_available_mb": 112037.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.46, 34.47, 34.47], "power_watts_avg": 35.13, "power_watts_peak": 36.46, "energy_joules_est": 7.8, "sample_count": 3, "duration_seconds": 0.222}, "timestamp": "2026-01-25T20:05:33.808782"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 149.762, "latencies_ms": [149.762], "images_per_second": 6.677, "prompt_tokens": 759, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n Man eating cake, plate in hand, glasses on face, trees and sky behind him", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.7, "ram_available_mb": 112037.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10465.8, "ram_available_mb": 112040.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.47, 34.47], "power_watts_avg": 34.47, "power_watts_peak": 34.47, "energy_joules_est": 5.18, "sample_count": 2, "duration_seconds": 0.15}, "timestamp": "2026-01-25T20:05:34.014081"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 100.059, "latencies_ms": [100.059], "images_per_second": 9.994, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\nA man eating a piece of cake.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.8, "ram_available_mb": 112040.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [34.47], "power_watts_avg": 34.47, "power_watts_peak": 34.47, "energy_joules_est": 3.47, "sample_count": 1, "duration_seconds": 0.101}, "timestamp": "2026-01-25T20:05:34.119105"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 136.805, "latencies_ms": [136.805], "images_per_second": 7.31, "prompt_tokens": 757, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA man in a blue shirt is sitting on a bench eating cake.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.9, "ram_available_mb": 112040.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [33.16, 33.16], "power_watts_avg": 33.16, "power_watts_peak": 33.16, "energy_joules_est": 4.54, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T20:05:34.323823"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 100.094, "latencies_ms": [100.094], "images_per_second": 9.991, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The man is eating a piece of cake outdoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [33.16, 33.16], "power_watts_avg": 33.16, "power_watts_peak": 33.16, "energy_joules_est": 3.33, "sample_count": 2, "duration_seconds": 0.1}, "timestamp": "2026-01-25T20:05:34.528938"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 273.889, "latencies_ms": [273.889], "images_per_second": 3.651, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA man stands next to a brown horse, both wearing purple vests and holding onto bags of luggage strapped to their backs.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [33.16, 29.67, 29.67], "power_watts_avg": 30.83, "power_watts_peak": 33.16, "energy_joules_est": 8.47, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T20:05:34.838806"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.643, "latencies_ms": [78.643], "images_per_second": 12.716, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Horse saddlebags", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10468.7, "ram_available_mb": 112037.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [29.67], "power_watts_avg": 29.67, "power_watts_peak": 29.67, "energy_joules_est": 2.35, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T20:05:34.943632"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 131.568, "latencies_ms": [131.568], "images_per_second": 7.601, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nA man is standing next to a horse carrying several bags on its back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.7, "ram_available_mb": 112037.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10465.2, "ram_available_mb": 112041.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [29.67, 29.67], "power_watts_avg": 29.67, "power_watts_peak": 29.67, "energy_joules_est": 3.92, "sample_count": 2, "duration_seconds": 0.132}, "timestamp": "2026-01-25T20:05:35.149345"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 233.099, "latencies_ms": [233.099], "images_per_second": 4.29, "prompt_tokens": 757, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA man in a purple vest stands next to a brown horse carrying several bags on its back. The horse is walking through a forested area, with trees surrounding them.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 10465.2, "ram_available_mb": 112041.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [32.97, 32.97, 32.97], "power_watts_avg": 32.97, "power_watts_peak": 32.97, "energy_joules_est": 7.72, "sample_count": 3, "duration_seconds": 0.234}, "timestamp": "2026-01-25T20:05:35.457863"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 199.248, "latencies_ms": [199.248], "images_per_second": 5.019, "prompt_tokens": 756, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\n A man is standing next to a horse carrying several bags on its back. The horse and the bags are brown in color with red accents.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [32.97, 32.97], "power_watts_avg": 32.97, "power_watts_peak": 32.97, "energy_joules_est": 6.58, "sample_count": 2, "duration_seconds": 0.2}, "timestamp": "2026-01-25T20:05:35.662858"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 271.794, "latencies_ms": [271.794], "images_per_second": 3.679, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA boat is docked at a pier, with people walking along the boardwalk and enjoying the illuminated bridge in the background.", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 10460.1, "ram_available_mb": 112046.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10458.3, "ram_available_mb": 112048.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [35.0, 35.0, 35.0], "power_watts_avg": 35.0, "power_watts_peak": 35.0, "energy_joules_est": 9.54, "sample_count": 3, "duration_seconds": 0.273}, "timestamp": "2026-01-25T20:05:35.975735"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 221.637, "latencies_ms": [221.637], "images_per_second": 4.512, "prompt_tokens": 759, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\n 1. Bridge  2. Bridge  3. Bridge  4. Bridge  5. Bridge  6. Bridge  7. Bridge  8. Bridge", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.3, "ram_available_mb": 112048.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10458.1, "ram_available_mb": 112048.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [35.0, 35.52, 35.52], "power_watts_avg": 35.35, "power_watts_peak": 35.52, "energy_joules_est": 7.86, "sample_count": 3, "duration_seconds": 0.222}, "timestamp": "2026-01-25T20:05:36.282069"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 97.432, "latencies_ms": [97.432], "images_per_second": 10.264, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urns of water in front of bridge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.1, "ram_available_mb": 112048.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [35.52], "power_watts_avg": 35.52, "power_watts_peak": 35.52, "energy_joules_est": 3.48, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T20:05:36.386137"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 347.752, "latencies_ms": [347.752], "images_per_second": 2.876, "prompt_tokens": 757, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\nA group of people are walking along a pier, with a boat docked at one end. The boat has lights on its side that illuminate the water below. In the background, there's a bridge spanning across a body of water, which is also illuminated by blue lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10459.2, "ram_available_mb": 112047.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [35.52, 35.52, 36.88, 36.88], "power_watts_avg": 36.2, "power_watts_peak": 36.88, "energy_joules_est": 12.59, "sample_count": 4, "duration_seconds": 0.348}, "timestamp": "2026-01-25T20:05:36.791972"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 109.424, "latencies_ms": [109.424], "images_per_second": 9.139, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "urns of blue and white lights on the bridge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.2, "ram_available_mb": 112047.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [36.88, 36.88], "power_watts_avg": 36.88, "power_watts_peak": 36.88, "energy_joules_est": 4.04, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T20:05:36.997670"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 238.149, "latencies_ms": [238.149], "images_per_second": 4.199, "prompt_tokens": 744, "response_tokens_est": 19, "n_tiles": 1, "output_text": "urn of pink high heels with a bow on them, worn by a person's foot.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10462.0, "ram_available_mb": 112044.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [36.88, 31.0, 31.0], "power_watts_avg": 32.96, "power_watts_peak": 36.88, "energy_joules_est": 7.88, "sample_count": 3, "duration_seconds": 0.239}, "timestamp": "2026-01-25T20:05:37.310818"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 100.994, "latencies_ms": [100.994], "images_per_second": 9.902, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Pink high heel shoe", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.9, "ram_available_mb": 112044.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10470.0, "ram_available_mb": 112036.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [31.0, 31.0], "power_watts_avg": 31.0, "power_watts_peak": 31.0, "energy_joules_est": 3.14, "sample_count": 2, "duration_seconds": 0.101}, "timestamp": "2026-01-25T20:05:37.516809"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 125.61, "latencies_ms": [125.61], "images_per_second": 7.961, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A pink high heel shoe is shown in a close up view.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.0, "ram_available_mb": 112036.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [31.0, 31.11], "power_watts_avg": 31.05, "power_watts_peak": 31.11, "energy_joules_est": 3.91, "sample_count": 2, "duration_seconds": 0.126}, "timestamp": "2026-01-25T20:05:37.721460"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 138.913, "latencies_ms": [138.913], "images_per_second": 7.199, "prompt_tokens": 757, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A pink high heel shoe with a bow sits on a wooden bench.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10468.8, "ram_available_mb": 112037.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [31.11, 31.11], "power_watts_avg": 31.11, "power_watts_peak": 31.11, "energy_joules_est": 4.34, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T20:05:37.927462"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 117.967, "latencies_ms": [117.967], "images_per_second": 8.477, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n The pink high heel shoe is the main focus of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.8, "ram_available_mb": 112037.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10473.6, "ram_available_mb": 112032.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 36.0}, "power_stats": {"power_watts_samples": [31.11, 31.11], "power_watts_avg": 31.11, "power_watts_peak": 31.11, "energy_joules_est": 3.69, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:05:38.134292"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 237.446, "latencies_ms": [237.446], "images_per_second": 4.211, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urn of flowers sits on a table, with two people standing behind it and smiling at each other.\n", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 10473.6, "ram_available_mb": 112032.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10473.6, "ram_available_mb": 112032.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 36.0}, "power_stats": {"power_watts_samples": [30.59, 30.59, 30.59], "power_watts_avg": 30.59, "power_watts_peak": 30.59, "energy_joules_est": 7.28, "sample_count": 3, "duration_seconds": 0.238}, "timestamp": "2026-01-25T20:05:38.445011"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 80.985, "latencies_ms": [80.985], "images_per_second": 12.348, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Knife", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.6, "ram_available_mb": 112032.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10472.4, "ram_available_mb": 112033.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 36.0}, "power_stats": {"power_watts_samples": [30.59], "power_watts_avg": 30.59, "power_watts_peak": 30.59, "energy_joules_est": 2.49, "sample_count": 1, "duration_seconds": 0.081}, "timestamp": "2026-01-25T20:05:38.549915"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 130.137, "latencies_ms": [130.137], "images_per_second": 7.684, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nThe boy is standing next to a table with a cake on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.4, "ram_available_mb": 112033.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10471.0, "ram_available_mb": 112035.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 36.0}, "power_stats": {"power_watts_samples": [30.59, 30.52], "power_watts_avg": 30.56, "power_watts_peak": 30.59, "energy_joules_est": 3.99, "sample_count": 2, "duration_seconds": 0.13}, "timestamp": "2026-01-25T20:05:38.755940"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 213.514, "latencies_ms": [213.514], "images_per_second": 4.684, "prompt_tokens": 757, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA woman in a green sweater stands next to a young boy, both holding knives. They are positioned near a dining table with a cake on top of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.0, "ram_available_mb": 112035.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10472.3, "ram_available_mb": 112034.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 36.0}, "power_stats": {"power_watts_samples": [30.52, 30.52, 30.52], "power_watts_avg": 30.52, "power_watts_peak": 30.52, "energy_joules_est": 6.53, "sample_count": 3, "duration_seconds": 0.214}, "timestamp": "2026-01-25T20:05:39.060706"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 138.969, "latencies_ms": [138.969], "images_per_second": 7.196, "prompt_tokens": 756, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n The woman is wearing a green sweater and the boy has blue plaid shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.3, "ram_available_mb": 112034.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [30.52, 31.08], "power_watts_avg": 30.8, "power_watts_peak": 31.08, "energy_joules_est": 4.29, "sample_count": 2, "duration_seconds": 0.139}, "timestamp": "2026-01-25T20:05:39.265174"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 256.217, "latencies_ms": [256.217], "images_per_second": 3.903, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nTwo elephants, one brown and one gray, stand in an indoor arena with a trainer standing nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10463.5, "ram_available_mb": 112042.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 59.0}, "power_stats": {"power_watts_samples": [31.08, 31.08, 31.08], "power_watts_avg": 31.08, "power_watts_peak": 31.08, "energy_joules_est": 7.99, "sample_count": 3, "duration_seconds": 0.257}, "timestamp": "2026-01-25T20:05:39.575016"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 489.551, "latencies_ms": [489.551], "images_per_second": 2.043, "prompt_tokens": 759, "response_tokens_est": 88, "n_tiles": 1, "output_text": "\n 1. Elephant: brown trunk and ears, gray body, blue harness, white face, black eyes, white nose, white legs, white feet, white tail, white backside, white belly, white chest, white head, white ears, white paws, white legs, white tail, white backside, white belly, white chest, white head, white ears, white paws, white legs, white tail, white backside", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.5, "ram_available_mb": 112042.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10460.8, "ram_available_mb": 112045.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.36, 33.36, 33.36, 33.36, 33.36], "power_watts_avg": 33.36, "power_watts_peak": 33.36, "energy_joules_est": 16.35, "sample_count": 5, "duration_seconds": 0.49}, "timestamp": "2026-01-25T20:05:40.081200"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 123.974, "latencies_ms": [123.974], "images_per_second": 8.066, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe elephant in front is being led by a person.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.4, "ram_available_mb": 112047.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [39.15, 39.15], "power_watts_avg": 39.15, "power_watts_peak": 39.15, "energy_joules_est": 4.86, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T20:05:40.287028"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 218.248, "latencies_ms": [218.248], "images_per_second": 4.582, "prompt_tokens": 757, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nTwo elephants, one brown and one gray, are performing in a circus ring. The trainer is holding onto them as they walk around on a dirt floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [39.15, 39.15, 39.15], "power_watts_avg": 39.15, "power_watts_peak": 39.15, "energy_joules_est": 8.56, "sample_count": 3, "duration_seconds": 0.219}, "timestamp": "2026-01-25T20:05:40.593238"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 368.422, "latencies_ms": [368.422], "images_per_second": 2.714, "prompt_tokens": 756, "response_tokens_est": 64, "n_tiles": 1, "output_text": "\n The image shows two elephants in an indoor circus ring. One elephant is brown and the other one is gray. They are standing next to each other on a dirt floor with white walls surrounding them. There are several chairs placed around the area, indicating that this might be a part of a larger performance or show.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10460.0, "ram_available_mb": 112046.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.52, 34.52, 34.52, 34.52], "power_watts_avg": 34.52, "power_watts_peak": 34.52, "energy_joules_est": 12.73, "sample_count": 4, "duration_seconds": 0.369}, "timestamp": "2026-01-25T20:05:41.000346"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 259.244, "latencies_ms": [259.244], "images_per_second": 3.857, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nTwo people are riding horses on a beach, with their backs to the camera and the ocean in the background.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10460.0, "ram_available_mb": 112046.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [34.52, 31.95, 31.95], "power_watts_avg": 32.8, "power_watts_peak": 34.52, "energy_joules_est": 8.51, "sample_count": 3, "duration_seconds": 0.26}, "timestamp": "2026-01-25T20:05:41.313384"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 114.93, "latencies_ms": [114.93], "images_per_second": 8.701, "prompt_tokens": 759, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Horse and rider in front of the water", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [31.95, 31.95], "power_watts_avg": 31.95, "power_watts_peak": 31.95, "energy_joules_est": 3.68, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T20:05:41.519725"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 119.713, "latencies_ms": [119.713], "images_per_second": 8.353, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Horse on left is running towards horse on right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.2, "ram_available_mb": 112043.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [31.95, 35.08], "power_watts_avg": 33.52, "power_watts_peak": 35.08, "energy_joules_est": 4.03, "sample_count": 2, "duration_seconds": 0.12}, "timestamp": "2026-01-25T20:05:41.727176"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 229.871, "latencies_ms": [229.871], "images_per_second": 4.35, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nTwo people are riding horses on a beach, with one horse slightly ahead of the other. The riders appear to be enjoying their time as they gallop along the shoreline.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 50.0}, "power_stats": {"power_watts_samples": [35.08, 35.08, 35.08], "power_watts_avg": 35.08, "power_watts_peak": 35.08, "energy_joules_est": 8.08, "sample_count": 3, "duration_seconds": 0.23}, "timestamp": "2026-01-25T20:05:42.034322"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 237.066, "latencies_ms": [237.066], "images_per_second": 4.218, "prompt_tokens": 756, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nThe image is a black and white photograph of two people riding horses on the beach. The sky in the background appears to be cloudy, which adds an atmospheric quality to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [35.08, 32.6, 32.6], "power_watts_avg": 33.43, "power_watts_peak": 35.08, "energy_joules_est": 7.94, "sample_count": 3, "duration_seconds": 0.238}, "timestamp": "2026-01-25T20:05:42.341586"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 188.785, "latencies_ms": [188.785], "images_per_second": 5.297, "prompt_tokens": 744, "response_tokens_est": 9, "n_tiles": 1, "output_text": "iced coffee in a cup on a table", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [32.6, 32.6], "power_watts_avg": 32.6, "power_watts_peak": 32.6, "energy_joules_est": 6.16, "sample_count": 2, "duration_seconds": 0.189}, "timestamp": "2026-01-25T20:05:42.553315"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 61.564, "latencies_ms": [61.564], "images_per_second": 16.243, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Hat", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [32.6], "power_watts_avg": 32.6, "power_watts_peak": 32.6, "energy_joules_est": 2.01, "sample_count": 1, "duration_seconds": 0.062}, "timestamp": "2026-01-25T20:05:42.659132"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 131.971, "latencies_ms": [131.971], "images_per_second": 7.577, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nThe man is talking on his cell phone while standing in a snowy field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.6, "ram_available_mb": 112038.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10467.1, "ram_available_mb": 112039.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [34.3, 34.3], "power_watts_avg": 34.3, "power_watts_peak": 34.3, "energy_joules_est": 4.54, "sample_count": 2, "duration_seconds": 0.132}, "timestamp": "2026-01-25T20:05:42.866064"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 160.333, "latencies_ms": [160.333], "images_per_second": 6.237, "prompt_tokens": 757, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA young man in a black jacket stands on a snowy path, talking on his cell phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.1, "ram_available_mb": 112039.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10461.8, "ram_available_mb": 112044.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [34.3, 34.3], "power_watts_avg": 34.3, "power_watts_peak": 34.3, "energy_joules_est": 5.52, "sample_count": 2, "duration_seconds": 0.161}, "timestamp": "2026-01-25T20:05:43.071808"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 137.743, "latencies_ms": [137.743], "images_per_second": 7.26, "prompt_tokens": 756, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n The young man is wearing a black jacket and talking on his cell phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.8, "ram_available_mb": 112044.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [33.63, 33.63], "power_watts_avg": 33.63, "power_watts_peak": 33.63, "energy_joules_est": 4.65, "sample_count": 2, "duration_seconds": 0.138}, "timestamp": "2026-01-25T20:05:43.279269"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 308.159, "latencies_ms": [308.159], "images_per_second": 3.245, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA white motorcycle is parked in a field, with a green tent and black bag nearby. The scene also includes trees and bushes in the background.", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [33.63, 33.63, 33.63, 32.78], "power_watts_avg": 33.42, "power_watts_peak": 33.63, "energy_joules_est": 10.32, "sample_count": 4, "duration_seconds": 0.309}, "timestamp": "2026-01-25T20:05:43.695112"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 93.043, "latencies_ms": [93.043], "images_per_second": 10.748, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Tent - 2", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10470.0, "ram_available_mb": 112036.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [32.78], "power_watts_avg": 32.78, "power_watts_peak": 32.78, "energy_joules_est": 3.07, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T20:05:43.800044"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 138.436, "latencies_ms": [138.436], "images_per_second": 7.224, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A green tent is on the left side of a white motorcycle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.0, "ram_available_mb": 112036.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 81.0}, "power_stats": {"power_watts_samples": [32.78, 32.78], "power_watts_avg": 32.78, "power_watts_peak": 32.78, "energy_joules_est": 4.55, "sample_count": 2, "duration_seconds": 0.139}, "timestamp": "2026-01-25T20:05:44.005335"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 160.612, "latencies_ms": [160.612], "images_per_second": 6.226, "prompt_tokens": 757, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n A green tent and a motorcycle are parked in a field, with trees visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [32.78, 30.68], "power_watts_avg": 31.73, "power_watts_peak": 32.78, "energy_joules_est": 5.12, "sample_count": 2, "duration_seconds": 0.161}, "timestamp": "2026-01-25T20:05:44.212716"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 112.431, "latencies_ms": [112.431], "images_per_second": 8.894, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n The motorcycle is parked in a field with tall grass and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10473.3, "ram_available_mb": 112033.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [30.68, 30.68], "power_watts_avg": 30.68, "power_watts_peak": 30.68, "energy_joules_est": 3.46, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T20:05:44.418303"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 276.464, "latencies_ms": [276.464], "images_per_second": 3.617, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA black steam locomotive numbered 5717 is parked at a train station, with people standing nearby and waiting for their ride.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10473.2, "ram_available_mb": 112033.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10475.9, "ram_available_mb": 112030.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [30.68, 30.68, 29.81], "power_watts_avg": 30.39, "power_watts_peak": 30.68, "energy_joules_est": 8.41, "sample_count": 3, "duration_seconds": 0.277}, "timestamp": "2026-01-25T20:05:44.732283"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 81.609, "latencies_ms": [81.609], "images_per_second": 12.254, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Train engine 57717", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.9, "ram_available_mb": 112030.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [29.81], "power_watts_avg": 29.81, "power_watts_peak": 29.81, "energy_joules_est": 2.44, "sample_count": 1, "duration_seconds": 0.082}, "timestamp": "2026-01-25T20:05:44.837749"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 121.504, "latencies_ms": [121.504], "images_per_second": 8.23, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Train station platform with people standing on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10480.1, "ram_available_mb": 112026.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [29.81, 29.81], "power_watts_avg": 29.81, "power_watts_peak": 29.81, "energy_joules_est": 3.63, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:05:45.043920"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 146.662, "latencies_ms": [146.662], "images_per_second": 6.818, "prompt_tokens": 757, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nA black steam locomotive numbered 5717 is parked on a train track, with people standing nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.1, "ram_available_mb": 112026.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [29.81, 32.4], "power_watts_avg": 31.1, "power_watts_peak": 32.4, "energy_joules_est": 4.57, "sample_count": 2, "duration_seconds": 0.147}, "timestamp": "2026-01-25T20:05:45.250434"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 50.695, "latencies_ms": [50.695], "images_per_second": 19.726, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10482.6, "ram_available_mb": 112023.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [32.4], "power_watts_avg": 32.4, "power_watts_peak": 32.4, "energy_joules_est": 1.65, "sample_count": 1, "duration_seconds": 0.051}, "timestamp": "2026-01-25T20:05:45.356520"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 791.592, "latencies_ms": [791.592], "images_per_second": 1.263, "prompt_tokens": 744, "response_tokens_est": 128, "n_tiles": 1, "output_text": "\u4e2d\u592e\u533a\u57ce\u5e02\u5927\u962a\u5e02\u5730\u533a\u57ce\u5e02\u5927\u962a\u5e02\u5730\u533a\u57ce\u5e02\u5927\u962a\u5e02\u5730\u533a\u57ce\u5e02\u5927\u962a\u5e02\u5730\u533a\u57ce\u5e02\u5927\u962a\u5e02\u5730\u533a\u57ce\u5e02\u5927\u962a\u5e02\u5730\u533a\u57ce\u5e02\u5927\u962a\u5e02\u5730\u533a\u57ce\u5e02\u5927\u962a\u5e02\u5730\u533a\u57ce", "error": null, "sys_before": {"cpu_percent": 3.8, "ram_used_mb": 10482.6, "ram_available_mb": 112023.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10474.4, "ram_available_mb": 112031.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [32.4, 32.4, 32.4, 32.55, 32.55, 32.55, 32.55, 32.55], "power_watts_avg": 32.49, "power_watts_peak": 32.55, "energy_joules_est": 25.73, "sample_count": 8, "duration_seconds": 0.792}, "timestamp": "2026-01-25T20:05:46.171612"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 230.226, "latencies_ms": [230.226], "images_per_second": 4.344, "prompt_tokens": 759, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\n 1. Building  2. Building  3. Building  4. Building  5. Building  6. Building  7. Building  8. Building", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.4, "ram_available_mb": 112031.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10468.4, "ram_available_mb": 112037.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [39.92, 39.92, 39.92], "power_watts_avg": 39.92, "power_watts_peak": 39.92, "energy_joules_est": 9.21, "sample_count": 3, "duration_seconds": 0.231}, "timestamp": "2026-01-25T20:05:46.478410"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 139.117, "latencies_ms": [139.117], "images_per_second": 7.188, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\u4e2d\u6587\u5b57\u6bcd\u5927\u5199\u975e\u5e38\u5927", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.4, "ram_available_mb": 112037.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10467.1, "ram_available_mb": 112039.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [39.92, 38.08], "power_watts_avg": 39.0, "power_watts_peak": 39.92, "energy_joules_est": 5.45, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T20:05:46.684374"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 202.239, "latencies_ms": [202.239], "images_per_second": 4.945, "prompt_tokens": 757, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA large building with many signs in Chinese characters, including one that says \"\u5927\u962a\u5e02\", indicating a bustling city environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [38.08, 38.08, 38.08], "power_watts_avg": 38.08, "power_watts_peak": 38.08, "energy_joules_est": 7.71, "sample_count": 3, "duration_seconds": 0.203}, "timestamp": "2026-01-25T20:05:46.991497"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 84.644, "latencies_ms": [84.644], "images_per_second": 11.814, "prompt_tokens": 756, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urns hanging from wires.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10468.4, "ram_available_mb": 112037.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [38.08], "power_watts_avg": 38.08, "power_watts_peak": 38.08, "energy_joules_est": 3.25, "sample_count": 1, "duration_seconds": 0.085}, "timestamp": "2026-01-25T20:05:47.097624"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 421.176, "latencies_ms": [421.176], "images_per_second": 2.374, "prompt_tokens": 744, "response_tokens_est": 55, "n_tiles": 1, "output_text": "\nA man is sitting on a concrete slab by a body of water, with his legs crossed and arms resting on his knees. A \"CLOSED TRACK\" sign is placed next to him, indicating that the trail may be closed for safety reasons or maintenance purposes.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10468.4, "ram_available_mb": 112037.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.01, 30.01, 30.01, 30.01, 30.01], "power_watts_avg": 30.01, "power_watts_peak": 30.01, "energy_joules_est": 12.65, "sample_count": 5, "duration_seconds": 0.422}, "timestamp": "2026-01-25T20:05:47.615204"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 101.452, "latencies_ms": [101.452], "images_per_second": 9.857, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Man sitting on a pole - 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10463.9, "ram_available_mb": 112042.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [35.4, 35.4], "power_watts_avg": 35.4, "power_watts_peak": 35.4, "energy_joules_est": 3.6, "sample_count": 2, "duration_seconds": 0.102}, "timestamp": "2026-01-25T20:05:47.821274"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 118.9, "latencies_ms": [118.9], "images_per_second": 8.41, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nA man is sitting on a pole by the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.9, "ram_available_mb": 112042.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [35.4, 35.4], "power_watts_avg": 35.4, "power_watts_peak": 35.4, "energy_joules_est": 4.23, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T20:05:48.027097"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 237.501, "latencies_ms": [237.501], "images_per_second": 4.211, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nIn a park, there is a man sitting on a pole next to a closed trail sign. The man appears to be enjoying his time outdoors despite the closure of the trail.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.0, "ram_available_mb": 112038.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10468.7, "ram_available_mb": 112037.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [35.4, 32.3, 32.3], "power_watts_avg": 33.33, "power_watts_peak": 35.4, "energy_joules_est": 7.93, "sample_count": 3, "duration_seconds": 0.238}, "timestamp": "2026-01-25T20:05:48.334656"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 117.325, "latencies_ms": [117.325], "images_per_second": 8.523, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe man is sitting on a pole by the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.7, "ram_available_mb": 112037.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10468.7, "ram_available_mb": 112037.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [32.3, 32.3], "power_watts_avg": 32.3, "power_watts_peak": 32.3, "energy_joules_est": 3.81, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:05:48.540725"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 244.07, "latencies_ms": [244.07], "images_per_second": 4.097, "prompt_tokens": 744, "response_tokens_est": 20, "n_tiles": 1, "output_text": "urn of a brown horse and a white dog in a field, with trees in the background.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10468.7, "ram_available_mb": 112037.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [32.3, 25.12, 25.12], "power_watts_avg": 27.51, "power_watts_peak": 32.3, "energy_joules_est": 6.72, "sample_count": 3, "duration_seconds": 0.244}, "timestamp": "2026-01-25T20:05:48.854536"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 92.34, "latencies_ms": [92.34], "images_per_second": 10.83, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Horse: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 31.0}, "power_stats": {"power_watts_samples": [25.12], "power_watts_avg": 25.12, "power_watts_peak": 25.12, "energy_joules_est": 2.33, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:05:48.960219"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 115.752, "latencies_ms": [115.752], "images_per_second": 8.639, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nA brown horse and a white dog are standing in a field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [25.12, 25.12], "power_watts_avg": 25.12, "power_watts_peak": 25.12, "energy_joules_est": 2.91, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T20:05:49.165180"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 137.131, "latencies_ms": [137.131], "images_per_second": 7.292, "prompt_tokens": 757, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nA brown horse stands next to a white dog in a grassy field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [33.59, 33.59], "power_watts_avg": 33.59, "power_watts_peak": 33.59, "energy_joules_est": 4.62, "sample_count": 2, "duration_seconds": 0.138}, "timestamp": "2026-01-25T20:05:49.370974"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 93.7, "latencies_ms": [93.7], "images_per_second": 10.672, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The horse is brown and white.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10468.5, "ram_available_mb": 112037.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [33.59], "power_watts_avg": 33.59, "power_watts_peak": 33.59, "energy_joules_est": 3.16, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T20:05:49.476159"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 285.868, "latencies_ms": [285.868], "images_per_second": 3.498, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA group of people are playing volleyball in a gymnasium, with two players on each side of the net preparing to hit the ball.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10468.5, "ram_available_mb": 112037.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [33.59, 32.93, 32.93], "power_watts_avg": 33.15, "power_watts_peak": 33.59, "energy_joules_est": 9.49, "sample_count": 3, "duration_seconds": 0.286}, "timestamp": "2026-01-25T20:05:49.789865"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 82.247, "latencies_ms": [82.247], "images_per_second": 12.158, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Volleyball net", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 47.0}, "power_stats": {"power_watts_samples": [32.93], "power_watts_avg": 32.93, "power_watts_peak": 32.93, "energy_joules_est": 2.72, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:05:49.895592"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 147.155, "latencies_ms": [147.155], "images_per_second": 6.796, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nThe man in a blue tank top is about to hit the ball with his racket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [32.93, 32.93], "power_watts_avg": 32.93, "power_watts_peak": 32.93, "energy_joules_est": 4.87, "sample_count": 2, "duration_seconds": 0.148}, "timestamp": "2026-01-25T20:05:50.100825"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 115.752, "latencies_ms": [115.752], "images_per_second": 8.639, "prompt_tokens": 757, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nA group of people are playing volleyball in a gymnasium.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10467.1, "ram_available_mb": 112039.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [33.02, 33.02], "power_watts_avg": 33.02, "power_watts_peak": 33.02, "energy_joules_est": 3.83, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T20:05:50.307588"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 411.776, "latencies_ms": [411.776], "images_per_second": 2.429, "prompt_tokens": 756, "response_tokens_est": 70, "n_tiles": 1, "output_text": "\n The image shows a group of people playing volleyball in an indoor gym. There are at least 12 players on the court, and they are all focused on the game. One player is holding a volleyball over his head as he prepares to serve it. The gym has blue walls that match the floor, creating a visually appealing environment for the sport.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.1, "ram_available_mb": 112039.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [33.02, 33.02, 33.02, 34.7, 34.7], "power_watts_avg": 33.69, "power_watts_peak": 34.7, "energy_joules_est": 13.88, "sample_count": 5, "duration_seconds": 0.412}, "timestamp": "2026-01-25T20:05:50.816776"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 524.605, "latencies_ms": [524.605], "images_per_second": 1.906, "prompt_tokens": 744, "response_tokens_est": 75, "n_tiles": 1, "output_text": "\nIn a grassy field, there are several zebras and wildebeests grazing peacefully. The zebras are scattered throughout the scene, with some standing closer to the foreground and others further back in the background. The wildebeests can be seen near the center of the image, while the zebras are more towards the right side.\n", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [34.7, 34.7, 34.7, 35.48, 35.48, 35.48], "power_watts_avg": 35.09, "power_watts_peak": 35.48, "energy_joules_est": 18.43, "sample_count": 6, "duration_seconds": 0.525}, "timestamp": "2026-01-25T20:05:51.431535"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 85.391, "latencies_ms": [85.391], "images_per_second": 11.711, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Zebra: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10471.8, "ram_available_mb": 112034.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [35.48], "power_watts_avg": 35.48, "power_watts_peak": 35.48, "energy_joules_est": 3.06, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T20:05:51.537061"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 135.655, "latencies_ms": [135.655], "images_per_second": 7.372, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n 1. Zebra in front of a group of wildebeests.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.8, "ram_available_mb": 112034.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10472.0, "ram_available_mb": 112034.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [35.48, 37.0], "power_watts_avg": 36.24, "power_watts_peak": 37.0, "energy_joules_est": 4.92, "sample_count": 2, "duration_seconds": 0.136}, "timestamp": "2026-01-25T20:05:51.740994"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 255.609, "latencies_ms": [255.609], "images_per_second": 3.912, "prompt_tokens": 757, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\n A group of zebras are grazing in a field, with several wildebeests nearby. The animals appear to be peacefully coexisting as they graze on the grassy plain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.0, "ram_available_mb": 112034.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10472.5, "ram_available_mb": 112033.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [37.0, 37.0, 37.0], "power_watts_avg": 37.0, "power_watts_peak": 37.0, "energy_joules_est": 9.47, "sample_count": 3, "duration_seconds": 0.256}, "timestamp": "2026-01-25T20:05:52.047812"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 275.786, "latencies_ms": [275.786], "images_per_second": 3.626, "prompt_tokens": 756, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\n A herd of zebras and a group of wildebeests are grazing in the grassy field. The sky is clear blue with some clouds, providing good visibility for the animals to graze peacefully.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.5, "ram_available_mb": 112033.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [37.0, 32.77, 32.77], "power_watts_avg": 34.18, "power_watts_peak": 37.0, "energy_joules_est": 9.44, "sample_count": 3, "duration_seconds": 0.276}, "timestamp": "2026-01-25T20:05:52.354738"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 261.513, "latencies_ms": [261.513], "images_per_second": 3.824, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "\nA ginger and white cat sits on a wooden deck, gazing at its reflection in a mirror with its eyes wide open.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [32.77, 32.77, 32.77], "power_watts_avg": 32.77, "power_watts_peak": 32.77, "energy_joules_est": 8.59, "sample_count": 3, "duration_seconds": 0.262}, "timestamp": "2026-01-25T20:05:52.663648"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.448, "latencies_ms": [78.448], "images_per_second": 12.747, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Cat: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [37.45], "power_watts_avg": 37.45, "power_watts_peak": 37.45, "energy_joules_est": 2.95, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T20:05:52.770083"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 126.496, "latencies_ms": [126.496], "images_per_second": 7.905, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nA cat with white and orange fur is sitting on a wooden deck.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10462.7, "ram_available_mb": 112043.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [37.45, 37.45], "power_watts_avg": 37.45, "power_watts_peak": 37.45, "energy_joules_est": 4.76, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:05:52.974894"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 156.624, "latencies_ms": [156.624], "images_per_second": 6.385, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA cat with orange and white fur sits on a wooden deck, looking at its reflection in a mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.7, "ram_available_mb": 112043.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10462.7, "ram_available_mb": 112043.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [37.45, 37.45], "power_watts_avg": 37.45, "power_watts_peak": 37.45, "energy_joules_est": 5.89, "sample_count": 2, "duration_seconds": 0.157}, "timestamp": "2026-01-25T20:05:53.180897"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 97.142, "latencies_ms": [97.142], "images_per_second": 10.294, "prompt_tokens": 756, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n The cat is sitting on a wooden deck.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.7, "ram_available_mb": 112043.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10462.7, "ram_available_mb": 112043.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [35.58], "power_watts_avg": 35.58, "power_watts_peak": 35.58, "energy_joules_est": 3.48, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T20:05:53.286295"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 265.103, "latencies_ms": [265.103], "images_per_second": 3.772, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\n A dock with several boats is visible, including a white boat and a red boat docked at the pier.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10462.7, "ram_available_mb": 112043.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10464.4, "ram_available_mb": 112041.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [35.58, 35.58, 35.58], "power_watts_avg": 35.58, "power_watts_peak": 35.58, "energy_joules_est": 9.45, "sample_count": 3, "duration_seconds": 0.266}, "timestamp": "2026-01-25T20:05:53.599955"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 85.697, "latencies_ms": [85.697], "images_per_second": 11.669, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Boat: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.4, "ram_available_mb": 112041.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [34.1], "power_watts_avg": 34.1, "power_watts_peak": 34.1, "energy_joules_est": 2.93, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T20:05:53.704186"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 107.091, "latencies_ms": [107.091], "images_per_second": 9.338, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Boat on right side of photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [34.1, 34.1], "power_watts_avg": 34.1, "power_watts_peak": 34.1, "energy_joules_est": 3.66, "sample_count": 2, "duration_seconds": 0.107}, "timestamp": "2026-01-25T20:05:53.910303"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 268.923, "latencies_ms": [268.923], "images_per_second": 3.719, "prompt_tokens": 757, "response_tokens_est": 43, "n_tiles": 1, "output_text": "\nA row of boats, including a pontoons and motorboats, are docked at a pier in front of a large building. The sky above them appears hazy, suggesting an overcast day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10451.5, "ram_available_mb": 112054.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10455.4, "ram_available_mb": 112050.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [34.1, 34.1, 33.14], "power_watts_avg": 33.78, "power_watts_peak": 34.1, "energy_joules_est": 9.11, "sample_count": 3, "duration_seconds": 0.27}, "timestamp": "2026-01-25T20:05:54.215693"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 184.446, "latencies_ms": [184.446], "images_per_second": 5.422, "prompt_tokens": 756, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\n A large white boat is docked next to a smaller gray boat. The sky above them appears hazy and overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.4, "ram_available_mb": 112050.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10455.4, "ram_available_mb": 112050.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [33.14, 33.14], "power_watts_avg": 33.14, "power_watts_peak": 33.14, "energy_joules_est": 6.12, "sample_count": 2, "duration_seconds": 0.185}, "timestamp": "2026-01-25T20:05:54.420986"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 255.11, "latencies_ms": [255.11], "images_per_second": 3.92, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA man stands on a street, wearing only his underwear and holding a bicycle with an umbrella attached to it.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 10455.4, "ram_available_mb": 112050.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10458.3, "ram_available_mb": 112048.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [33.14, 33.14, 33.66], "power_watts_avg": 33.31, "power_watts_peak": 33.66, "energy_joules_est": 8.5, "sample_count": 3, "duration_seconds": 0.255}, "timestamp": "2026-01-25T20:05:54.727843"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 75.688, "latencies_ms": [75.688], "images_per_second": 13.212, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Bicycle", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.3, "ram_available_mb": 112048.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10457.0, "ram_available_mb": 112049.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [33.66], "power_watts_avg": 33.66, "power_watts_peak": 33.66, "energy_joules_est": 2.56, "sample_count": 1, "duration_seconds": 0.076}, "timestamp": "2026-01-25T20:05:54.832440"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 113.064, "latencies_ms": [113.064], "images_per_second": 8.845, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe man is standing next to a bicycle on the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.0, "ram_available_mb": 112049.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10457.0, "ram_available_mb": 112049.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [33.66, 33.66], "power_watts_avg": 33.66, "power_watts_peak": 33.66, "energy_joules_est": 3.82, "sample_count": 2, "duration_seconds": 0.113}, "timestamp": "2026-01-25T20:05:55.038699"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 331.698, "latencies_ms": [331.698], "images_per_second": 3.015, "prompt_tokens": 757, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\nA man stands on a street, wearing only his underwear. He holds a bicycle with one hand while wearing a towel around his waist with the other. The street has signs in Chinese characters that read \"\u5927\u962a\u5e02\", indicating this scene takes place in China.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.0, "ram_available_mb": 112049.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10458.3, "ram_available_mb": 112048.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [33.66, 34.11, 34.11, 34.11], "power_watts_avg": 34.0, "power_watts_peak": 34.11, "energy_joules_est": 11.29, "sample_count": 4, "duration_seconds": 0.332}, "timestamp": "2026-01-25T20:05:55.444721"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 123.807, "latencies_ms": [123.807], "images_per_second": 8.077, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The man is wearing a white towel around his waist.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.3, "ram_available_mb": 112048.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10455.3, "ram_available_mb": 112051.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [34.11, 34.11], "power_watts_avg": 34.11, "power_watts_peak": 34.11, "energy_joules_est": 4.23, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T20:05:55.650549"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 279.87, "latencies_ms": [279.87], "images_per_second": 3.573, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\n A store front displays a large number of bananas hanging from strings, with some bunches overlapping each other and others standing alone.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 10455.3, "ram_available_mb": 112051.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [31.41, 31.41, 31.41], "power_watts_avg": 31.41, "power_watts_peak": 31.41, "energy_joules_est": 8.81, "sample_count": 3, "duration_seconds": 0.28}, "timestamp": "2026-01-25T20:05:55.960315"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 83.124, "latencies_ms": [83.124], "images_per_second": 12.03, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Bananas", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.9, "ram_available_mb": 112046.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 37.0}, "power_stats": {"power_watts_samples": [31.41], "power_watts_avg": 31.41, "power_watts_peak": 31.41, "energy_joules_est": 2.62, "sample_count": 1, "duration_seconds": 0.083}, "timestamp": "2026-01-25T20:05:56.064756"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 98.784, "latencies_ms": [98.784], "images_per_second": 10.123, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n A bunch of bananas hanging from a rope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [31.41], "power_watts_avg": 31.41, "power_watts_peak": 31.41, "energy_joules_est": 3.11, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T20:05:56.168422"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 215.969, "latencies_ms": [215.969], "images_per_second": 4.63, "prompt_tokens": 757, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\n A market stall displays a large number of bananas hanging from strings. The bananas are yellow in color, indicating they are ripe and ready to be eaten.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10457.1, "ram_available_mb": 112049.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [35.02, 35.02, 35.02], "power_watts_avg": 35.02, "power_watts_peak": 35.02, "energy_joules_est": 7.57, "sample_count": 3, "duration_seconds": 0.216}, "timestamp": "2026-01-25T20:05:56.473726"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 464.868, "latencies_ms": [464.868], "images_per_second": 2.151, "prompt_tokens": 756, "response_tokens_est": 82, "n_tiles": 1, "output_text": "\n The image shows a bunch of bananas hanging from the ceiling in front of a store. The bananas are yellow and ripe, indicating that they have been recently harvested or are at their peak ripeness. They are arranged in bunches on strings, creating an eye-catching display for potential customers to see. The store appears to be located outdoors, as suggested by the presence of a window and a door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.1, "ram_available_mb": 112049.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10461.0, "ram_available_mb": 112045.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [35.02, 35.02, 33.02, 33.02, 33.02], "power_watts_avg": 33.82, "power_watts_peak": 35.02, "energy_joules_est": 15.73, "sample_count": 5, "duration_seconds": 0.465}, "timestamp": "2026-01-25T20:05:56.980017"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 325.622, "latencies_ms": [325.622], "images_per_second": 3.071, "prompt_tokens": 744, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA green and white train is traveling down a track, with three red carts attached to it. The train is moving through an open field surrounded by mountains in the distance.", "error": null, "sys_before": {"cpu_percent": 11.8, "ram_used_mb": 10461.0, "ram_available_mb": 112045.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [33.02, 35.41, 35.41, 35.41], "power_watts_avg": 34.81, "power_watts_peak": 35.41, "energy_joules_est": 11.36, "sample_count": 4, "duration_seconds": 0.326}, "timestamp": "2026-01-25T20:05:57.390687"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 155.704, "latencies_ms": [155.704], "images_per_second": 6.422, "prompt_tokens": 759, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n 1. Train car #1  2. Train car #2  3. Train car #3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.4, "ram_available_mb": 112044.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [35.41, 35.41], "power_watts_avg": 35.41, "power_watts_peak": 35.41, "energy_joules_est": 5.52, "sample_count": 2, "duration_seconds": 0.156}, "timestamp": "2026-01-25T20:05:57.596590"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 126.663, "latencies_ms": [126.663], "images_per_second": 7.895, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Train car on left side of train track.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10461.0, "ram_available_mb": 112045.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [39.66, 39.66], "power_watts_avg": 39.66, "power_watts_peak": 39.66, "energy_joules_est": 5.04, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:05:57.801661"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 242.737, "latencies_ms": [242.737], "images_per_second": 4.12, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA green and white train travels down a track through a rural countryside, with mountains in the background. The train has three cars attached to it as it moves along its route.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.0, "ram_available_mb": 112045.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [39.66, 39.66, 39.66], "power_watts_avg": 39.66, "power_watts_peak": 39.66, "energy_joules_est": 9.65, "sample_count": 3, "duration_seconds": 0.243}, "timestamp": "2026-01-25T20:05:58.107547"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 95.437, "latencies_ms": [95.437], "images_per_second": 10.478, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The train is green and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.7, "ram_available_mb": 112044.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10463.0, "ram_available_mb": 112043.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.88], "power_watts_avg": 34.88, "power_watts_peak": 34.88, "energy_joules_est": 3.35, "sample_count": 1, "duration_seconds": 0.096}, "timestamp": "2026-01-25T20:05:58.212462"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 294.288, "latencies_ms": [294.288], "images_per_second": 3.398, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA man stands on a sandy beach, holding his arm up in the air and pointing towards the ocean with a cell phone in hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.0, "ram_available_mb": 112043.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.88, 34.88, 34.88], "power_watts_avg": 34.88, "power_watts_peak": 34.88, "energy_joules_est": 10.29, "sample_count": 3, "duration_seconds": 0.295}, "timestamp": "2026-01-25T20:05:58.519276"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 74.717, "latencies_ms": [74.717], "images_per_second": 13.384, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Chair - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.88], "power_watts_avg": 34.88, "power_watts_peak": 34.88, "energy_joules_est": 2.62, "sample_count": 1, "duration_seconds": 0.075}, "timestamp": "2026-01-25T20:05:58.624498"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 171.504, "latencies_ms": [171.504], "images_per_second": 5.831, "prompt_tokens": 763, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA man standing on a beach holding his arm up and taking a picture of an umbrella in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.6, 33.6], "power_watts_avg": 33.6, "power_watts_peak": 33.6, "energy_joules_est": 5.77, "sample_count": 2, "duration_seconds": 0.172}, "timestamp": "2026-01-25T20:05:58.830464"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 222.976, "latencies_ms": [222.976], "images_per_second": 4.485, "prompt_tokens": 757, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\nA man in a black shirt stands on a sandy beach, holding his arm up. A green chair is nearby, and an umbrella can be seen further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.6, 33.6, 33.6], "power_watts_avg": 33.6, "power_watts_peak": 33.6, "energy_joules_est": 7.5, "sample_count": 3, "duration_seconds": 0.223}, "timestamp": "2026-01-25T20:05:59.136090"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 144.92, "latencies_ms": [144.92], "images_per_second": 6.9, "prompt_tokens": 756, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n The man is standing on a beach with his arm raised and holding a cell phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.0, "ram_available_mb": 112039.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10463.8, "ram_available_mb": 112042.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.51, 35.51], "power_watts_avg": 35.51, "power_watts_peak": 35.51, "energy_joules_est": 5.17, "sample_count": 2, "duration_seconds": 0.146}, "timestamp": "2026-01-25T20:05:59.341621"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 257.214, "latencies_ms": [257.214], "images_per_second": 3.888, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urns of broccoli plants sit on a dirt ground, with some leaves and flowers visible in the foreground.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10463.8, "ram_available_mb": 112042.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.51, 35.51, 35.51], "power_watts_avg": 35.51, "power_watts_peak": 35.51, "energy_joules_est": 9.15, "sample_count": 3, "duration_seconds": 0.258}, "timestamp": "2026-01-25T20:05:59.656832"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 175.536, "latencies_ms": [175.536], "images_per_second": 5.697, "prompt_tokens": 759, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\n 1. Broccoli plant in red pot (0.39, 0.24, 0.63, 0.43)", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 10463.1, "ram_available_mb": 112043.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.23, 34.23], "power_watts_avg": 34.23, "power_watts_peak": 34.23, "energy_joules_est": 6.02, "sample_count": 2, "duration_seconds": 0.176}, "timestamp": "2026-01-25T20:05:59.862799"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 100.215, "latencies_ms": [100.215], "images_per_second": 9.979, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe broccoli plant is in a pot on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 10456.8, "ram_available_mb": 112049.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.23], "power_watts_avg": 34.23, "power_watts_peak": 34.23, "energy_joules_est": 3.45, "sample_count": 1, "duration_seconds": 0.101}, "timestamp": "2026-01-25T20:05:59.967918"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 254.299, "latencies_ms": [254.299], "images_per_second": 3.932, "prompt_tokens": 757, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\n Two large potted plants filled with green leaves are sitting on a dirt ground. One plant has a small broccoli growing in it, while the other one has a larger broccoli plant nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.8, "ram_available_mb": 112049.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10457.2, "ram_available_mb": 112049.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [34.23, 34.23, 33.45], "power_watts_avg": 33.97, "power_watts_peak": 34.23, "energy_joules_est": 8.65, "sample_count": 3, "duration_seconds": 0.255}, "timestamp": "2026-01-25T20:06:00.273215"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 319.878, "latencies_ms": [319.878], "images_per_second": 3.126, "prompt_tokens": 756, "response_tokens_est": 54, "n_tiles": 1, "output_text": "\n The image shows a garden with two large potted plants. One plant has dark green leaves and the other is covered in light green leaves. Both plants have some yellow flowers on them. The background of the image features a dirt area where the plants are growing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.2, "ram_available_mb": 112049.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [33.45, 33.45, 33.45, 33.45], "power_watts_avg": 33.45, "power_watts_peak": 33.45, "energy_joules_est": 10.71, "sample_count": 4, "duration_seconds": 0.32}, "timestamp": "2026-01-25T20:06:00.679278"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 278.767, "latencies_ms": [278.767], "images_per_second": 3.587, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "urn of flowers sits on a windowsill, and a man in blue shirt holds a horse's reins while walking it down the street.\n", "error": null, "sys_before": {"cpu_percent": 4.0, "ram_used_mb": 10452.6, "ram_available_mb": 112053.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10454.8, "ram_available_mb": 112051.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 52.0}, "power_stats": {"power_watts_samples": [34.94, 34.94, 34.94], "power_watts_avg": 34.94, "power_watts_peak": 34.94, "energy_joules_est": 9.75, "sample_count": 3, "duration_seconds": 0.279}, "timestamp": "2026-01-25T20:06:00.993600"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 701.7, "latencies_ms": [701.7], "images_per_second": 1.425, "prompt_tokens": 759, "response_tokens_est": 128, "n_tiles": 1, "output_text": "\n 1. House  2. Window 3. Pony tail 4. Pony tail 5. Pony tail 6. Pony tail 7. Pony tail 8. Pony tail 9. Pony tail 10. Pony tail 11. Pony tail 12. Pony tail 13. Pony tail 14. Pony tail 15. Pony tail 16. Pony tail 17. Pony tail 18. Pony tail 19. Pony tail 20. Pony tail 21. Pony tail 22. Pony tail 23. Pony tail 24. Pony tail 25. Pony tail 26. Pony tail 27. Pony tail 28. Pony tail 29. Pony tail 30. Pony tail 31. Pony tail 32. Pony tail", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10454.8, "ram_available_mb": 112051.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [34.94, 37.8, 37.8, 37.8, 37.8, 37.8, 40.67, 40.67], "power_watts_avg": 38.16, "power_watts_peak": 40.67, "energy_joules_est": 26.79, "sample_count": 8, "duration_seconds": 0.702}, "timestamp": "2026-01-25T20:06:01.801753"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 152.837, "latencies_ms": [152.837], "images_per_second": 6.543, "prompt_tokens": 763, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA man is holding a blue rope and walking with a small boy on top of a pony.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.1, "ram_available_mb": 112050.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [40.67, 40.67], "power_watts_avg": 40.67, "power_watts_peak": 40.67, "energy_joules_est": 6.23, "sample_count": 2, "duration_seconds": 0.153}, "timestamp": "2026-01-25T20:06:02.007572"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 222.486, "latencies_ms": [222.486], "images_per_second": 4.495, "prompt_tokens": 757, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA man in a blue shirt is walking with his young son on a pony, holding its reins. They are passing by a red house with white trim.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10460.2, "ram_available_mb": 112046.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10462.2, "ram_available_mb": 112044.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 45.0}, "power_stats": {"power_watts_samples": [40.67, 36.41, 36.41], "power_watts_avg": 37.83, "power_watts_peak": 40.67, "energy_joules_est": 8.45, "sample_count": 3, "duration_seconds": 0.223}, "timestamp": "2026-01-25T20:06:02.314025"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 311.706, "latencies_ms": [311.706], "images_per_second": 3.208, "prompt_tokens": 756, "response_tokens_est": 51, "n_tiles": 1, "output_text": "\n The image shows a man and a boy riding on the back of a small brown pony. They are walking down a street with red brick buildings in the background. The man is holding the horse's reins while the boy sits comfortably on its back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.2, "ram_available_mb": 112044.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 45.0}, "power_stats": {"power_watts_samples": [36.41, 36.41, 36.41, 32.26], "power_watts_avg": 35.38, "power_watts_peak": 36.41, "energy_joules_est": 11.04, "sample_count": 4, "duration_seconds": 0.312}, "timestamp": "2026-01-25T20:06:02.719201"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 248.086, "latencies_ms": [248.086], "images_per_second": 4.031, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urn of flowers in a field, with a young boy standing next to it and holding a stuffed animal.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10464.2, "ram_available_mb": 112042.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 45.0}, "power_stats": {"power_watts_samples": [32.26, 32.26, 32.26], "power_watts_avg": 32.26, "power_watts_peak": 32.26, "energy_joules_est": 8.01, "sample_count": 3, "duration_seconds": 0.248}, "timestamp": "2026-01-25T20:06:03.028629"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 304.166, "latencies_ms": [304.166], "images_per_second": 3.288, "prompt_tokens": 759, "response_tokens_est": 52, "n_tiles": 1, "output_text": "\n 1. Bluebells 2. Bluebells 3. Bluebells 4. Bluebells 5. Bluebells 6. Bluebells 7. Bluebells 8. Bluebells 9. Bluebells 10. Bluebells", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10457.9, "ram_available_mb": 112048.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10458.1, "ram_available_mb": 112048.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [32.26, 35.56, 35.56, 35.56], "power_watts_avg": 34.74, "power_watts_peak": 35.56, "energy_joules_est": 10.59, "sample_count": 4, "duration_seconds": 0.305}, "timestamp": "2026-01-25T20:06:03.435291"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 151.121, "latencies_ms": [151.121], "images_per_second": 6.617, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nA young boy is standing in a field of blue flowers and holding a stuffed animal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.1, "ram_available_mb": 112048.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10454.8, "ram_available_mb": 112051.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [35.56, 35.56], "power_watts_avg": 35.56, "power_watts_peak": 35.56, "energy_joules_est": 5.4, "sample_count": 2, "duration_seconds": 0.152}, "timestamp": "2026-01-25T20:06:03.643177"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 251.935, "latencies_ms": [251.935], "images_per_second": 3.969, "prompt_tokens": 757, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\nA young boy in a striped shirt stands on a dirt path, holding a brown stuffed animal. The path is surrounded by blue flowers, creating a picturesque scene of nature's beauty.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.8, "ram_available_mb": 112051.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [35.27, 35.27, 35.27], "power_watts_avg": 35.27, "power_watts_peak": 35.27, "energy_joules_est": 8.89, "sample_count": 3, "duration_seconds": 0.252}, "timestamp": "2026-01-25T20:06:03.950126"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 330.158, "latencies_ms": [330.158], "images_per_second": 3.029, "prompt_tokens": 756, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\n The image shows a young boy standing in front of a field with blue flowers. He is wearing a striped shirt and jeans while holding a brown teddy bear. The scene has a warm and inviting atmosphere due to the presence of the colorful flowers and the child's attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10456.9, "ram_available_mb": 112049.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10459.0, "ram_available_mb": 112047.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [35.27, 35.27, 32.97, 32.97], "power_watts_avg": 34.12, "power_watts_peak": 35.27, "energy_joules_est": 11.27, "sample_count": 4, "duration_seconds": 0.33}, "timestamp": "2026-01-25T20:06:04.357475"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 242.551, "latencies_ms": [242.551], "images_per_second": 4.123, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA large, brown and orange fruit is sitting on a black asphalt road in an urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.0, "ram_available_mb": 112047.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [32.97, 32.97, 32.97], "power_watts_avg": 32.97, "power_watts_peak": 32.97, "energy_joules_est": 8.01, "sample_count": 3, "duration_seconds": 0.243}, "timestamp": "2026-01-25T20:06:04.666022"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 70.766, "latencies_ms": [70.766], "images_per_second": 14.131, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Orange - 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.5, "ram_available_mb": 112045.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [37.03], "power_watts_avg": 37.03, "power_watts_peak": 37.03, "energy_joules_est": 2.63, "sample_count": 1, "duration_seconds": 0.071}, "timestamp": "2026-01-25T20:06:04.770964"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 149.408, "latencies_ms": [149.408], "images_per_second": 6.693, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nThe orange is on the ground in front of a parking lot with cars parked behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.2, "ram_available_mb": 112045.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 15.0}, "power_stats": {"power_watts_samples": [37.03, 37.03], "power_watts_avg": 37.03, "power_watts_peak": 37.03, "energy_joules_est": 5.54, "sample_count": 2, "duration_seconds": 0.15}, "timestamp": "2026-01-25T20:06:04.976768"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 237.059, "latencies_ms": [237.059], "images_per_second": 4.218, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nIn a parking lot, an orange sits on the ground next to a white line. The orange appears to be rotting or decaying, indicating that it has been discarded by someone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.9, "ram_available_mb": 112041.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10464.6, "ram_available_mb": 112041.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 14.0}, "power_stats": {"power_watts_samples": [37.03, 37.03, 35.01], "power_watts_avg": 36.36, "power_watts_peak": 37.03, "energy_joules_est": 8.63, "sample_count": 3, "duration_seconds": 0.237}, "timestamp": "2026-01-25T20:06:05.283006"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 308.339, "latencies_ms": [308.339], "images_per_second": 3.243, "prompt_tokens": 756, "response_tokens_est": 52, "n_tiles": 1, "output_text": "\n The image features a large orange sitting on the side of a road. The orange is positioned in front of a parking lot filled with cars and other vehicles. The scene also includes trees in the background, adding to the overall atmosphere of an urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.6, "ram_available_mb": 112041.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 14.0}, "power_stats": {"power_watts_samples": [35.01, 35.01, 35.01, 32.3], "power_watts_avg": 34.34, "power_watts_peak": 35.01, "energy_joules_est": 10.6, "sample_count": 4, "duration_seconds": 0.309}, "timestamp": "2026-01-25T20:06:05.690251"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 297.054, "latencies_ms": [297.054], "images_per_second": 3.366, "prompt_tokens": 744, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA man in a suit and tie is seated at a table with two beer bottles, one of which has a label that reads \"HOTTO\".", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 10466.2, "ram_available_mb": 112040.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10463.0, "ram_available_mb": 112043.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 14.0}, "power_stats": {"power_watts_samples": [32.3, 32.3, 32.3], "power_watts_avg": 32.3, "power_watts_peak": 32.3, "energy_joules_est": 9.61, "sample_count": 3, "duration_seconds": 0.298}, "timestamp": "2026-01-25T20:06:05.999227"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 252.183, "latencies_ms": [252.183], "images_per_second": 3.965, "prompt_tokens": 759, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\n 1. Beer bottle  2. Beer bottle  3. Beer bottle  4. Beer bottle  5. Beer bottle  6. Beer bottle  7. Beer bottle  8. Beer bottle", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.0, "ram_available_mb": 112043.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 78.0}, "power_stats": {"power_watts_samples": [32.3, 37.75, 37.75], "power_watts_avg": 35.94, "power_watts_peak": 37.75, "energy_joules_est": 9.07, "sample_count": 3, "duration_seconds": 0.252}, "timestamp": "2026-01-25T20:06:06.304955"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 195.183, "latencies_ms": [195.183], "images_per_second": 5.123, "prompt_tokens": 763, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\n 1. A man in a suit sitting at a table with two beer bottles on it and a bowl of food next to him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 78.0}, "power_stats": {"power_watts_samples": [37.75, 37.75], "power_watts_avg": 37.75, "power_watts_peak": 37.75, "energy_joules_est": 7.39, "sample_count": 2, "duration_seconds": 0.196}, "timestamp": "2026-01-25T20:06:06.510188"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 211.334, "latencies_ms": [211.334], "images_per_second": 4.732, "prompt_tokens": 757, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA man in a suit is sitting at a table with two beer bottles, one of which has a label that reads \"HOTTO\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 78.0}, "power_stats": {"power_watts_samples": [37.75, 36.34, 36.34], "power_watts_avg": 36.81, "power_watts_peak": 37.75, "energy_joules_est": 7.79, "sample_count": 3, "duration_seconds": 0.211}, "timestamp": "2026-01-25T20:06:06.815144"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 92.23, "latencies_ms": [92.23], "images_per_second": 10.843, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "urns of beer on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.1, "ram_available_mb": 112041.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 78.0}, "power_stats": {"power_watts_samples": [36.34], "power_watts_avg": 36.34, "power_watts_peak": 36.34, "energy_joules_est": 3.37, "sample_count": 1, "duration_seconds": 0.093}, "timestamp": "2026-01-25T20:06:06.920735"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 241.604, "latencies_ms": [241.604], "images_per_second": 4.139, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urns of water sit on a bed in a hotel room, with two pillows and white sheets.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.1, "ram_available_mb": 112042.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [36.34, 36.34, 36.06], "power_watts_avg": 36.25, "power_watts_peak": 36.34, "energy_joules_est": 8.77, "sample_count": 3, "duration_seconds": 0.242}, "timestamp": "2026-01-25T20:06:07.229990"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 99.429, "latencies_ms": [99.429], "images_per_second": 10.057, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Bed in the corner of a room", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [36.06], "power_watts_avg": 36.06, "power_watts_peak": 36.06, "energy_joules_est": 3.6, "sample_count": 1, "duration_seconds": 0.1}, "timestamp": "2026-01-25T20:06:07.333902"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 110.028, "latencies_ms": [110.028], "images_per_second": 9.089, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n 1. Bed with white sheets and pillows on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.0, "ram_available_mb": 112040.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10467.9, "ram_available_mb": 112038.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [36.06, 36.06], "power_watts_avg": 36.06, "power_watts_peak": 36.06, "energy_joules_est": 3.98, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T20:06:07.539433"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 143.282, "latencies_ms": [143.282], "images_per_second": 6.979, "prompt_tokens": 757, "response_tokens_est": 17, "n_tiles": 1, "output_text": "urns of white bedding on a twin size bed in a hotel room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.9, "ram_available_mb": 112038.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10468.7, "ram_available_mb": 112037.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [36.06, 32.67], "power_watts_avg": 34.36, "power_watts_peak": 36.06, "energy_joules_est": 4.94, "sample_count": 2, "duration_seconds": 0.144}, "timestamp": "2026-01-25T20:06:07.744610"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 94.016, "latencies_ms": [94.016], "images_per_second": 10.637, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urns of white and beige colors.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10468.7, "ram_available_mb": 112037.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 43.0}, "power_stats": {"power_watts_samples": [32.67], "power_watts_avg": 32.67, "power_watts_peak": 32.67, "energy_joules_est": 3.08, "sample_count": 1, "duration_seconds": 0.094}, "timestamp": "2026-01-25T20:06:07.848814"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 260.918, "latencies_ms": [260.918], "images_per_second": 3.833, "prompt_tokens": 744, "response_tokens_est": 26, "n_tiles": 1, "output_text": "urns of gold and silver are displayed on a table, with two teddy bears wearing Santa hats in front of them.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 10468.9, "ram_available_mb": 112037.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10468.8, "ram_available_mb": 112037.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [32.67, 32.67, 32.67], "power_watts_avg": 32.67, "power_watts_peak": 32.67, "energy_joules_est": 8.53, "sample_count": 3, "duration_seconds": 0.261}, "timestamp": "2026-01-25T20:06:08.157634"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 80.219, "latencies_ms": [80.219], "images_per_second": 12.466, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Bear", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.8, "ram_available_mb": 112037.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [33.84], "power_watts_avg": 33.84, "power_watts_peak": 33.84, "energy_joules_est": 2.72, "sample_count": 1, "duration_seconds": 0.081}, "timestamp": "2026-01-25T20:06:08.262657"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 214.026, "latencies_ms": [214.026], "images_per_second": 4.672, "prompt_tokens": 763, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\n 1. A brown teddy bear wearing a green hat and scarf with a red bow tie sitting next to two other stuffed animals on a blue blanket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10457.3, "ram_available_mb": 112049.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [33.84, 33.84, 33.84], "power_watts_avg": 33.84, "power_watts_peak": 33.84, "energy_joules_est": 7.26, "sample_count": 3, "duration_seconds": 0.214}, "timestamp": "2026-01-25T20:06:08.568814"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 149.215, "latencies_ms": [149.215], "images_per_second": 6.702, "prompt_tokens": 757, "response_tokens_est": 18, "n_tiles": 1, "output_text": "urns of water on a table, with two teddy bears wearing Santa hats.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.3, "ram_available_mb": 112049.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10457.6, "ram_available_mb": 112048.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [33.84, 33.3], "power_watts_avg": 33.57, "power_watts_peak": 33.84, "energy_joules_est": 5.03, "sample_count": 2, "duration_seconds": 0.15}, "timestamp": "2026-01-25T20:06:08.776098"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 114.886, "latencies_ms": [114.886], "images_per_second": 8.704, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n A stuffed animal bear wearing a green hat and scarf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10457.6, "ram_available_mb": 112048.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10458.9, "ram_available_mb": 112047.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 89.0}, "power_stats": {"power_watts_samples": [33.3, 33.3], "power_watts_avg": 33.3, "power_watts_peak": 33.3, "energy_joules_est": 3.85, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T20:06:08.982443"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 302.864, "latencies_ms": [302.864], "images_per_second": 3.302, "prompt_tokens": 744, "response_tokens_est": 32, "n_tiles": 1, "output_text": "\nA glass bowl filled with approximately 12 oranges, some of which are partially peeled and others still whole, sits on a table covered in a metallic fabric.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.9, "ram_available_mb": 112047.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10455.9, "ram_available_mb": 112050.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [33.3, 28.46, 28.46, 28.46], "power_watts_avg": 29.67, "power_watts_peak": 33.3, "energy_joules_est": 9.0, "sample_count": 4, "duration_seconds": 0.303}, "timestamp": "2026-01-25T20:06:09.394424"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 72.051, "latencies_ms": [72.051], "images_per_second": 13.879, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Orange", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.9, "ram_available_mb": 112050.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10452.8, "ram_available_mb": 112053.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [28.46], "power_watts_avg": 28.46, "power_watts_peak": 28.46, "energy_joules_est": 2.06, "sample_count": 1, "duration_seconds": 0.072}, "timestamp": "2026-01-25T20:06:09.500230"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 103.946, "latencies_ms": [103.946], "images_per_second": 9.62, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nThe bowl of oranges is placed on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10452.8, "ram_available_mb": 112053.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [28.46, 30.96], "power_watts_avg": 29.71, "power_watts_peak": 30.96, "energy_joules_est": 3.1, "sample_count": 2, "duration_seconds": 0.104}, "timestamp": "2026-01-25T20:06:09.706081"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 256.982, "latencies_ms": [256.982], "images_per_second": 3.891, "prompt_tokens": 757, "response_tokens_est": 40, "n_tiles": 1, "output_text": "\nA glass bowl filled with oranges sits on a table, surrounded by a silver and white patterned cloth. The oranges are piled high in the bowl, creating an inviting display of fresh fruit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10454.6, "ram_available_mb": 112051.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10447.5, "ram_available_mb": 112058.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 0.0}, "power_stats": {"power_watts_samples": [30.96, 30.96, 30.96], "power_watts_avg": 30.96, "power_watts_peak": 30.96, "energy_joules_est": 7.97, "sample_count": 3, "duration_seconds": 0.258}, "timestamp": "2026-01-25T20:06:10.012473"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 468.771, "latencies_ms": [468.771], "images_per_second": 2.133, "prompt_tokens": 756, "response_tokens_est": 81, "n_tiles": 1, "output_text": "\n The image features a bowl filled with oranges on a table. The oranges are bright orange in color and appear to be freshly picked from the tree. They are placed in various positions within the bowl, some closer to the top while others are near the bottom or at different angles. The background of the image is blurred, making it difficult to discern any other objects or details beyond the bowl of oranges.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10447.9, "ram_available_mb": 112058.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10441.5, "ram_available_mb": 112064.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [30.96, 31.33, 31.33, 31.33, 31.33], "power_watts_avg": 31.26, "power_watts_peak": 31.33, "energy_joules_est": 14.66, "sample_count": 5, "duration_seconds": 0.469}, "timestamp": "2026-01-25T20:06:10.519144"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 302.646, "latencies_ms": [302.646], "images_per_second": 3.304, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA surfer in a black wetsuit is skillfully riding a large wave on their surfboard, with the ocean stretching out behind them.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 10441.5, "ram_available_mb": 112064.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10446.2, "ram_available_mb": 112060.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [31.33, 33.72, 33.72, 33.72], "power_watts_avg": 33.13, "power_watts_peak": 33.72, "energy_joules_est": 10.05, "sample_count": 4, "duration_seconds": 0.303}, "timestamp": "2026-01-25T20:06:10.929038"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 71.692, "latencies_ms": [71.692], "images_per_second": 13.949, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Water - 8", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10446.3, "ram_available_mb": 112060.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10444.9, "ram_available_mb": 112061.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 53.0}, "power_stats": {"power_watts_samples": [33.72], "power_watts_avg": 33.72, "power_watts_peak": 33.72, "energy_joules_est": 2.43, "sample_count": 1, "duration_seconds": 0.072}, "timestamp": "2026-01-25T20:06:11.034602"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 131.896, "latencies_ms": [131.896], "images_per_second": 7.582, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nThe surfer is riding a wave in front of an ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10444.9, "ram_available_mb": 112061.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10445.1, "ram_available_mb": 112061.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [33.72, 37.39], "power_watts_avg": 35.56, "power_watts_peak": 37.39, "energy_joules_est": 4.72, "sample_count": 2, "duration_seconds": 0.133}, "timestamp": "2026-01-25T20:06:11.240667"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 443.436, "latencies_ms": [443.436], "images_per_second": 2.255, "prompt_tokens": 757, "response_tokens_est": 77, "n_tiles": 1, "output_text": "\nIn this image, a surfer in a black wetsuit is riding a large wave on their surfboard. The surfer is positioned near the center of the frame, skillfully navigating the powerful force of nature beneath them. The ocean surrounding the surfer is a deep blue-green color, indicating that it's likely an overcast day or during low light conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10445.1, "ram_available_mb": 112061.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10453.9, "ram_available_mb": 112052.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 35.0}, "power_stats": {"power_watts_samples": [37.39, 37.39, 37.39, 37.39, 36.09], "power_watts_avg": 37.13, "power_watts_peak": 37.39, "energy_joules_est": 16.48, "sample_count": 5, "duration_seconds": 0.444}, "timestamp": "2026-01-25T20:06:11.749486"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 336.717, "latencies_ms": [336.717], "images_per_second": 2.97, "prompt_tokens": 756, "response_tokens_est": 58, "n_tiles": 1, "output_text": "\n The image shows a surfer riding a wave in the ocean. The sky is dark and cloudy, which creates an overcast atmosphere. The water appears to be choppy, adding to the sense of motion as the surfer skillfully navigates the waves on their surfboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.9, "ram_available_mb": 112052.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10462.0, "ram_available_mb": 112044.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.09, 36.09, 36.09, 36.09], "power_watts_avg": 36.09, "power_watts_peak": 36.09, "energy_joules_est": 12.18, "sample_count": 4, "duration_seconds": 0.337}, "timestamp": "2026-01-25T20:06:12.157118"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 157.953, "latencies_ms": [157.953], "images_per_second": 6.331, "prompt_tokens": 744, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.0, "ram_available_mb": 112044.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.38, 36.38], "power_watts_avg": 36.38, "power_watts_peak": 36.38, "energy_joules_est": 5.76, "sample_count": 2, "duration_seconds": 0.158}, "timestamp": "2026-01-25T20:06:12.370442"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 88.567, "latencies_ms": [88.567], "images_per_second": 11.291, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Laptop screen", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10461.4, "ram_available_mb": 112044.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.38], "power_watts_avg": 36.38, "power_watts_peak": 36.38, "energy_joules_est": 3.24, "sample_count": 1, "duration_seconds": 0.089}, "timestamp": "2026-01-25T20:06:12.474960"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 125.042, "latencies_ms": [125.042], "images_per_second": 7.997, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Cat on laptop screen - 2nd from right", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10461.4, "ram_available_mb": 112044.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10462.1, "ram_available_mb": 112044.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.38, 36.38], "power_watts_avg": 36.38, "power_watts_peak": 36.38, "energy_joules_est": 4.56, "sample_count": 2, "duration_seconds": 0.125}, "timestamp": "2026-01-25T20:06:12.680578"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 173.816, "latencies_ms": [173.816], "images_per_second": 5.753, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA cat is sitting on a couch, looking at a laptop screen with its head resting on the keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.1, "ram_available_mb": 112044.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10460.9, "ram_available_mb": 112045.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.55, 34.55], "power_watts_avg": 34.55, "power_watts_peak": 34.55, "energy_joules_est": 6.02, "sample_count": 2, "duration_seconds": 0.174}, "timestamp": "2026-01-25T20:06:12.885971"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 98.435, "latencies_ms": [98.435], "images_per_second": 10.159, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n The cat is sitting on a laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.9, "ram_available_mb": 112045.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.55], "power_watts_avg": 34.55, "power_watts_peak": 34.55, "energy_joules_est": 3.41, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T20:06:12.990725"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 286.632, "latencies_ms": [286.632], "images_per_second": 3.489, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA group of horses, including a brown horse and two black horses, are gathered around a hay feeder in an open field.", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [34.55, 30.33, 30.33], "power_watts_avg": 31.74, "power_watts_peak": 34.55, "energy_joules_est": 9.1, "sample_count": 3, "duration_seconds": 0.287}, "timestamp": "2026-01-25T20:06:13.305219"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 63.047, "latencies_ms": [63.047], "images_per_second": 15.861, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10459.5, "ram_available_mb": 112046.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10461.4, "ram_available_mb": 112044.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [30.33], "power_watts_avg": 30.33, "power_watts_peak": 30.33, "energy_joules_est": 1.93, "sample_count": 1, "duration_seconds": 0.064}, "timestamp": "2026-01-25T20:06:13.410569"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 106.537, "latencies_ms": [106.537], "images_per_second": 9.386, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Horse eating hay on right side of photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.4, "ram_available_mb": 112044.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [30.33, 30.33], "power_watts_avg": 30.33, "power_watts_peak": 30.33, "energy_joules_est": 3.25, "sample_count": 2, "duration_seconds": 0.107}, "timestamp": "2026-01-25T20:06:13.616917"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 307.144, "latencies_ms": [307.144], "images_per_second": 3.256, "prompt_tokens": 757, "response_tokens_est": 51, "n_tiles": 1, "output_text": "\nA group of horses, including a brown horse with a white blaze on its face, are gathered around a hay feeder in an open field. The horses appear to be eating from the hay feeder together as they stand close by each other.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [33.19, 33.19, 33.19, 33.19], "power_watts_avg": 33.19, "power_watts_peak": 33.19, "energy_joules_est": 10.2, "sample_count": 4, "duration_seconds": 0.307}, "timestamp": "2026-01-25T20:06:14.023832"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 377.398, "latencies_ms": [377.398], "images_per_second": 2.65, "prompt_tokens": 756, "response_tokens_est": 65, "n_tiles": 1, "output_text": "\n The image shows a group of horses standing in the dirt. There are at least six horses visible, with some brown and others black. They appear to be eating hay from a trough or feeding bin placed on the ground. The scene is set outdoors during daylight hours, as evidenced by the bright sunlight illuminating the area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10465.0, "ram_available_mb": 112041.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 18.0}, "power_stats": {"power_watts_samples": [33.19, 34.19, 34.19, 34.19], "power_watts_avg": 33.94, "power_watts_peak": 34.19, "energy_joules_est": 12.82, "sample_count": 4, "duration_seconds": 0.378}, "timestamp": "2026-01-25T20:06:14.431765"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 294.326, "latencies_ms": [294.326], "images_per_second": 3.398, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA surfer in a black wetsuit rides a wave on a yellow surfboard, with their arms outstretched and legs bent for balance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.0, "ram_available_mb": 112041.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 18.0}, "power_stats": {"power_watts_samples": [34.19, 34.19, 36.78], "power_watts_avg": 35.06, "power_watts_peak": 36.78, "energy_joules_est": 10.35, "sample_count": 3, "duration_seconds": 0.295}, "timestamp": "2026-01-25T20:06:14.744115"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.798, "latencies_ms": [78.798], "images_per_second": 12.691, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.9, "ram_available_mb": 112039.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10466.8, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 18.0}, "power_stats": {"power_watts_samples": [36.78], "power_watts_avg": 36.78, "power_watts_peak": 36.78, "energy_joules_est": 2.91, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T20:06:14.849155"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 107.31, "latencies_ms": [107.31], "images_per_second": 9.319, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nThe man is surfing in a body of water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.8, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10474.1, "ram_available_mb": 112032.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 18.0}, "power_stats": {"power_watts_samples": [36.78, 36.78], "power_watts_avg": 36.78, "power_watts_peak": 36.78, "energy_joules_est": 3.96, "sample_count": 2, "duration_seconds": 0.108}, "timestamp": "2026-01-25T20:06:15.054404"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 190.473, "latencies_ms": [190.473], "images_per_second": 5.25, "prompt_tokens": 757, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA surfer in a black wetsuit rides a wave on his surfboard, with the ocean stretching out behind him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.1, "ram_available_mb": 112032.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10473.2, "ram_available_mb": 112033.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [36.78, 37.14], "power_watts_avg": 36.96, "power_watts_peak": 37.14, "energy_joules_est": 7.05, "sample_count": 2, "duration_seconds": 0.191}, "timestamp": "2026-01-25T20:06:15.260345"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 152.918, "latencies_ms": [152.918], "images_per_second": 6.539, "prompt_tokens": 756, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n The man is wearing a black wetsuit and riding the waves on his surfboard.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10473.2, "ram_available_mb": 112033.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [37.14, 37.14], "power_watts_avg": 37.14, "power_watts_peak": 37.14, "energy_joules_est": 5.7, "sample_count": 2, "duration_seconds": 0.153}, "timestamp": "2026-01-25T20:06:15.466709"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 282.853, "latencies_ms": [282.853], "images_per_second": 3.535, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "urns of flowers and a carved pumpkin with a cat face are displayed on a table, accompanied by a drawing in front of them.\n", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10471.5, "ram_available_mb": 112034.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 7.0, "ram_used_mb": 10475.9, "ram_available_mb": 112030.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [37.14, 37.14, 32.4], "power_watts_avg": 35.56, "power_watts_peak": 37.14, "energy_joules_est": 10.08, "sample_count": 3, "duration_seconds": 0.284}, "timestamp": "2026-01-25T20:06:15.779850"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 90.829, "latencies_ms": [90.829], "images_per_second": 11.01, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Pumpkin: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.9, "ram_available_mb": 112030.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10477.7, "ram_available_mb": 112028.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [32.4], "power_watts_avg": 32.4, "power_watts_peak": 32.4, "energy_joules_est": 2.95, "sample_count": 1, "duration_seconds": 0.091}, "timestamp": "2026-01-25T20:06:15.885043"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 151.893, "latencies_ms": [151.893], "images_per_second": 6.584, "prompt_tokens": 763, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n A carved pumpkin with a cat face and flowers on top sits next to two other carved pumpkins.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.7, "ram_available_mb": 112028.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10479.2, "ram_available_mb": 112027.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 78.0}, "power_stats": {"power_watts_samples": [32.4, 32.4], "power_watts_avg": 32.4, "power_watts_peak": 32.4, "energy_joules_est": 4.93, "sample_count": 2, "duration_seconds": 0.152}, "timestamp": "2026-01-25T20:06:16.090341"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 386.6, "latencies_ms": [386.6], "images_per_second": 2.587, "prompt_tokens": 757, "response_tokens_est": 66, "n_tiles": 1, "output_text": "\nIn a room, there are two carved pumpkins on a table. One pumpkin has a face with a mustache and a smiley expression, while the other pumpkin features a cat's face. The pumpkins are surrounded by flowers, including pink roses. In the background, there are bookshelves filled with various books.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.2, "ram_available_mb": 112027.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10481.9, "ram_available_mb": 112024.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 78.0}, "power_stats": {"power_watts_samples": [35.97, 35.97, 35.97, 35.97], "power_watts_avg": 35.97, "power_watts_peak": 35.97, "energy_joules_est": 13.92, "sample_count": 4, "duration_seconds": 0.387}, "timestamp": "2026-01-25T20:06:16.497065"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 118.339, "latencies_ms": [118.339], "images_per_second": 8.45, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A carved pumpkin with a cat face and a flower on top.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.9, "ram_available_mb": 112024.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10483.9, "ram_available_mb": 112022.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 78.0}, "power_stats": {"power_watts_samples": [35.97, 36.69], "power_watts_avg": 36.33, "power_watts_peak": 36.69, "energy_joules_est": 4.31, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T20:06:16.702396"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 263.319, "latencies_ms": [263.319], "images_per_second": 3.798, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "urns of soap and shampoo are on a shelf in a bathroom, with a black bag sitting next to them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.9, "ram_available_mb": 112022.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10486.1, "ram_available_mb": 112020.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 78.0}, "power_stats": {"power_watts_samples": [36.69, 36.69, 36.69], "power_watts_avg": 36.69, "power_watts_peak": 36.69, "energy_joules_est": 9.67, "sample_count": 3, "duration_seconds": 0.264}, "timestamp": "2026-01-25T20:06:17.013095"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 97.719, "latencies_ms": [97.719], "images_per_second": 10.233, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Toilet paper roll (1)", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10486.1, "ram_available_mb": 112020.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10487.1, "ram_available_mb": 112019.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [36.69], "power_watts_avg": 36.69, "power_watts_peak": 36.69, "energy_joules_est": 3.6, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T20:06:17.118401"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 131.011, "latencies_ms": [131.011], "images_per_second": 7.633, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nA black bag is on the floor next to a white sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10487.1, "ram_available_mb": 112019.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10486.3, "ram_available_mb": 112020.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [36.25, 36.25], "power_watts_avg": 36.25, "power_watts_peak": 36.25, "energy_joules_est": 4.76, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T20:06:17.323229"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 229.602, "latencies_ms": [229.602], "images_per_second": 4.355, "prompt_tokens": 757, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nThe image shows a small bathroom with white walls, a sink, and a trash bag on the floor. The room appears to be in need of cleaning or renovation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10486.3, "ram_available_mb": 112020.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10487.8, "ram_available_mb": 112018.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [36.25, 36.25, 36.25], "power_watts_avg": 36.25, "power_watts_peak": 36.25, "energy_joules_est": 8.35, "sample_count": 3, "duration_seconds": 0.23}, "timestamp": "2026-01-25T20:06:17.629008"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 287.454, "latencies_ms": [287.454], "images_per_second": 3.479, "prompt_tokens": 756, "response_tokens_est": 45, "n_tiles": 1, "output_text": "\nThe bathroom is white and has a black bag on the floor. The walls are painted yellow, which contrasts with the white sink and toilet. There's also a mirror above the sink that reflects light in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10487.8, "ram_available_mb": 112018.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10486.1, "ram_available_mb": 112020.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [34.45, 34.45, 34.45], "power_watts_avg": 34.45, "power_watts_peak": 34.45, "energy_joules_est": 9.92, "sample_count": 3, "duration_seconds": 0.288}, "timestamp": "2026-01-25T20:06:17.934377"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 256.947, "latencies_ms": [256.947], "images_per_second": 3.892, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "urn of water on a bedspread, with a baby sitting in it and looking at a laptop screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10486.1, "ram_available_mb": 112020.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10484.4, "ram_available_mb": 112021.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [34.45, 34.45, 34.75], "power_watts_avg": 34.55, "power_watts_peak": 34.75, "energy_joules_est": 8.9, "sample_count": 3, "duration_seconds": 0.258}, "timestamp": "2026-01-25T20:06:18.244446"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.417, "latencies_ms": [79.417], "images_per_second": 12.592, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Laptop screen", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.4, "ram_available_mb": 112021.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10484.2, "ram_available_mb": 112022.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [34.75], "power_watts_avg": 34.75, "power_watts_peak": 34.75, "energy_joules_est": 2.78, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T20:06:18.350282"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 133.569, "latencies_ms": [133.569], "images_per_second": 7.487, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. A baby sitting on a bed looking at a laptop screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.2, "ram_available_mb": 112022.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10487.6, "ram_available_mb": 112018.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [34.75, 34.75], "power_watts_avg": 34.75, "power_watts_peak": 34.75, "energy_joules_est": 4.65, "sample_count": 2, "duration_seconds": 0.134}, "timestamp": "2026-01-25T20:06:18.556176"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 146.348, "latencies_ms": [146.348], "images_per_second": 6.833, "prompt_tokens": 757, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nA young child sits on a bed, focused on watching something on a laptop computer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10487.6, "ram_available_mb": 112018.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10489.6, "ram_available_mb": 112016.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [34.75, 33.51], "power_watts_avg": 34.13, "power_watts_peak": 34.75, "energy_joules_est": 5.01, "sample_count": 2, "duration_seconds": 0.147}, "timestamp": "2026-01-25T20:06:18.762766"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 164.599, "latencies_ms": [164.599], "images_per_second": 6.075, "prompt_tokens": 756, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\n The image is a black and white photo of a baby sitting on the floor in front of an open laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10489.6, "ram_available_mb": 112016.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10489.0, "ram_available_mb": 112017.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [33.51, 33.51], "power_watts_avg": 33.51, "power_watts_peak": 33.51, "energy_joules_est": 5.53, "sample_count": 2, "duration_seconds": 0.165}, "timestamp": "2026-01-25T20:06:18.969694"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 261.829, "latencies_ms": [261.829], "images_per_second": 3.819, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "iced skier in a brown jacket and black pants, wearing goggles and holding ski poles on a snowy mountain slope.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10489.0, "ram_available_mb": 112017.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10489.5, "ram_available_mb": 112016.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [33.51, 33.51, 31.25], "power_watts_avg": 32.76, "power_watts_peak": 33.51, "energy_joules_est": 8.59, "sample_count": 3, "duration_seconds": 0.262}, "timestamp": "2026-01-25T20:06:19.284175"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 68.675, "latencies_ms": [68.675], "images_per_second": 14.561, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Tree", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10489.5, "ram_available_mb": 112016.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10491.2, "ram_available_mb": 112015.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [31.25], "power_watts_avg": 31.25, "power_watts_peak": 31.25, "energy_joules_est": 2.16, "sample_count": 1, "duration_seconds": 0.069}, "timestamp": "2026-01-25T20:06:19.389896"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 105.019, "latencies_ms": [105.019], "images_per_second": 9.522, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\nThe skier is in front of a tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10491.2, "ram_available_mb": 112015.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10495.1, "ram_available_mb": 112011.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [31.25, 31.25], "power_watts_avg": 31.25, "power_watts_peak": 31.25, "energy_joules_est": 3.3, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T20:06:19.596973"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 163.953, "latencies_ms": [163.953], "images_per_second": 6.099, "prompt_tokens": 757, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA skier in a brown jacket and blue pants is skiing down a snowy mountain, surrounded by trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10495.1, "ram_available_mb": 112011.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10485.5, "ram_available_mb": 112020.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [32.7, 32.7], "power_watts_avg": 32.7, "power_watts_peak": 32.7, "energy_joules_est": 5.37, "sample_count": 2, "duration_seconds": 0.164}, "timestamp": "2026-01-25T20:06:19.803480"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 109.27, "latencies_ms": [109.27], "images_per_second": 9.152, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The skier is wearing a brown jacket and blue pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10485.5, "ram_available_mb": 112020.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10489.4, "ram_available_mb": 112016.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 21.0}, "power_stats": {"power_watts_samples": [32.7, 32.7], "power_watts_avg": 32.7, "power_watts_peak": 32.7, "energy_joules_est": 3.58, "sample_count": 2, "duration_seconds": 0.109}, "timestamp": "2026-01-25T20:06:20.009197"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 252.342, "latencies_ms": [252.342], "images_per_second": 3.963, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urns of gold and white are on a bedside table in a hotel room with a large window.\n", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 10489.4, "ram_available_mb": 112016.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10488.1, "ram_available_mb": 112018.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [32.7, 30.58, 30.58], "power_watts_avg": 31.29, "power_watts_peak": 32.7, "energy_joules_est": 7.91, "sample_count": 3, "duration_seconds": 0.253}, "timestamp": "2026-01-25T20:06:20.320570"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 67.384, "latencies_ms": [67.384], "images_per_second": 14.84, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Bed", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.1, "ram_available_mb": 112018.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10488.5, "ram_available_mb": 112017.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [30.58], "power_watts_avg": 30.58, "power_watts_peak": 30.58, "energy_joules_est": 2.07, "sample_count": 1, "duration_seconds": 0.068}, "timestamp": "2026-01-25T20:06:20.427515"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 101.039, "latencies_ms": [101.039], "images_per_second": 9.897, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urn on a table next to bed.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.5, "ram_available_mb": 112017.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10488.5, "ram_available_mb": 112017.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 6.0}, "power_stats": {"power_watts_samples": [30.58, 30.58], "power_watts_avg": 30.58, "power_watts_peak": 30.58, "energy_joules_est": 3.1, "sample_count": 2, "duration_seconds": 0.101}, "timestamp": "2026-01-25T20:06:20.633515"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 633.018, "latencies_ms": [633.018], "images_per_second": 1.58, "prompt_tokens": 757, "response_tokens_est": 115, "n_tiles": 1, "output_text": "\nThe image shows a hotel room with a large bed, two lamps on either side of the bed, and a window. The bed has white sheets and blue pillows, creating a cozy atmosphere. A suitcase can be seen next to the bed, suggesting that someone may have recently arrived or is preparing to leave.\n\nIn addition to the main elements in the room, there are two chairs present - one located near the center of the room and another closer to the window. The overall setting appears to be a comfortable and inviting space for guests staying at the hotel.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10488.5, "ram_available_mb": 112017.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10490.3, "ram_available_mb": 112016.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [30.95, 30.95, 30.95, 30.95, 30.95, 36.44, 36.44], "power_watts_avg": 32.52, "power_watts_peak": 36.44, "energy_joules_est": 20.6, "sample_count": 7, "duration_seconds": 0.634}, "timestamp": "2026-01-25T20:06:21.342399"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 115.359, "latencies_ms": [115.359], "images_per_second": 8.669, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The room is well lit and has a large window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10490.3, "ram_available_mb": 112016.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10490.5, "ram_available_mb": 112015.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.44, 36.44], "power_watts_avg": 36.44, "power_watts_peak": 36.44, "energy_joules_est": 4.23, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T20:06:21.549916"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 287.461, "latencies_ms": [287.461], "images_per_second": 3.479, "prompt_tokens": 744, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA skier in a white jacket and orange pants is skiing down a snowy hill, with other skiers visible in the background.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10490.5, "ram_available_mb": 112015.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10490.8, "ram_available_mb": 112015.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.44, 33.12, 33.12], "power_watts_avg": 34.23, "power_watts_peak": 36.44, "energy_joules_est": 9.87, "sample_count": 3, "duration_seconds": 0.288}, "timestamp": "2026-01-25T20:06:21.863286"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 84.508, "latencies_ms": [84.508], "images_per_second": 11.833, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10490.8, "ram_available_mb": 112015.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10488.4, "ram_available_mb": 112017.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.12], "power_watts_avg": 33.12, "power_watts_peak": 33.12, "energy_joules_est": 2.81, "sample_count": 1, "duration_seconds": 0.085}, "timestamp": "2026-01-25T20:06:21.968691"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 156.822, "latencies_ms": [156.822], "images_per_second": 6.377, "prompt_tokens": 763, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nThe skier is in the center of a snowy hill and appears to be skiing down it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.4, "ram_available_mb": 112017.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10490.9, "ram_available_mb": 112015.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [33.12, 33.12], "power_watts_avg": 33.12, "power_watts_peak": 33.12, "energy_joules_est": 5.2, "sample_count": 2, "duration_seconds": 0.157}, "timestamp": "2026-01-25T20:06:22.174675"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 181.504, "latencies_ms": [181.504], "images_per_second": 5.51, "prompt_tokens": 757, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA skier in a white jacket and orange pants is skiing down a snowy hill, with other skiers nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10490.9, "ram_available_mb": 112015.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10485.2, "ram_available_mb": 112021.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [34.55, 34.55], "power_watts_avg": 34.55, "power_watts_peak": 34.55, "energy_joules_est": 6.29, "sample_count": 2, "duration_seconds": 0.182}, "timestamp": "2026-01-25T20:06:22.380591"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 117.652, "latencies_ms": [117.652], "images_per_second": 8.5, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The skier is wearing a white jacket and orange pants.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10485.2, "ram_available_mb": 112021.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10483.8, "ram_available_mb": 112022.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [34.55, 34.55], "power_watts_avg": 34.55, "power_watts_peak": 34.55, "energy_joules_est": 4.07, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:06:22.587296"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 271.21, "latencies_ms": [271.21], "images_per_second": 3.687, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA street scene features a building with graffiti on it, including a sticker that reads \"The One N' Only.\"", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 10483.8, "ram_available_mb": 112022.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10482.4, "ram_available_mb": 112023.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [34.16, 34.16, 34.16], "power_watts_avg": 34.16, "power_watts_peak": 34.16, "energy_joules_est": 9.27, "sample_count": 3, "duration_seconds": 0.271}, "timestamp": "2026-01-25T20:06:22.897485"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 89.129, "latencies_ms": [89.129], "images_per_second": 11.22, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Graffiti on the wall", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.4, "ram_available_mb": 112023.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 7.4, "ram_used_mb": 10481.6, "ram_available_mb": 112024.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [34.16], "power_watts_avg": 34.16, "power_watts_peak": 34.16, "energy_joules_est": 3.06, "sample_count": 1, "duration_seconds": 0.089}, "timestamp": "2026-01-25T20:06:23.003175"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 132.46, "latencies_ms": [132.46], "images_per_second": 7.549, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nThe graffiti on the wall is in front of a metal fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.6, "ram_available_mb": 112024.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10481.1, "ram_available_mb": 112025.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [34.16, 33.08], "power_watts_avg": 33.62, "power_watts_peak": 34.16, "energy_joules_est": 4.47, "sample_count": 2, "duration_seconds": 0.133}, "timestamp": "2026-01-25T20:06:23.209289"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 367.31, "latencies_ms": [367.31], "images_per_second": 2.722, "prompt_tokens": 757, "response_tokens_est": 60, "n_tiles": 1, "output_text": "\nThe image shows a street corner with graffiti on the side of a building. The graffiti features various colors, including blue, green, red, yellow, white, black, and gray. A metal pole stands in front of the graffiti-covered wall, and there are two parking meters nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.1, "ram_available_mb": 112025.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10482.8, "ram_available_mb": 112023.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [33.08, 33.08, 33.08, 33.08], "power_watts_avg": 33.08, "power_watts_peak": 33.08, "energy_joules_est": 12.17, "sample_count": 4, "duration_seconds": 0.368}, "timestamp": "2026-01-25T20:06:23.616738"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 96.354, "latencies_ms": [96.354], "images_per_second": 10.378, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The graffiti on the wall is colorful and vibrant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.8, "ram_available_mb": 112023.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10482.1, "ram_available_mb": 112024.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [36.11], "power_watts_avg": 36.11, "power_watts_peak": 36.11, "energy_joules_est": 3.49, "sample_count": 1, "duration_seconds": 0.097}, "timestamp": "2026-01-25T20:06:23.721607"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 268.75, "latencies_ms": [268.75], "images_per_second": 3.721, "prompt_tokens": 744, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\nA surfer in a black wetsuit rides a wave on their surfboard, skillfully navigating the ocean's surface.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10482.1, "ram_available_mb": 112024.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10485.3, "ram_available_mb": 112021.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [36.11, 36.11, 36.11], "power_watts_avg": 36.11, "power_watts_peak": 36.11, "energy_joules_est": 9.71, "sample_count": 3, "duration_seconds": 0.269}, "timestamp": "2026-01-25T20:06:24.035590"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 63.48, "latencies_ms": [63.48], "images_per_second": 15.753, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10485.3, "ram_available_mb": 112021.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10485.5, "ram_available_mb": 112020.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [36.11], "power_watts_avg": 36.11, "power_watts_peak": 36.11, "energy_joules_est": 2.31, "sample_count": 1, "duration_seconds": 0.064}, "timestamp": "2026-01-25T20:06:24.141968"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 115.141, "latencies_ms": [115.141], "images_per_second": 8.685, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe surfer is riding a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10485.5, "ram_available_mb": 112020.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10487.8, "ram_available_mb": 112018.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [36.81, 36.81], "power_watts_avg": 36.81, "power_watts_peak": 36.81, "energy_joules_est": 4.26, "sample_count": 2, "duration_seconds": 0.116}, "timestamp": "2026-01-25T20:06:24.349176"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 254.392, "latencies_ms": [254.392], "images_per_second": 3.931, "prompt_tokens": 757, "response_tokens_est": 41, "n_tiles": 1, "output_text": "\nIn the image, a person wearing black shorts is surfing on a wave in the ocean. The surfer is skillfully riding the wave with their surfboard, showcasing their talent for the sport.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10487.8, "ram_available_mb": 112018.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10488.5, "ram_available_mb": 112017.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [36.81, 36.81, 36.81], "power_watts_avg": 36.81, "power_watts_peak": 36.81, "energy_joules_est": 9.39, "sample_count": 3, "duration_seconds": 0.255}, "timestamp": "2026-01-25T20:06:24.657206"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 644.811, "latencies_ms": [644.811], "images_per_second": 1.551, "prompt_tokens": 756, "response_tokens_est": 118, "n_tiles": 1, "output_text": "\n The image shows a surfer riding the waves in the ocean. The surfer is wearing black clothing and appears to be crouching on their surfboard as they navigate through the water. The wave has white foam on it, indicating that it's a relatively small wave. The background of the photo features a sky with clouds, suggesting an overcast day or possibly a cloudy morning.\n \nThe surfer is positioned in the center of the image and appears to be focused on their ride. They are skillfully maneuvering through the water while maintaining balance on their surfboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10488.5, "ram_available_mb": 112017.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10485.8, "ram_available_mb": 112020.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.15, 34.15, 34.15, 34.15, 34.15, 34.23, 34.23], "power_watts_avg": 34.18, "power_watts_peak": 34.23, "energy_joules_est": 22.05, "sample_count": 7, "duration_seconds": 0.645}, "timestamp": "2026-01-25T20:06:25.364659"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 313.322, "latencies_ms": [313.322], "images_per_second": 3.192, "prompt_tokens": 744, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nTwo men are standing on a sidewalk next to a yellow double-decker bus, with one man wearing a black hat and the other carrying a backpack.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10485.8, "ram_available_mb": 112020.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10487.3, "ram_available_mb": 112019.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.23, 34.23, 34.23, 40.43], "power_watts_avg": 35.78, "power_watts_peak": 40.43, "energy_joules_est": 11.22, "sample_count": 4, "duration_seconds": 0.314}, "timestamp": "2026-01-25T20:06:25.776446"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 70.696, "latencies_ms": [70.696], "images_per_second": 14.145, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1 - Bus", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10487.3, "ram_available_mb": 112019.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10486.0, "ram_available_mb": 112020.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [40.43], "power_watts_avg": 40.43, "power_watts_peak": 40.43, "energy_joules_est": 2.87, "sample_count": 1, "duration_seconds": 0.071}, "timestamp": "2026-01-25T20:06:25.882009"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 114.792, "latencies_ms": [114.792], "images_per_second": 8.711, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Bus stop sign is in front of bus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10486.0, "ram_available_mb": 112020.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10477.6, "ram_available_mb": 112028.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 64.0}, "power_stats": {"power_watts_samples": [40.43, 40.43], "power_watts_avg": 40.43, "power_watts_peak": 40.43, "energy_joules_est": 4.65, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T20:06:26.087167"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 274.349, "latencies_ms": [274.349], "images_per_second": 3.645, "prompt_tokens": 757, "response_tokens_est": 45, "n_tiles": 1, "output_text": "\nA yellow double-decker bus with the number \"941\" on its front, parked at a bus stop. Two men are standing next to the bus, possibly waiting for their ride or just passing by.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.6, "ram_available_mb": 112028.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10483.9, "ram_available_mb": 112022.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 64.0}, "power_stats": {"power_watts_samples": [34.78, 34.78, 34.78], "power_watts_avg": 34.78, "power_watts_peak": 34.78, "energy_joules_est": 9.55, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T20:06:26.391473"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 90.504, "latencies_ms": [90.504], "images_per_second": 11.049, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The bus is yellow and black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.9, "ram_available_mb": 112022.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10483.9, "ram_available_mb": 112022.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 64.0}, "power_stats": {"power_watts_samples": [34.78], "power_watts_avg": 34.78, "power_watts_peak": 34.78, "energy_joules_est": 3.16, "sample_count": 1, "duration_seconds": 0.091}, "timestamp": "2026-01-25T20:06:26.496461"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 318.707, "latencies_ms": [318.707], "images_per_second": 3.138, "prompt_tokens": 744, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nA yellow and red airplane with the letters \"SP-AWF\" on its tail is flying through a cloudy sky, moving from left to right in the frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.9, "ram_available_mb": 112022.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10481.7, "ram_available_mb": 112024.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 64.0}, "power_stats": {"power_watts_samples": [34.78, 30.46, 30.46, 30.46], "power_watts_avg": 31.54, "power_watts_peak": 34.78, "energy_joules_est": 10.06, "sample_count": 4, "duration_seconds": 0.319}, "timestamp": "2026-01-25T20:06:26.905995"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.948, "latencies_ms": [76.948], "images_per_second": 12.996, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Plane", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.7, "ram_available_mb": 112024.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10479.8, "ram_available_mb": 112026.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 64.0}, "power_stats": {"power_watts_samples": [30.46], "power_watts_avg": 30.46, "power_watts_peak": 30.46, "energy_joules_est": 2.35, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T20:06:27.010960"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 101.282, "latencies_ms": [101.282], "images_per_second": 9.873, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\nThe airplane is flying in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.8, "ram_available_mb": 112026.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10479.3, "ram_available_mb": 112027.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [30.46, 34.53], "power_watts_avg": 32.5, "power_watts_peak": 34.53, "energy_joules_est": 3.3, "sample_count": 2, "duration_seconds": 0.102}, "timestamp": "2026-01-25T20:06:27.217065"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 271.109, "latencies_ms": [271.109], "images_per_second": 3.689, "prompt_tokens": 757, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nA small yellow airplane with red accents is flying through a cloudy sky. The plane has the letters \"SP-AWF\" on its side, indicating that it might be an older model or a vintage aircraft.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10479.3, "ram_available_mb": 112027.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10479.8, "ram_available_mb": 112026.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [34.53, 34.53, 34.53], "power_watts_avg": 34.53, "power_watts_peak": 34.53, "energy_joules_est": 9.37, "sample_count": 3, "duration_seconds": 0.271}, "timestamp": "2026-01-25T20:06:27.521527"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 114.177, "latencies_ms": [114.177], "images_per_second": 8.758, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The airplane is yellow and red with blue accents.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.8, "ram_available_mb": 112026.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10478.5, "ram_available_mb": 112027.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 26.0}, "power_stats": {"power_watts_samples": [34.53, 32.74], "power_watts_avg": 33.64, "power_watts_peak": 34.53, "energy_joules_est": 3.86, "sample_count": 2, "duration_seconds": 0.115}, "timestamp": "2026-01-25T20:06:27.726250"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 357.927, "latencies_ms": [357.927], "images_per_second": 2.794, "prompt_tokens": 744, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nAn aerial view of a parking lot shows numerous cars in various colors, including red and white, parked next to each other. The scene is set against a backdrop of buildings and trees, creating an urban landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.7, "ram_available_mb": 112027.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10478.9, "ram_available_mb": 112027.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [32.74, 32.74, 32.74, 32.74], "power_watts_avg": 32.74, "power_watts_peak": 32.74, "energy_joules_est": 11.75, "sample_count": 4, "duration_seconds": 0.359}, "timestamp": "2026-01-25T20:06:28.138546"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 91.396, "latencies_ms": [91.396], "images_per_second": 10.941, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Wing of plane", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.5, "ram_available_mb": 112027.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.45], "power_watts_avg": 35.45, "power_watts_peak": 35.45, "energy_joules_est": 3.26, "sample_count": 1, "duration_seconds": 0.092}, "timestamp": "2026-01-25T20:06:28.243402"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 145.402, "latencies_ms": [145.402], "images_per_second": 6.877, "prompt_tokens": 763, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nA large airplane wing is visible in the top left corner of a parking lot filled with cars.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10475.7, "ram_available_mb": 112030.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.45, 35.45], "power_watts_avg": 35.45, "power_watts_peak": 35.45, "energy_joules_est": 5.16, "sample_count": 2, "duration_seconds": 0.146}, "timestamp": "2026-01-25T20:06:28.448574"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 149.202, "latencies_ms": [149.202], "images_per_second": 6.702, "prompt_tokens": 757, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nAn aerial view of a parking lot filled with cars, including several white and blue cars.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.7, "ram_available_mb": 112030.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.45, 35.45], "power_watts_avg": 35.45, "power_watts_peak": 35.45, "energy_joules_est": 5.3, "sample_count": 2, "duration_seconds": 0.15}, "timestamp": "2026-01-25T20:06:28.653992"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 327.268, "latencies_ms": [327.268], "images_per_second": 3.056, "prompt_tokens": 756, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\n The image shows an aerial view of a parking lot filled with cars. There are at least 15 cars visible in the scene, some parked and others possibly moving around. The sky above is blue with scattered clouds, creating a pleasant atmosphere for the vehicles to be parked under.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10470.4, "ram_available_mb": 112035.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.78, 34.78, 34.78, 34.78], "power_watts_avg": 34.78, "power_watts_peak": 34.78, "energy_joules_est": 11.39, "sample_count": 4, "duration_seconds": 0.328}, "timestamp": "2026-01-25T20:06:29.061127"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 280.716, "latencies_ms": [280.716], "images_per_second": 3.562, "prompt_tokens": 744, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA pink flip phone is being held by a person's hand, with a cartoon character on the screen and a cup of coffee nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.4, "ram_available_mb": 112035.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 69.0}, "power_stats": {"power_watts_samples": [34.78, 31.06, 31.06], "power_watts_avg": 32.3, "power_watts_peak": 34.78, "energy_joules_est": 9.09, "sample_count": 3, "duration_seconds": 0.281}, "timestamp": "2026-01-25T20:06:29.371019"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 83.362, "latencies_ms": [83.362], "images_per_second": 11.996, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Pink cell phone", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10472.2, "ram_available_mb": 112034.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 69.0}, "power_stats": {"power_watts_samples": [31.06], "power_watts_avg": 31.06, "power_watts_peak": 31.06, "energy_joules_est": 2.6, "sample_count": 1, "duration_seconds": 0.084}, "timestamp": "2026-01-25T20:06:29.475763"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 132.434, "latencies_ms": [132.434], "images_per_second": 7.551, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n 1. A pink cell phone with a cartoon character on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.2, "ram_available_mb": 112034.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10473.9, "ram_available_mb": 112032.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 69.0}, "power_stats": {"power_watts_samples": [31.06, 31.06], "power_watts_avg": 31.06, "power_watts_peak": 31.06, "energy_joules_est": 4.12, "sample_count": 2, "duration_seconds": 0.133}, "timestamp": "2026-01-25T20:06:29.680930"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 129.041, "latencies_ms": [129.041], "images_per_second": 7.749, "prompt_tokens": 757, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A person holding a pink flip phone with a cartoon character on it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10473.9, "ram_available_mb": 112032.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10472.2, "ram_available_mb": 112034.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 69.0}, "power_stats": {"power_watts_samples": [36.05, 36.05], "power_watts_avg": 36.05, "power_watts_peak": 36.05, "energy_joules_est": 4.66, "sample_count": 2, "duration_seconds": 0.129}, "timestamp": "2026-01-25T20:06:29.887707"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 126.087, "latencies_ms": [126.087], "images_per_second": 7.931, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The pink cell phone is the main focus of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.2, "ram_available_mb": 112034.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [36.05, 36.05], "power_watts_avg": 36.05, "power_watts_peak": 36.05, "energy_joules_est": 4.55, "sample_count": 2, "duration_seconds": 0.126}, "timestamp": "2026-01-25T20:06:30.093604"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 268.658, "latencies_ms": [268.658], "images_per_second": 3.722, "prompt_tokens": 744, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\nTwo zebras stand in a field of tall, dry grass with trees visible in the background.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [31.91, 31.91, 31.91], "power_watts_avg": 31.91, "power_watts_peak": 31.91, "energy_joules_est": 8.61, "sample_count": 3, "duration_seconds": 0.27}, "timestamp": "2026-01-25T20:06:30.404741"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 87.853, "latencies_ms": [87.853], "images_per_second": 11.383, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Tree  2. Tree", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10462.3, "ram_available_mb": 112044.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [31.91], "power_watts_avg": 31.91, "power_watts_peak": 31.91, "energy_joules_est": 2.81, "sample_count": 1, "duration_seconds": 0.088}, "timestamp": "2026-01-25T20:06:30.509308"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 240.556, "latencies_ms": [240.556], "images_per_second": 4.157, "prompt_tokens": 763, "response_tokens_est": 39, "n_tiles": 1, "output_text": "\nThe zebra on the left is facing towards the right side of the image. The other zebra is facing towards the left side and appears to be looking at something in the distance.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10462.3, "ram_available_mb": 112044.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10453.2, "ram_available_mb": 112053.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 60.0}, "power_stats": {"power_watts_samples": [31.91, 30.97, 30.97], "power_watts_avg": 31.28, "power_watts_peak": 31.91, "energy_joules_est": 7.53, "sample_count": 3, "duration_seconds": 0.241}, "timestamp": "2026-01-25T20:06:30.815025"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 269.638, "latencies_ms": [269.638], "images_per_second": 3.709, "prompt_tokens": 757, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\n Two zebras are standing in a field of tall grass, facing opposite directions. The zebra on the left appears to be looking at something off-camera while the other one looks directly at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10453.2, "ram_available_mb": 112053.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10462.8, "ram_available_mb": 112043.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.97, 30.97, 30.97], "power_watts_avg": 30.97, "power_watts_peak": 30.97, "energy_joules_est": 8.36, "sample_count": 3, "duration_seconds": 0.27}, "timestamp": "2026-01-25T20:06:31.120846"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 330.089, "latencies_ms": [330.089], "images_per_second": 3.029, "prompt_tokens": 756, "response_tokens_est": 53, "n_tiles": 1, "output_text": "\n The image features two zebras standing in a field of tall grass. One zebra is facing the camera while the other one has its back turned to it. The background consists of trees and bushes, providing a natural setting for the zebras.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.8, "ram_available_mb": 112043.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.69, 36.69, 36.69, 36.69], "power_watts_avg": 36.69, "power_watts_peak": 36.69, "energy_joules_est": 12.12, "sample_count": 4, "duration_seconds": 0.33}, "timestamp": "2026-01-25T20:06:31.527546"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 290.585, "latencies_ms": [290.585], "images_per_second": 3.441, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA man in a blue shirt and shorts walks along the beach, carrying his yellow surfboard under his arm as he heads towards the ocean waves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.6, "ram_available_mb": 112044.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.69, 34.42, 34.42], "power_watts_avg": 35.17, "power_watts_peak": 36.69, "energy_joules_est": 10.23, "sample_count": 3, "duration_seconds": 0.291}, "timestamp": "2026-01-25T20:06:31.837216"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 97.567, "latencies_ms": [97.567], "images_per_second": 10.249, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Surfer carrying surfboard", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10465.3, "ram_available_mb": 112041.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.42], "power_watts_avg": 34.42, "power_watts_peak": 34.42, "energy_joules_est": 3.36, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T20:06:31.941371"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 117.71, "latencies_ms": [117.71], "images_per_second": 8.495, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nThe man is walking on a beach with his surfboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.5, "ram_available_mb": 112038.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10467.9, "ram_available_mb": 112038.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [34.42, 34.42], "power_watts_avg": 34.42, "power_watts_peak": 34.42, "energy_joules_est": 4.06, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:06:32.145707"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 151.694, "latencies_ms": [151.694], "images_per_second": 6.592, "prompt_tokens": 757, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nA man in a bathing suit is walking out of the ocean, carrying his surfboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.9, "ram_available_mb": 112038.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [36.99, 36.99], "power_watts_avg": 36.99, "power_watts_peak": 36.99, "energy_joules_est": 5.63, "sample_count": 2, "duration_seconds": 0.152}, "timestamp": "2026-01-25T20:06:32.352579"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 124.223, "latencies_ms": [124.223], "images_per_second": 8.05, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n The man is holding a yellow surfboard and walking into the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [36.99, 36.99], "power_watts_avg": 36.99, "power_watts_peak": 36.99, "energy_joules_est": 4.61, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T20:06:32.558400"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 406.465, "latencies_ms": [406.465], "images_per_second": 2.46, "prompt_tokens": 744, "response_tokens_est": 52, "n_tiles": 1, "output_text": "\nA black and white photograph captures a cow standing on a sandy beach, facing away from the camera with its head turned to the left. The cow is positioned in front of the ocean, creating an interesting contrast between the natural elements captured in the photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.4, "ram_available_mb": 112038.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10466.7, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 48.0}, "power_stats": {"power_watts_samples": [36.99, 33.27, 33.27, 33.27, 33.27], "power_watts_avg": 34.01, "power_watts_peak": 36.99, "energy_joules_est": 13.84, "sample_count": 5, "duration_seconds": 0.407}, "timestamp": "2026-01-25T20:06:33.067365"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 64.048, "latencies_ms": [64.048], "images_per_second": 15.613, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.7, "ram_available_mb": 112039.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [33.27], "power_watts_avg": 33.27, "power_watts_peak": 33.27, "energy_joules_est": 2.15, "sample_count": 1, "duration_seconds": 0.065}, "timestamp": "2026-01-25T20:06:33.172412"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 133.487, "latencies_ms": [133.487], "images_per_second": 7.491, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nA black and white cow stands on a beach next to water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.1, "ram_available_mb": 112045.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10463.3, "ram_available_mb": 112043.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [33.1, 33.1], "power_watts_avg": 33.1, "power_watts_peak": 33.1, "energy_joules_est": 4.43, "sample_count": 2, "duration_seconds": 0.134}, "timestamp": "2026-01-25T20:06:33.378711"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 199.22, "latencies_ms": [199.22], "images_per_second": 5.02, "prompt_tokens": 757, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA black and white cow stands on a sandy beach, facing away from the camera. The cow appears to be alone in this serene environment.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10462.9, "ram_available_mb": 112043.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 58.0}, "power_stats": {"power_watts_samples": [33.1, 33.1], "power_watts_avg": 33.1, "power_watts_peak": 33.1, "energy_joules_est": 6.61, "sample_count": 2, "duration_seconds": 0.2}, "timestamp": "2026-01-25T20:06:33.582903"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 415.296, "latencies_ms": [415.296], "images_per_second": 2.408, "prompt_tokens": 756, "response_tokens_est": 71, "n_tiles": 1, "output_text": "\nThe image is a black and white photograph of a cow standing on the beach. The cow has a white face with black spots and stands out against the monochrome background. The water in the foreground appears calm, reflecting the light from above. The sky overhead is overcast, casting a soft light that enhances the overall mood of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10457.2, "ram_available_mb": 112049.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [33.1, 35.15, 35.15, 35.15, 35.15], "power_watts_avg": 34.74, "power_watts_peak": 35.15, "energy_joules_est": 14.43, "sample_count": 5, "duration_seconds": 0.416}, "timestamp": "2026-01-25T20:06:34.089752"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 238.963, "latencies_ms": [238.963], "images_per_second": 4.185, "prompt_tokens": 744, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA woman stands in a snowy landscape, dressed for skiing and holding ski poles.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10457.2, "ram_available_mb": 112049.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 10458.9, "ram_available_mb": 112047.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [32.82, 32.82, 32.82], "power_watts_avg": 32.82, "power_watts_peak": 32.82, "energy_joules_est": 7.86, "sample_count": 3, "duration_seconds": 0.24}, "timestamp": "2026-01-25T20:06:34.399491"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 85.486, "latencies_ms": [85.486], "images_per_second": 11.698, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Skis", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10458.9, "ram_available_mb": 112047.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10455.3, "ram_available_mb": 112051.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [32.82], "power_watts_avg": 32.82, "power_watts_peak": 32.82, "energy_joules_est": 2.83, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T20:06:34.504845"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 134.366, "latencies_ms": [134.366], "images_per_second": 7.442, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nThe woman is standing in front of a tree and holding ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.3, "ram_available_mb": 112051.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10455.1, "ram_available_mb": 112051.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [32.82, 35.3], "power_watts_avg": 34.06, "power_watts_peak": 35.3, "energy_joules_est": 4.58, "sample_count": 2, "duration_seconds": 0.135}, "timestamp": "2026-01-25T20:06:34.710895"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 257.043, "latencies_ms": [257.043], "images_per_second": 3.89, "prompt_tokens": 757, "response_tokens_est": 42, "n_tiles": 1, "output_text": "\nA woman in a white shirt, black pants, and a flowery top stands on skis with ski poles. She holds her ski poles up to her face as she talks on a cell phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10455.1, "ram_available_mb": 112051.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10458.0, "ram_available_mb": 112048.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 84.0}, "power_stats": {"power_watts_samples": [35.3, 35.3, 35.3], "power_watts_avg": 35.3, "power_watts_peak": 35.3, "energy_joules_est": 9.09, "sample_count": 3, "duration_seconds": 0.258}, "timestamp": "2026-01-25T20:06:35.017172"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 117.585, "latencies_ms": [117.585], "images_per_second": 8.504, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The woman is wearing a white top and black pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10458.0, "ram_available_mb": 112048.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [35.3, 33.01], "power_watts_avg": 34.15, "power_watts_peak": 35.3, "energy_joules_est": 4.04, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:06:35.223570"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 324.217, "latencies_ms": [324.217], "images_per_second": 3.084, "prompt_tokens": 744, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nA dog is standing on a sandy beach, holding a yellow frisbee in its mouth and looking directly at the camera with an expression of curiosity or playfulness.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10458.6, "ram_available_mb": 112047.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10460.8, "ram_available_mb": 112045.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [33.01, 33.01, 33.01, 33.01], "power_watts_avg": 33.01, "power_watts_peak": 33.01, "energy_joules_est": 10.71, "sample_count": 4, "duration_seconds": 0.325}, "timestamp": "2026-01-25T20:06:35.636490"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 75.718, "latencies_ms": [75.718], "images_per_second": 13.207, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Dog", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10460.8, "ram_available_mb": 112045.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10462.4, "ram_available_mb": 112043.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [35.55], "power_watts_avg": 35.55, "power_watts_peak": 35.55, "energy_joules_est": 2.7, "sample_count": 1, "duration_seconds": 0.076}, "timestamp": "2026-01-25T20:06:35.741021"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 140.055, "latencies_ms": [140.055], "images_per_second": 7.14, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nA dog holding a frisbee on its mouth is standing in front of an ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10462.4, "ram_available_mb": 112043.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10464.7, "ram_available_mb": 112041.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 57.0}, "power_stats": {"power_watts_samples": [35.55, 35.55], "power_watts_avg": 35.55, "power_watts_peak": 35.55, "energy_joules_est": 4.99, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T20:06:35.946769"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 163.541, "latencies_ms": [163.541], "images_per_second": 6.115, "prompt_tokens": 757, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\nA dog with a yellow frisbee in its mouth stands on a sandy beach, looking at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.7, "ram_available_mb": 112041.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 17.0}, "power_stats": {"power_watts_samples": [35.55, 35.55], "power_watts_avg": 35.55, "power_watts_peak": 35.55, "energy_joules_est": 5.82, "sample_count": 2, "duration_seconds": 0.164}, "timestamp": "2026-01-25T20:06:36.152073"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 131.333, "latencies_ms": [131.333], "images_per_second": 7.614, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n The dog is on a beach with the ocean in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 17.0}, "power_stats": {"power_watts_samples": [33.29, 33.29], "power_watts_avg": 33.29, "power_watts_peak": 33.29, "energy_joules_est": 4.39, "sample_count": 2, "duration_seconds": 0.132}, "timestamp": "2026-01-25T20:06:36.357920"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 335.049, "latencies_ms": [335.049], "images_per_second": 2.985, "prompt_tokens": 744, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA group of people, including a woman in military uniform and two women with bags, stand around a kitchen counter filled with various items such as bowls, cups, and bottles.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 17.0}, "power_stats": {"power_watts_samples": [33.29, 33.29, 33.29, 31.66], "power_watts_avg": 32.88, "power_watts_peak": 33.29, "energy_joules_est": 11.05, "sample_count": 4, "duration_seconds": 0.336}, "timestamp": "2026-01-25T20:06:36.767298"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 565.451, "latencies_ms": [565.451], "images_per_second": 1.768, "prompt_tokens": 759, "response_tokens_est": 101, "n_tiles": 1, "output_text": "\n 1. Refrigerator  2. Drawer 3. Counter 4. Drawer 5. Drawer 6. Drawer 7. Drawer 8. Drawer 9. Drawer 10. Drawer 11. Drawer 12. Drawer 13. Drawer 14. Drawer 15. Drawer 16. Drawer 17. Drawer 18. Drawer 19. Drawer 20. Drawer 21. Drawer 22. Drawer 23. Drawer 24. Drawer 25.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.3, "ram_available_mb": 112032.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [31.66, 31.66, 31.66, 31.66, 36.56, 36.56], "power_watts_avg": 33.29, "power_watts_peak": 36.56, "energy_joules_est": 18.85, "sample_count": 6, "duration_seconds": 0.566}, "timestamp": "2026-01-25T20:06:37.374389"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 144.976, "latencies_ms": [144.976], "images_per_second": 6.898, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n A large metal refrigerator is in front of a group of people standing around it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.56, 36.56], "power_watts_avg": 36.56, "power_watts_peak": 36.56, "energy_joules_est": 5.32, "sample_count": 2, "duration_seconds": 0.146}, "timestamp": "2026-01-25T20:06:37.579749"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 188.968, "latencies_ms": [188.968], "images_per_second": 5.292, "prompt_tokens": 757, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA group of people, including a woman in military uniform, are gathered around an open kitchen area with various cooking utensils.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.56, 35.72], "power_watts_avg": 36.14, "power_watts_peak": 36.56, "energy_joules_est": 6.84, "sample_count": 2, "duration_seconds": 0.189}, "timestamp": "2026-01-25T20:06:37.784086"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 340.187, "latencies_ms": [340.187], "images_per_second": 2.94, "prompt_tokens": 756, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\n The image shows a group of people standing in front of an open refrigerator. One person is holding a book and appears to be reading something out loud. There are several items inside the refrigerator, including a green bowl on top shelf and a white plastic container on the bottom shelf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10470.7, "ram_available_mb": 112035.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.72, 35.72, 35.72, 35.72], "power_watts_avg": 35.72, "power_watts_peak": 35.72, "energy_joules_est": 12.16, "sample_count": 4, "duration_seconds": 0.34}, "timestamp": "2026-01-25T20:06:38.189128"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 259.04, "latencies_ms": [259.04], "images_per_second": 3.86, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urns of soap and shampoo are on a shelf above the toilet in a bathroom with beige tiles.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.7, "ram_available_mb": 112035.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.22, 36.22, 36.22], "power_watts_avg": 36.22, "power_watts_peak": 36.22, "energy_joules_est": 9.42, "sample_count": 3, "duration_seconds": 0.26}, "timestamp": "2026-01-25T20:06:38.498831"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 97.671, "latencies_ms": [97.671], "images_per_second": 10.238, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Toilet paper roll", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.22], "power_watts_avg": 36.22, "power_watts_peak": 36.22, "energy_joules_est": 3.56, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T20:06:38.604040"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 109.709, "latencies_ms": [109.709], "images_per_second": 9.115, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "urn of toiletries on shelf above toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10464.8, "ram_available_mb": 112041.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.23, 37.23], "power_watts_avg": 37.23, "power_watts_peak": 37.23, "energy_joules_est": 4.09, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T20:06:38.809193"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 374.13, "latencies_ms": [374.13], "images_per_second": 2.673, "prompt_tokens": 757, "response_tokens_est": 63, "n_tiles": 1, "output_text": "\nThe image shows a bathroom with white tiles on the walls, a toilet, and a shelf above it holding various items. The shelf contains soap, shampoo, and other toiletries. A roll of toilet paper can be seen next to the toilet, and there are two towels hanging from a rack nearby.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.8, "ram_available_mb": 112041.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [37.23, 37.23, 37.23, 34.19], "power_watts_avg": 36.47, "power_watts_peak": 37.23, "energy_joules_est": 13.66, "sample_count": 4, "duration_seconds": 0.374}, "timestamp": "2026-01-25T20:06:39.214609"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 120.372, "latencies_ms": [120.372], "images_per_second": 8.308, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n The toilet is white and the bathroom has a beige color scheme.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.6, "ram_available_mb": 112039.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10471.9, "ram_available_mb": 112034.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.19, 34.19], "power_watts_avg": 34.19, "power_watts_peak": 34.19, "energy_joules_est": 4.14, "sample_count": 2, "duration_seconds": 0.121}, "timestamp": "2026-01-25T20:06:39.420580"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 140.982, "latencies_ms": [140.982], "images_per_second": 7.093, "prompt_tokens": 744, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.9, "ram_available_mb": 112034.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.19, 34.19], "power_watts_avg": 34.19, "power_watts_peak": 34.19, "energy_joules_est": 4.83, "sample_count": 2, "duration_seconds": 0.141}, "timestamp": "2026-01-25T20:06:39.628403"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 95.714, "latencies_ms": [95.714], "images_per_second": 10.448, "prompt_tokens": 759, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1. Green shamrock sticker - 2", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10473.4, "ram_available_mb": 112032.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.12], "power_watts_avg": 34.12, "power_watts_peak": 34.12, "energy_joules_est": 3.27, "sample_count": 1, "duration_seconds": 0.096}, "timestamp": "2026-01-25T20:06:39.732132"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 159.884, "latencies_ms": [159.884], "images_per_second": 6.255, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\nThe dog is wearing a green hat and sitting in the driver's seat of a car.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.4, "ram_available_mb": 112032.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10473.8, "ram_available_mb": 112032.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.12, 34.12], "power_watts_avg": 34.12, "power_watts_peak": 34.12, "energy_joules_est": 5.46, "sample_count": 2, "duration_seconds": 0.16}, "timestamp": "2026-01-25T20:06:39.937138"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 309.613, "latencies_ms": [309.613], "images_per_second": 3.23, "prompt_tokens": 757, "response_tokens_est": 52, "n_tiles": 1, "output_text": "\nIn this image, a person wearing a green hat is driving a silver truck with an Irish flag on top. A dog in a green hat is sitting in the passenger seat of the truck, looking out the window as they drive down the road together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.8, "ram_available_mb": 112032.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [34.12, 34.12, 33.35, 33.35], "power_watts_avg": 33.73, "power_watts_peak": 34.12, "energy_joules_est": 10.46, "sample_count": 4, "duration_seconds": 0.31}, "timestamp": "2026-01-25T20:06:40.345210"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 164.809, "latencies_ms": [164.809], "images_per_second": 6.068, "prompt_tokens": 756, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\n The image shows a person wearing a green hat and driving a silver truck with an Irish flag on the back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [33.35, 33.35], "power_watts_avg": 33.35, "power_watts_peak": 33.35, "energy_joules_est": 5.51, "sample_count": 2, "duration_seconds": 0.165}, "timestamp": "2026-01-25T20:06:40.550401"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 233.626, "latencies_ms": [233.626], "images_per_second": 4.28, "prompt_tokens": 744, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA large gray elephant stands in a water hole, surrounded by rocks and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10472.8, "ram_available_mb": 112033.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [33.35, 30.1, 30.1], "power_watts_avg": 31.19, "power_watts_peak": 33.35, "energy_joules_est": 7.29, "sample_count": 3, "duration_seconds": 0.234}, "timestamp": "2026-01-25T20:06:40.862686"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 88.898, "latencies_ms": [88.898], "images_per_second": 11.249, "prompt_tokens": 759, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n 1. Trunk of an elephant", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.8, "ram_available_mb": 112033.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10466.8, "ram_available_mb": 112039.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 25.0}, "power_stats": {"power_watts_samples": [30.1], "power_watts_avg": 30.1, "power_watts_peak": 30.1, "energy_joules_est": 2.69, "sample_count": 1, "duration_seconds": 0.089}, "timestamp": "2026-01-25T20:06:40.967049"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 110.117, "latencies_ms": [110.117], "images_per_second": 9.081, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n A large elephant standing in front of a smaller one.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10466.8, "ram_available_mb": 112039.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [30.1, 30.1], "power_watts_avg": 30.1, "power_watts_peak": 30.1, "energy_joules_est": 3.32, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T20:06:41.172328"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 274.646, "latencies_ms": [274.646], "images_per_second": 3.641, "prompt_tokens": 757, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nIn a zoo, an elephant stands in water with rocks surrounding its enclosure. The elephant appears to be drinking from the pond or watering hole. Several people are present around the area, observing the elephant's actions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10467.3, "ram_available_mb": 112039.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [34.7, 34.7, 34.7], "power_watts_avg": 34.7, "power_watts_peak": 34.7, "energy_joules_est": 9.54, "sample_count": 3, "duration_seconds": 0.275}, "timestamp": "2026-01-25T20:06:41.477767"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 100.571, "latencies_ms": [100.571], "images_per_second": 9.943, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The elephant is gray and brown.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.3, "ram_available_mb": 112039.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [34.7], "power_watts_avg": 34.7, "power_watts_peak": 34.7, "energy_joules_est": 3.5, "sample_count": 1, "duration_seconds": 0.101}, "timestamp": "2026-01-25T20:06:41.582677"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 245.114, "latencies_ms": [245.114], "images_per_second": 4.08, "prompt_tokens": 744, "response_tokens_est": 21, "n_tiles": 1, "output_text": "iced tea and a glass of water on a table, with four people standing in front of them.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10465.6, "ram_available_mb": 112040.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10467.8, "ram_available_mb": 112038.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 41.0}, "power_stats": {"power_watts_samples": [33.73, 33.73, 33.73], "power_watts_avg": 33.73, "power_watts_peak": 33.73, "energy_joules_est": 8.29, "sample_count": 3, "duration_seconds": 0.246}, "timestamp": "2026-01-25T20:06:41.896826"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 263.254, "latencies_ms": [263.254], "images_per_second": 3.799, "prompt_tokens": 759, "response_tokens_est": 42, "n_tiles": 1, "output_text": "\n 1. Skier in blue jacket and black pants 2. Skier in blue jacket and white pants 3. Skier in black pants and brown coat 4. Skier in black pants and blue coat", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.8, "ram_available_mb": 112038.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [33.73, 33.73, 35.19], "power_watts_avg": 34.22, "power_watts_peak": 35.19, "energy_joules_est": 9.02, "sample_count": 3, "duration_seconds": 0.264}, "timestamp": "2026-01-25T20:06:42.205418"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 178.792, "latencies_ms": [178.792], "images_per_second": 5.593, "prompt_tokens": 763, "response_tokens_est": 24, "n_tiles": 1, "output_text": "\n 1. Person in blue jacket and black pants standing next to a person wearing a white coat with brown pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [35.19, 35.19], "power_watts_avg": 35.19, "power_watts_peak": 35.19, "energy_joules_est": 6.32, "sample_count": 2, "duration_seconds": 0.179}, "timestamp": "2026-01-25T20:06:42.411573"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 224.565, "latencies_ms": [224.565], "images_per_second": 4.453, "prompt_tokens": 757, "response_tokens_est": 35, "n_tiles": 1, "output_text": "\nA group of four people are standing on a snowy mountain, all wearing ski gear. They have their skis on and appear to be posing for a photo together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.6, "ram_available_mb": 112035.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10470.8, "ram_available_mb": 112035.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 55.0}, "power_stats": {"power_watts_samples": [35.19, 35.19, 37.01], "power_watts_avg": 35.8, "power_watts_peak": 37.01, "energy_joules_est": 8.05, "sample_count": 3, "duration_seconds": 0.225}, "timestamp": "2026-01-25T20:06:42.717321"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 370.254, "latencies_ms": [370.254], "images_per_second": 2.701, "prompt_tokens": 756, "response_tokens_est": 65, "n_tiles": 1, "output_text": "\n The image shows a group of four people standing on skis in the snow. They are all wearing winter clothing and have their skis attached to their feet. One person is wearing a blue jacket while another has a black jacket. The sky above them appears clear with no clouds, suggesting good weather conditions for skiing.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10470.8, "ram_available_mb": 112035.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [37.01, 37.01, 37.01, 37.01], "power_watts_avg": 37.01, "power_watts_peak": 37.01, "energy_joules_est": 13.73, "sample_count": 4, "duration_seconds": 0.371}, "timestamp": "2026-01-25T20:06:43.124615"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 252.569, "latencies_ms": [252.569], "images_per_second": 3.959, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "iphone in a person's hand, displaying a photo of a keyboard and the time \"14:45\".", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10471.1, "ram_available_mb": 112035.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10473.4, "ram_available_mb": 112032.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.24, 35.24, 35.24], "power_watts_avg": 35.24, "power_watts_peak": 35.24, "energy_joules_est": 8.91, "sample_count": 3, "duration_seconds": 0.253}, "timestamp": "2026-01-25T20:06:43.439663"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 73.007, "latencies_ms": [73.007], "images_per_second": 13.697, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Cell phone", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10473.4, "ram_available_mb": 112032.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10475.4, "ram_available_mb": 112030.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.24], "power_watts_avg": 35.24, "power_watts_peak": 35.24, "energy_joules_est": 2.59, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T20:06:43.544831"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 119.157, "latencies_ms": [119.157], "images_per_second": 8.392, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nThe hand holding a cell phone is in front of a keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.4, "ram_available_mb": 112030.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10476.2, "ram_available_mb": 112030.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.24, 35.91], "power_watts_avg": 35.58, "power_watts_peak": 35.91, "energy_joules_est": 4.25, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T20:06:43.752102"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 224.322, "latencies_ms": [224.322], "images_per_second": 4.458, "prompt_tokens": 757, "response_tokens_est": 33, "n_tiles": 1, "output_text": "\nA person's hand holds a black smartphone displaying a photo of a keyboard. The phone appears to be turned on, with its screen visible in the image.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10476.2, "ram_available_mb": 112030.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10478.3, "ram_available_mb": 112028.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.91, 35.91, 35.91], "power_watts_avg": 35.91, "power_watts_peak": 35.91, "energy_joules_est": 8.07, "sample_count": 3, "duration_seconds": 0.225}, "timestamp": "2026-01-25T20:06:44.057938"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 129.742, "latencies_ms": [129.742], "images_per_second": 7.708, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The image shows a person holding an iPhone in their hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.3, "ram_available_mb": 112028.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [35.91, 32.03], "power_watts_avg": 33.97, "power_watts_peak": 35.91, "energy_joules_est": 4.42, "sample_count": 2, "duration_seconds": 0.13}, "timestamp": "2026-01-25T20:06:44.263610"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 282.322, "latencies_ms": [282.322], "images_per_second": 3.542, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA red and white parking meter is located on a sidewalk next to a sign that says \" Denver's Road Home\".", "error": null, "sys_before": {"cpu_percent": 4.2, "ram_used_mb": 10469.9, "ram_available_mb": 112036.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [32.03, 32.03, 32.03], "power_watts_avg": 32.03, "power_watts_peak": 32.03, "energy_joules_est": 9.05, "sample_count": 3, "duration_seconds": 0.283}, "timestamp": "2026-01-25T20:06:44.578578"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 227.853, "latencies_ms": [227.853], "images_per_second": 4.389, "prompt_tokens": 759, "response_tokens_est": 34, "n_tiles": 1, "output_text": "\n 1. Parking meter  2. Sign  3. Plant  4. Plant  5. Plant  6. Plant  7. Plant  8. Plant", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 65.0}, "power_stats": {"power_watts_samples": [32.03, 32.72, 32.72], "power_watts_avg": 32.49, "power_watts_peak": 32.72, "energy_joules_est": 7.42, "sample_count": 3, "duration_seconds": 0.228}, "timestamp": "2026-01-25T20:06:44.886206"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 152.29, "latencies_ms": [152.29], "images_per_second": 6.566, "prompt_tokens": 763, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nA parking meter is located next to a sign that says \"can't park here\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [32.72, 32.72], "power_watts_avg": 32.72, "power_watts_peak": 32.72, "energy_joules_est": 5.0, "sample_count": 2, "duration_seconds": 0.153}, "timestamp": "2026-01-25T20:06:45.092841"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 196.938, "latencies_ms": [196.938], "images_per_second": 5.078, "prompt_tokens": 757, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA red, white, and blue parking meter stands on a sidewalk next to a sign that says \"Homelessness ends here\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [33.52, 33.52], "power_watts_avg": 33.52, "power_watts_peak": 33.52, "energy_joules_est": 6.61, "sample_count": 2, "duration_seconds": 0.197}, "timestamp": "2026-01-25T20:06:45.297611"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 98.73, "latencies_ms": [98.73], "images_per_second": 10.129, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\nThe parking meter is red and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10473.9, "ram_available_mb": 112032.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 61.0}, "power_stats": {"power_watts_samples": [33.52], "power_watts_avg": 33.52, "power_watts_peak": 33.52, "energy_joules_est": 3.32, "sample_count": 1, "duration_seconds": 0.099}, "timestamp": "2026-01-25T20:06:45.401246"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 770.035, "latencies_ms": [770.035], "images_per_second": 1.299, "prompt_tokens": 744, "response_tokens_est": 123, "n_tiles": 1, "output_text": "\nIn a grassy field, four zebras are grazing peacefully. The zebra in the center is facing away from the camera and appears to be eating some grass. Two more zebras can be seen on either side of this central zebra, with their backs turned towards the camera. They seem to be enjoying the serene surroundings as well.\n\nThe field they are grazing in has a mix of green and brown hues, suggesting that it might be autumn or winter season. The zebras' black and white stripes contrast beautifully against this backdrop, making them stand out even more.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.9, "ram_available_mb": 112032.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.52, 33.52, 34.97, 34.97, 34.97, 34.97, 34.97, 41.69], "power_watts_avg": 35.45, "power_watts_peak": 41.69, "energy_joules_est": 27.3, "sample_count": 8, "duration_seconds": 0.77}, "timestamp": "2026-01-25T20:06:46.214026"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 94.777, "latencies_ms": [94.777], "images_per_second": 10.551, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Zebra in field", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.2, "ram_available_mb": 112039.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 10464.7, "ram_available_mb": 112041.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [41.69], "power_watts_avg": 41.69, "power_watts_peak": 41.69, "energy_joules_est": 3.96, "sample_count": 1, "duration_seconds": 0.095}, "timestamp": "2026-01-25T20:06:46.318636"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 130.833, "latencies_ms": [130.833], "images_per_second": 7.643, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. Zebra eating grass in front of other zebras.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.7, "ram_available_mb": 112041.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10467.3, "ram_available_mb": 112039.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [41.69, 41.69], "power_watts_avg": 41.69, "power_watts_peak": 41.69, "energy_joules_est": 5.46, "sample_count": 2, "duration_seconds": 0.131}, "timestamp": "2026-01-25T20:06:46.523048"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 296.913, "latencies_ms": [296.913], "images_per_second": 3.368, "prompt_tokens": 757, "response_tokens_est": 47, "n_tiles": 1, "output_text": "\nIn a grassy field, four zebras are grazing peacefully. One zebra is bending down to eat some grass while the other three stand nearby. The background features trees in the distance, adding depth to the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.3, "ram_available_mb": 112039.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 10473.5, "ram_available_mb": 112032.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [41.69, 35.98, 35.98], "power_watts_avg": 37.88, "power_watts_peak": 41.69, "energy_joules_est": 11.26, "sample_count": 3, "duration_seconds": 0.297}, "timestamp": "2026-01-25T20:06:46.828686"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 330.123, "latencies_ms": [330.123], "images_per_second": 3.029, "prompt_tokens": 756, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\n The image features a group of zebras grazing in the grass. One zebra is eating while another stands nearby with its head down and ears perked up. The background includes trees and bushes, providing a natural setting for these animals as they enjoy their meal together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.5, "ram_available_mb": 112032.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10474.4, "ram_available_mb": 112031.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.98, 35.98, 35.98, 34.52], "power_watts_avg": 35.62, "power_watts_peak": 35.98, "energy_joules_est": 11.77, "sample_count": 4, "duration_seconds": 0.33}, "timestamp": "2026-01-25T20:06:47.235093"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 296.126, "latencies_ms": [296.126], "images_per_second": 3.377, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA surfer in a black wetsuit is skillfully riding a wave on their surfboard, with arms outstretched for balance and control.", "error": null, "sys_before": {"cpu_percent": 4.5, "ram_used_mb": 10474.4, "ram_available_mb": 112031.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.52, 34.52, 34.52], "power_watts_avg": 34.52, "power_watts_peak": 34.52, "energy_joules_est": 10.24, "sample_count": 3, "duration_seconds": 0.297}, "timestamp": "2026-01-25T20:06:47.547224"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 141.615, "latencies_ms": [141.615], "images_per_second": 7.061, "prompt_tokens": 759, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\n [0.39, 0.13, 0.87, 0.46]", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10464.0, "ram_available_mb": 112042.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.52, 39.86], "power_watts_avg": 37.19, "power_watts_peak": 39.86, "energy_joules_est": 5.28, "sample_count": 2, "duration_seconds": 0.142}, "timestamp": "2026-01-25T20:06:47.751844"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 350.749, "latencies_ms": [350.749], "images_per_second": 2.851, "prompt_tokens": 763, "response_tokens_est": 59, "n_tiles": 1, "output_text": "\nThe image shows a surfer riding a wave on top of his surfboard. The surfer is positioned in the center of the frame and appears to be skillfully navigating the wave. In the background, there are other people present but they are not the main focus of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10461.5, "ram_available_mb": 112044.8, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10463.6, "ram_available_mb": 112042.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [39.86, 39.86, 39.86, 39.86], "power_watts_avg": 39.86, "power_watts_peak": 39.86, "energy_joules_est": 14.0, "sample_count": 4, "duration_seconds": 0.351}, "timestamp": "2026-01-25T20:06:48.158555"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 202.177, "latencies_ms": [202.177], "images_per_second": 4.946, "prompt_tokens": 757, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA surfer in a black wetsuit rides a wave on their surfboard, skillfully navigating the greenish-blue ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10463.6, "ram_available_mb": 112042.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [38.25, 38.25, 38.25], "power_watts_avg": 38.25, "power_watts_peak": 38.25, "energy_joules_est": 7.74, "sample_count": 3, "duration_seconds": 0.202}, "timestamp": "2026-01-25T20:06:48.464145"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 396.938, "latencies_ms": [396.938], "images_per_second": 2.519, "prompt_tokens": 756, "response_tokens_est": 70, "n_tiles": 1, "output_text": "\n The image shows a surfer riding a wave on top of his surfboard. He is wearing a black wetsuit and appears to be in the process of turning around a corner while surfing. The ocean water surrounding him has a greenish hue, suggesting that it might be a cloudy day or there could be some algae blooms present.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10470.7, "ram_available_mb": 112035.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [38.25, 38.25, 33.08, 33.08], "power_watts_avg": 35.67, "power_watts_peak": 38.25, "energy_joules_est": 14.17, "sample_count": 4, "duration_seconds": 0.397}, "timestamp": "2026-01-25T20:06:48.869784"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 265.164, "latencies_ms": [265.164], "images_per_second": 3.771, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nTwo skiers, dressed in white and black jackets, are preparing to ski down a snowy mountain slope at sunset.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.7, "ram_available_mb": 112035.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10467.3, "ram_available_mb": 112039.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [33.08, 33.08, 33.08], "power_watts_avg": 33.08, "power_watts_peak": 33.08, "energy_joules_est": 8.78, "sample_count": 3, "duration_seconds": 0.265}, "timestamp": "2026-01-25T20:06:49.178574"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 67.515, "latencies_ms": [67.515], "images_per_second": 14.812, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Skis", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.3, "ram_available_mb": 112039.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [37.83], "power_watts_avg": 37.83, "power_watts_peak": 37.83, "energy_joules_est": 2.57, "sample_count": 1, "duration_seconds": 0.068}, "timestamp": "2026-01-25T20:06:49.283835"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 160.461, "latencies_ms": [160.461], "images_per_second": 6.232, "prompt_tokens": 763, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\nThe skier holding his ski poles is in front of another skier who is bending over.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.1, "ram_available_mb": 112038.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [37.83, 37.83], "power_watts_avg": 37.83, "power_watts_peak": 37.83, "energy_joules_est": 6.08, "sample_count": 2, "duration_seconds": 0.161}, "timestamp": "2026-01-25T20:06:49.488921"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 157.008, "latencies_ms": [157.008], "images_per_second": 6.369, "prompt_tokens": 757, "response_tokens_est": 21, "n_tiles": 1, "output_text": "\n Two people are standing on a snowy mountain, one holding skis while wearing a white jacket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10467.7, "ram_available_mb": 112038.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10469.4, "ram_available_mb": 112036.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [37.83, 37.62], "power_watts_avg": 37.72, "power_watts_peak": 37.83, "energy_joules_est": 5.93, "sample_count": 2, "duration_seconds": 0.157}, "timestamp": "2026-01-25T20:06:49.694687"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 142.318, "latencies_ms": [142.318], "images_per_second": 7.027, "prompt_tokens": 756, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\n The sun is setting behind the mountain and casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.4, "ram_available_mb": 112036.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 83.0}, "power_stats": {"power_watts_samples": [37.62, 37.62], "power_watts_avg": 37.62, "power_watts_peak": 37.62, "energy_joules_est": 5.38, "sample_count": 2, "duration_seconds": 0.143}, "timestamp": "2026-01-25T20:06:49.900082"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 376.986, "latencies_ms": [376.986], "images_per_second": 2.653, "prompt_tokens": 744, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nA young baseball player in a red and white uniform stands at home plate, holding a bat and ready to swing. Behind him, an umpire and catcher are crouched behind the batter, prepared for action.", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 10474.4, "ram_available_mb": 112031.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [37.62, 37.62, 32.49, 32.49], "power_watts_avg": 35.05, "power_watts_peak": 37.62, "energy_joules_est": 13.23, "sample_count": 4, "duration_seconds": 0.377}, "timestamp": "2026-01-25T20:06:50.311470"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 80.394, "latencies_ms": [80.394], "images_per_second": 12.439, "prompt_tokens": 759, "response_tokens_est": 8, "n_tiles": 1, "output_text": "\n 1. Batter: 2", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10474.4, "ram_available_mb": 112031.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10474.6, "ram_available_mb": 112031.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [32.49], "power_watts_avg": 32.49, "power_watts_peak": 32.49, "energy_joules_est": 2.62, "sample_count": 1, "duration_seconds": 0.081}, "timestamp": "2026-01-25T20:06:50.416663"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 72.557, "latencies_ms": [72.557], "images_per_second": 13.782, "prompt_tokens": 763, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Catcher", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.6, "ram_available_mb": 112031.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [32.49], "power_watts_avg": 32.49, "power_watts_peak": 32.49, "energy_joules_est": 2.37, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T20:06:50.521589"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 362.306, "latencies_ms": [362.306], "images_per_second": 2.76, "prompt_tokens": 757, "response_tokens_est": 59, "n_tiles": 1, "output_text": "\nA young baseball player in a red jersey stands at home plate, holding a bat and waiting for the pitch. The catcher and umpire are crouched behind him, ready to react to the ball's trajectory. Spectators can be seen sitting on chairs nearby, watching the game unfold.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10475.8, "ram_available_mb": 112030.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [32.49, 33.62, 33.62, 33.62], "power_watts_avg": 33.34, "power_watts_peak": 33.62, "energy_joules_est": 12.1, "sample_count": 4, "duration_seconds": 0.363}, "timestamp": "2026-01-25T20:06:50.927171"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 135.501, "latencies_ms": [135.501], "images_per_second": 7.38, "prompt_tokens": 756, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n 1. A baseball player in a red and white uniform swinging his bat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10475.6, "ram_available_mb": 112030.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10478.6, "ram_available_mb": 112027.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [33.62, 33.62], "power_watts_avg": 33.62, "power_watts_peak": 33.62, "energy_joules_est": 4.57, "sample_count": 2, "duration_seconds": 0.136}, "timestamp": "2026-01-25T20:06:51.133453"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 232.45, "latencies_ms": [232.45], "images_per_second": 4.302, "prompt_tokens": 744, "response_tokens_est": 18, "n_tiles": 1, "output_text": "iced coffee with whipped cream and a slice of cake on a table in a restaurant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.6, "ram_available_mb": 112027.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10478.6, "ram_available_mb": 112027.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [34.15, 34.15, 34.15], "power_watts_avg": 34.15, "power_watts_peak": 34.15, "energy_joules_est": 7.94, "sample_count": 3, "duration_seconds": 0.233}, "timestamp": "2026-01-25T20:06:51.443613"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 126.783, "latencies_ms": [126.783], "images_per_second": 7.887, "prompt_tokens": 759, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n Glass of chocolate shake and fork, white plate with cake, silverware", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.6, "ram_available_mb": 112027.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [34.15, 34.15], "power_watts_avg": 34.15, "power_watts_peak": 34.15, "energy_joules_est": 4.34, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:06:51.649182"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 94.746, "latencies_ms": [94.746], "images_per_second": 10.555, "prompt_tokens": 763, "response_tokens_est": 10, "n_tiles": 1, "output_text": "iced coffee and cake on a table.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.5, "ram_available_mb": 112031.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 10476.9, "ram_available_mb": 112029.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [36.28], "power_watts_avg": 36.28, "power_watts_peak": 36.28, "energy_joules_est": 3.45, "sample_count": 1, "duration_seconds": 0.095}, "timestamp": "2026-01-25T20:06:51.753769"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 268.729, "latencies_ms": [268.729], "images_per_second": 3.721, "prompt_tokens": 757, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nA slice of cake with whipped cream sits on a white plate, accompanied by a glass of chocolate milkshake. The table is set in a restaurant or cafe environment, with other tables visible in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 10476.9, "ram_available_mb": 112029.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10479.0, "ram_available_mb": 112027.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [36.28, 36.28, 36.28], "power_watts_avg": 36.28, "power_watts_peak": 36.28, "energy_joules_est": 9.76, "sample_count": 3, "duration_seconds": 0.269}, "timestamp": "2026-01-25T20:06:52.060491"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 94.199, "latencies_ms": [94.199], "images_per_second": 10.616, "prompt_tokens": 756, "response_tokens_est": 10, "n_tiles": 1, "output_text": "iced coffee and cake on a table.\n", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.0, "ram_available_mb": 112027.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [36.28], "power_watts_avg": 36.28, "power_watts_peak": 36.28, "energy_joules_est": 3.43, "sample_count": 1, "duration_seconds": 0.095}, "timestamp": "2026-01-25T20:06:52.164846"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 261.28, "latencies_ms": [261.28], "images_per_second": 3.827, "prompt_tokens": 744, "response_tokens_est": 24, "n_tiles": 1, "output_text": "urn of flowers sits on a table with a three-tiered wedding cake, featuring gold and white icing designs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.0, 33.0, 33.0], "power_watts_avg": 33.0, "power_watts_peak": 33.0, "energy_joules_est": 8.63, "sample_count": 3, "duration_seconds": 0.261}, "timestamp": "2026-01-25T20:06:52.474725"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 86.907, "latencies_ms": [86.907], "images_per_second": 11.507, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Cake: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10483.1, "ram_available_mb": 112023.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.0], "power_watts_avg": 33.0, "power_watts_peak": 33.0, "energy_joules_est": 2.89, "sample_count": 1, "duration_seconds": 0.088}, "timestamp": "2026-01-25T20:06:52.580703"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 84.505, "latencies_ms": [84.505], "images_per_second": 11.834, "prompt_tokens": 763, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urns on tables in background", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.1, "ram_available_mb": 112023.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 10486.8, "ram_available_mb": 112019.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.0], "power_watts_avg": 33.0, "power_watts_peak": 33.0, "energy_joules_est": 2.8, "sample_count": 1, "duration_seconds": 0.085}, "timestamp": "2026-01-25T20:06:52.684905"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 366.689, "latencies_ms": [366.689], "images_per_second": 2.727, "prompt_tokens": 757, "response_tokens_est": 60, "n_tiles": 1, "output_text": "\nThe image shows a three-tiered wedding cake with gold accents, placed on a table covered in blue fabric. The cake has white icing and is adorned with flowers and other decorations. In the background, there are several tables set up for guests to enjoy food and drinks at an event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10486.8, "ram_available_mb": 112019.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10484.8, "ram_available_mb": 112021.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.52, 34.52, 34.52, 34.52], "power_watts_avg": 34.52, "power_watts_peak": 34.52, "energy_joules_est": 12.66, "sample_count": 4, "duration_seconds": 0.367}, "timestamp": "2026-01-25T20:06:53.092275"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 207.358, "latencies_ms": [207.358], "images_per_second": 4.823, "prompt_tokens": 756, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\n The cake is white with gold accents and has a statue of a woman on top. It's placed on a table covered in blue cloths.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.8, "ram_available_mb": 112021.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10482.2, "ram_available_mb": 112024.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.52, 36.52, 36.52], "power_watts_avg": 36.52, "power_watts_peak": 36.52, "energy_joules_est": 7.58, "sample_count": 3, "duration_seconds": 0.208}, "timestamp": "2026-01-25T20:06:53.399219"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 237.662, "latencies_ms": [237.662], "images_per_second": 4.208, "prompt_tokens": 744, "response_tokens_est": 18, "n_tiles": 1, "output_text": "urn of food on a stove, wearing a blue sweater with red and white stripes.", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 10482.2, "ram_available_mb": 112024.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10480.0, "ram_available_mb": 112026.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.52, 36.52, 35.94], "power_watts_avg": 36.33, "power_watts_peak": 36.52, "energy_joules_est": 8.65, "sample_count": 3, "duration_seconds": 0.238}, "timestamp": "2026-01-25T20:06:53.708805"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 78.365, "latencies_ms": [78.365], "images_per_second": 12.761, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Pot lid", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.0, "ram_available_mb": 112026.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10480.3, "ram_available_mb": 112026.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.94], "power_watts_avg": 35.94, "power_watts_peak": 35.94, "energy_joules_est": 2.83, "sample_count": 1, "duration_seconds": 0.079}, "timestamp": "2026-01-25T20:06:53.814112"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 134.27, "latencies_ms": [134.27], "images_per_second": 7.448, "prompt_tokens": 763, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA woman in a blue sweater is cooking food on top of an electric stove.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.3, "ram_available_mb": 112026.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10480.1, "ram_available_mb": 112026.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.94, 35.94], "power_watts_avg": 35.94, "power_watts_peak": 35.94, "energy_joules_est": 4.84, "sample_count": 2, "duration_seconds": 0.135}, "timestamp": "2026-01-25T20:06:54.019288"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 290.125, "latencies_ms": [290.125], "images_per_second": 3.447, "prompt_tokens": 757, "response_tokens_est": 48, "n_tiles": 1, "output_text": "\nA woman in a blue sweater stands at a kitchen counter, holding a plate of food. She is cooking something on the stovetop using a pot. The kitchen has various items such as pots, pans, and bottles scattered around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.1, "ram_available_mb": 112026.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10481.7, "ram_available_mb": 112024.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [35.94, 31.92, 31.92], "power_watts_avg": 33.26, "power_watts_peak": 35.94, "energy_joules_est": 9.66, "sample_count": 3, "duration_seconds": 0.29}, "timestamp": "2026-01-25T20:06:54.323428"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 106.62, "latencies_ms": [106.62], "images_per_second": 9.379, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\nThe woman is wearing a blue sweater and gray pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.7, "ram_available_mb": 112024.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10478.3, "ram_available_mb": 112028.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [31.92, 31.92], "power_watts_avg": 31.92, "power_watts_peak": 31.92, "energy_joules_est": 3.41, "sample_count": 2, "duration_seconds": 0.107}, "timestamp": "2026-01-25T20:06:54.528019"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 272.916, "latencies_ms": [272.916], "images_per_second": 3.664, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA woman in a pink shirt and black pants stands next to her white horse, holding its reins with both hands.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10478.3, "ram_available_mb": 112028.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [31.92, 29.16, 29.16], "power_watts_avg": 30.08, "power_watts_peak": 31.92, "energy_joules_est": 8.22, "sample_count": 3, "duration_seconds": 0.273}, "timestamp": "2026-01-25T20:06:54.837758"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 85.632, "latencies_ms": [85.632], "images_per_second": 11.678, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Horse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10477.5, "ram_available_mb": 112028.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 40.0}, "power_stats": {"power_watts_samples": [29.16], "power_watts_avg": 29.16, "power_watts_peak": 29.16, "energy_joules_est": 2.51, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T20:06:54.942326"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 119.978, "latencies_ms": [119.978], "images_per_second": 8.335, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nThe woman is holding a rope and walking next to a horse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.5, "ram_available_mb": 112028.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10479.0, "ram_available_mb": 112027.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [29.16, 29.16], "power_watts_avg": 29.16, "power_watts_peak": 29.16, "energy_joules_est": 3.51, "sample_count": 2, "duration_seconds": 0.12}, "timestamp": "2026-01-25T20:06:55.148231"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 193.681, "latencies_ms": [193.681], "images_per_second": 5.163, "prompt_tokens": 757, "response_tokens_est": 28, "n_tiles": 1, "output_text": "\nA woman in a pink shirt stands next to her white horse, holding its reins. The horse appears calm and well-trained.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.0, "ram_available_mb": 112027.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10480.3, "ram_available_mb": 112026.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [35.85, 35.85], "power_watts_avg": 35.85, "power_watts_peak": 35.85, "energy_joules_est": 6.96, "sample_count": 2, "duration_seconds": 0.194}, "timestamp": "2026-01-25T20:06:55.353592"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 121.494, "latencies_ms": [121.494], "images_per_second": 8.231, "prompt_tokens": 756, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n The woman is wearing a pink shirt and black pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.3, "ram_available_mb": 112026.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [35.85, 35.85], "power_watts_avg": 35.85, "power_watts_peak": 35.85, "energy_joules_est": 4.36, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:06:55.558979"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 261.665, "latencies_ms": [261.665], "images_per_second": 3.822, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "\nA street scene in a city with tall buildings, cars driving down it and traffic lights at an intersection.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.7, "ram_available_mb": 112033.6, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [35.85, 33.08, 33.08], "power_watts_avg": 34.0, "power_watts_peak": 35.85, "energy_joules_est": 8.93, "sample_count": 3, "duration_seconds": 0.262}, "timestamp": "2026-01-25T20:06:55.869601"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 76.248, "latencies_ms": [76.248], "images_per_second": 13.115, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Traffic light", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.0, "ram_available_mb": 112033.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10472.8, "ram_available_mb": 112033.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 39.0}, "power_stats": {"power_watts_samples": [33.08], "power_watts_avg": 33.08, "power_watts_peak": 33.08, "energy_joules_est": 2.54, "sample_count": 1, "duration_seconds": 0.077}, "timestamp": "2026-01-25T20:06:55.975029"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 163.371, "latencies_ms": [163.371], "images_per_second": 6.121, "prompt_tokens": 763, "response_tokens_est": 22, "n_tiles": 1, "output_text": "\n 1. A street sign with a yellow diamond shape and black letters reading 7am - 7pm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10473.6, "ram_available_mb": 112032.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [33.08, 33.08], "power_watts_avg": 33.08, "power_watts_peak": 33.08, "energy_joules_est": 5.42, "sample_count": 2, "duration_seconds": 0.164}, "timestamp": "2026-01-25T20:06:56.180303"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 140.131, "latencies_ms": [140.131], "images_per_second": 7.136, "prompt_tokens": 757, "response_tokens_est": 18, "n_tiles": 1, "output_text": "\nA city street with a black car driving down it, surrounded by tall buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.6, "ram_available_mb": 112032.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10474.1, "ram_available_mb": 112032.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [32.41, 32.41], "power_watts_avg": 32.41, "power_watts_peak": 32.41, "energy_joules_est": 4.55, "sample_count": 2, "duration_seconds": 0.14}, "timestamp": "2026-01-25T20:06:56.385712"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 101.934, "latencies_ms": [101.934], "images_per_second": 9.81, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The street is grey and wet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.1, "ram_available_mb": 112032.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10472.4, "ram_available_mb": 112033.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [32.41, 32.41], "power_watts_avg": 32.41, "power_watts_peak": 32.41, "energy_joules_est": 3.32, "sample_count": 2, "duration_seconds": 0.102}, "timestamp": "2026-01-25T20:06:56.590875"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 271.5, "latencies_ms": [271.5], "images_per_second": 3.683, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "urn with a toilet seat up, silver and black color, and a blue brush on top of it in a bathroom.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 10472.4, "ram_available_mb": 112033.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [31.84, 31.84, 31.84], "power_watts_avg": 31.83, "power_watts_peak": 31.84, "energy_joules_est": 8.66, "sample_count": 3, "duration_seconds": 0.272}, "timestamp": "2026-01-25T20:06:56.899920"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 97.634, "latencies_ms": [97.634], "images_per_second": 10.242, "prompt_tokens": 759, "response_tokens_est": 10, "n_tiles": 1, "output_text": "\n 1 toilet bowl cleaner  2 blue brush", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10469.4, "ram_available_mb": 112036.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 30.0}, "power_stats": {"power_watts_samples": [31.84], "power_watts_avg": 31.84, "power_watts_peak": 31.84, "energy_joules_est": 3.12, "sample_count": 1, "duration_seconds": 0.098}, "timestamp": "2026-01-25T20:06:57.004240"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 95.58, "latencies_ms": [95.58], "images_per_second": 10.462, "prompt_tokens": 763, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urn with lid open on floor", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.4, "ram_available_mb": 112036.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10474.6, "ram_available_mb": 112031.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [31.84], "power_watts_avg": 31.84, "power_watts_peak": 31.84, "energy_joules_est": 3.05, "sample_count": 1, "duration_seconds": 0.096}, "timestamp": "2026-01-25T20:06:57.107940"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 113.773, "latencies_ms": [113.773], "images_per_second": 8.789, "prompt_tokens": 757, "response_tokens_est": 12, "n_tiles": 1, "output_text": "urn with a hole in it, toilet seat up.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.6, "ram_available_mb": 112031.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10474.6, "ram_available_mb": 112031.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [31.66, 31.66], "power_watts_avg": 31.66, "power_watts_peak": 31.66, "energy_joules_est": 3.61, "sample_count": 2, "duration_seconds": 0.114}, "timestamp": "2026-01-25T20:06:57.311688"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 105.635, "latencies_ms": [105.635], "images_per_second": 9.467, "prompt_tokens": 756, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n The toilet is silver and has a black seat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.6, "ram_available_mb": 112031.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [31.66, 31.66], "power_watts_avg": 31.66, "power_watts_peak": 31.66, "energy_joules_est": 3.35, "sample_count": 2, "duration_seconds": 0.106}, "timestamp": "2026-01-25T20:06:57.516707"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 275.607, "latencies_ms": [275.607], "images_per_second": 3.628, "prompt_tokens": 744, "response_tokens_est": 25, "n_tiles": 1, "output_text": "\nA pink bicycle is parked in a room with white walls and wooden floors, surrounded by other bicycles of various colors.", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 10475.0, "ram_available_mb": 112031.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [31.66, 30.69, 30.69], "power_watts_avg": 31.01, "power_watts_peak": 31.66, "energy_joules_est": 8.56, "sample_count": 3, "duration_seconds": 0.276}, "timestamp": "2026-01-25T20:06:57.827189"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 88.433, "latencies_ms": [88.433], "images_per_second": 11.308, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bicycle in background", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.7, "ram_available_mb": 112029.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10477.9, "ram_available_mb": 112028.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 49.0}, "power_stats": {"power_watts_samples": [30.69], "power_watts_avg": 30.69, "power_watts_peak": 30.69, "energy_joules_est": 2.73, "sample_count": 1, "duration_seconds": 0.089}, "timestamp": "2026-01-25T20:06:57.932961"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 121.473, "latencies_ms": [121.473], "images_per_second": 8.232, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n 1. Pink bicycle in front of a wall with bicycles on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.9, "ram_available_mb": 112028.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10478.9, "ram_available_mb": 112027.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [30.69, 30.69], "power_watts_avg": 30.69, "power_watts_peak": 30.69, "energy_joules_est": 3.74, "sample_count": 2, "duration_seconds": 0.122}, "timestamp": "2026-01-25T20:06:58.138873"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 251.885, "latencies_ms": [251.885], "images_per_second": 3.97, "prompt_tokens": 757, "response_tokens_est": 42, "n_tiles": 1, "output_text": "\nA pink bicycle with a flower wreath on its handlebars is parked in front of a store window, drawing attention to itself. The bike appears to be stationary, possibly for sale or display purposes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.9, "ram_available_mb": 112027.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 10481.8, "ram_available_mb": 112024.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [31.47, 31.47, 31.47], "power_watts_avg": 31.47, "power_watts_peak": 31.47, "energy_joules_est": 7.95, "sample_count": 3, "duration_seconds": 0.253}, "timestamp": "2026-01-25T20:06:58.445435"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 133.594, "latencies_ms": [133.594], "images_per_second": 7.485, "prompt_tokens": 756, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\n The pink bicycle is parked in a room with white walls and wooden floors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.8, "ram_available_mb": 112024.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10479.3, "ram_available_mb": 112027.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [31.47, 31.47], "power_watts_avg": 31.47, "power_watts_peak": 31.47, "energy_joules_est": 4.21, "sample_count": 2, "duration_seconds": 0.134}, "timestamp": "2026-01-25T20:06:58.651095"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 390.923, "latencies_ms": [390.923], "images_per_second": 2.558, "prompt_tokens": 744, "response_tokens_est": 49, "n_tiles": 1, "output_text": "\nA giraffe stands tall in a grassy field, its long neck reaching towards the sky as it faces to the right side of the photo. The background features trees and bushes, creating a serene natural setting for this majestic creature.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 10479.3, "ram_available_mb": 112027.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 10478.0, "ram_available_mb": 112028.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [35.89, 35.89, 35.89, 35.89], "power_watts_avg": 35.89, "power_watts_peak": 35.89, "energy_joules_est": 14.04, "sample_count": 4, "duration_seconds": 0.391}, "timestamp": "2026-01-25T20:06:59.061272"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 72.157, "latencies_ms": [72.157], "images_per_second": 13.859, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n 1. Tree", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.0, "ram_available_mb": 112028.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10477.8, "ram_available_mb": 112028.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.89], "power_watts_avg": 35.89, "power_watts_peak": 35.89, "energy_joules_est": 2.59, "sample_count": 1, "duration_seconds": 0.072}, "timestamp": "2026-01-25T20:06:59.165169"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 126.049, "latencies_ms": [126.049], "images_per_second": 7.933, "prompt_tokens": 763, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\nThe giraffe is standing in a field with trees and bushes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.8, "ram_available_mb": 112028.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10478.2, "ram_available_mb": 112028.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.81, 35.81], "power_watts_avg": 35.81, "power_watts_peak": 35.81, "energy_joules_est": 4.54, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:06:59.371653"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 297.522, "latencies_ms": [297.522], "images_per_second": 3.361, "prompt_tokens": 757, "response_tokens_est": 49, "n_tiles": 1, "output_text": "\nA giraffe stands in a field, facing away from the camera. The sky above is cloudy, casting an overcast light on the scene below. In the background, there are trees that add to the natural landscape of the area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.2, "ram_available_mb": 112028.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10481.4, "ram_available_mb": 112024.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.81, 35.81, 35.81], "power_watts_avg": 35.81, "power_watts_peak": 35.81, "energy_joules_est": 10.67, "sample_count": 3, "duration_seconds": 0.298}, "timestamp": "2026-01-25T20:06:59.676981"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 130.187, "latencies_ms": [130.187], "images_per_second": 7.681, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The giraffe is brown and white with a tan belly.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.4, "ram_available_mb": 112024.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10481.4, "ram_available_mb": 112024.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [35.42, 35.42], "power_watts_avg": 35.42, "power_watts_peak": 35.42, "energy_joules_est": 4.62, "sample_count": 2, "duration_seconds": 0.13}, "timestamp": "2026-01-25T20:06:59.882846"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 319.418, "latencies_ms": [319.418], "images_per_second": 3.131, "prompt_tokens": 744, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nTwo children, a boy and a girl, sit on a luggage cart in front of a blue car parked next to a building with a \"Budgend\" sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10481.4, "ram_available_mb": 112024.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10480.4, "ram_available_mb": 112025.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_watts_samples": [35.42, 35.42, 34.5, 34.5], "power_watts_avg": 34.96, "power_watts_peak": 35.42, "energy_joules_est": 11.19, "sample_count": 4, "duration_seconds": 0.32}, "timestamp": "2026-01-25T20:07:00.293367"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 103.72, "latencies_ms": [103.72], "images_per_second": 9.641, "prompt_tokens": 759, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\n 1. Car in front of girl - 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.4, "ram_available_mb": 112025.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10480.2, "ram_available_mb": 112026.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_watts_samples": [34.5, 34.5], "power_watts_avg": 34.5, "power_watts_peak": 34.5, "energy_joules_est": 3.59, "sample_count": 2, "duration_seconds": 0.104}, "timestamp": "2026-01-25T20:07:00.499334"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 148.223, "latencies_ms": [148.223], "images_per_second": 6.747, "prompt_tokens": 763, "response_tokens_est": 20, "n_tiles": 1, "output_text": "\n 1. A girl in a pink shirt sitting on top of a suitcase with her brother.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.9, "ram_available_mb": 112026.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10480.1, "ram_available_mb": 112026.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_watts_samples": [34.5, 33.11], "power_watts_avg": 33.8, "power_watts_peak": 34.5, "energy_joules_est": 5.03, "sample_count": 2, "duration_seconds": 0.149}, "timestamp": "2026-01-25T20:07:00.705288"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 229.792, "latencies_ms": [229.792], "images_per_second": 4.352, "prompt_tokens": 757, "response_tokens_est": 36, "n_tiles": 1, "output_text": "\nA young girl in a pink shirt sits on top of a suitcase, accompanied by two children. They are being pushed along a cart through a parking lot filled with cars.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.1, "ram_available_mb": 112026.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10477.4, "ram_available_mb": 112028.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 70.0}, "power_stats": {"power_watts_samples": [33.11, 33.11, 33.11], "power_watts_avg": 33.11, "power_watts_peak": 33.11, "energy_joules_est": 7.63, "sample_count": 3, "duration_seconds": 0.231}, "timestamp": "2026-01-25T20:07:01.010851"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 366.66, "latencies_ms": [366.66], "images_per_second": 2.727, "prompt_tokens": 756, "response_tokens_est": 62, "n_tiles": 1, "output_text": "\n The image shows a young girl and boy sitting on top of a luggage cart with their suitcases. They are surrounded by several cars parked in the background, including one that is yellow and another that appears to be black. The sky above them seems overcast, casting a soft light over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.4, "ram_available_mb": 112028.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10473.4, "ram_available_mb": 112032.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [33.11, 28.72, 28.72, 28.72], "power_watts_avg": 29.82, "power_watts_peak": 33.11, "energy_joules_est": 10.94, "sample_count": 4, "duration_seconds": 0.367}, "timestamp": "2026-01-25T20:07:01.416943"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 371.29, "latencies_ms": [371.29], "images_per_second": 2.693, "prompt_tokens": 744, "response_tokens_est": 47, "n_tiles": 1, "output_text": "\nA blue and yellow bus is driving down a street, with a white van following behind it on the right side of the road. The scene takes place in front of houses and trees, creating an idyllic countryside setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.4, "ram_available_mb": 112032.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10476.5, "ram_available_mb": 112029.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [28.72, 28.72, 37.44, 37.44], "power_watts_avg": 33.08, "power_watts_peak": 37.44, "energy_joules_est": 12.29, "sample_count": 4, "duration_seconds": 0.372}, "timestamp": "2026-01-25T20:07:01.831189"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.166, "latencies_ms": [79.166], "images_per_second": 12.632, "prompt_tokens": 759, "response_tokens_est": 7, "n_tiles": 1, "output_text": "\n 1. Bus: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.5, "ram_available_mb": 112029.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 10478.3, "ram_available_mb": 112028.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 28.0}, "power_stats": {"power_watts_samples": [37.44], "power_watts_avg": 37.44, "power_watts_peak": 37.44, "energy_joules_est": 2.99, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T20:07:01.937429"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 136.639, "latencies_ms": [136.639], "images_per_second": 7.319, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nA yellow and blue bus is driving down a street next to some houses.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10478.3, "ram_available_mb": 112028.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10482.0, "ram_available_mb": 112024.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [37.44, 37.44], "power_watts_avg": 37.44, "power_watts_peak": 37.44, "energy_joules_est": 5.14, "sample_count": 2, "duration_seconds": 0.137}, "timestamp": "2026-01-25T20:07:02.144039"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 340.854, "latencies_ms": [340.854], "images_per_second": 2.934, "prompt_tokens": 757, "response_tokens_est": 56, "n_tiles": 1, "output_text": "\nA blue and yellow bus is driving down a street, passing by houses on either side. The bus appears to be in good condition with its headlights on. There are several people walking along the sidewalk near the bus, possibly waiting for their ride or just enjoying the outdoors.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10482.0, "ram_available_mb": 112024.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10482.0, "ram_available_mb": 112024.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [39.66, 39.66, 39.66, 39.66], "power_watts_avg": 39.66, "power_watts_peak": 39.66, "energy_joules_est": 13.54, "sample_count": 4, "duration_seconds": 0.341}, "timestamp": "2026-01-25T20:07:02.551526"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 332.755, "latencies_ms": [332.755], "images_per_second": 3.005, "prompt_tokens": 756, "response_tokens_est": 57, "n_tiles": 1, "output_text": "\n The image shows a yellow and blue bus driving down the street. There are several people walking on the sidewalk near the bus stop. A few cars can be seen parked along the side of the road. Additionally, there is a house visible in the background with a garden outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.0, "ram_available_mb": 112024.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10474.9, "ram_available_mb": 112031.4, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 27.0}, "power_stats": {"power_watts_samples": [39.66, 33.14, 33.14, 33.14], "power_watts_avg": 34.77, "power_watts_peak": 39.66, "energy_joules_est": 11.58, "sample_count": 4, "duration_seconds": 0.333}, "timestamp": "2026-01-25T20:07:02.958854"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 321.807, "latencies_ms": [321.807], "images_per_second": 3.107, "prompt_tokens": 744, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA pelican is perched on a rock overlooking the ocean, with its wings spread wide and beak open as if it's calling out to other birds or waiting for food.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10474.9, "ram_available_mb": 112031.4, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10484.3, "ram_available_mb": 112022.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 2.0}, "power_stats": {"power_watts_samples": [33.14, 33.14, 36.24, 36.24], "power_watts_avg": 34.69, "power_watts_peak": 36.24, "energy_joules_est": 11.17, "sample_count": 4, "duration_seconds": 0.322}, "timestamp": "2026-01-25T20:07:03.372061"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 191.158, "latencies_ms": [191.158], "images_per_second": 5.231, "prompt_tokens": 759, "response_tokens_est": 27, "n_tiles": 1, "output_text": "\n 1. Sky  2. Clouds 3. Ocean 4. Beach 5. Island 6. Bird 7. Rock 8. Water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10484.3, "ram_available_mb": 112022.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10480.3, "ram_available_mb": 112026.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 2.0}, "power_stats": {"power_watts_samples": [36.24, 36.24], "power_watts_avg": 36.24, "power_watts_peak": 36.24, "energy_joules_est": 6.94, "sample_count": 2, "duration_seconds": 0.191}, "timestamp": "2026-01-25T20:07:03.577592"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 117.784, "latencies_ms": [117.784], "images_per_second": 8.49, "prompt_tokens": 763, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\nA pelican is perched on a rock overlooking the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.3, "ram_available_mb": 112026.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 2.0}, "power_stats": {"power_watts_samples": [36.24, 37.86], "power_watts_avg": 37.05, "power_watts_peak": 37.86, "energy_joules_est": 4.39, "sample_count": 2, "duration_seconds": 0.118}, "timestamp": "2026-01-25T20:07:03.784259"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 327.435, "latencies_ms": [327.435], "images_per_second": 3.054, "prompt_tokens": 757, "response_tokens_est": 54, "n_tiles": 1, "output_text": "\nA pelican stands on a rock overlooking the ocean, with its wings spread wide. The bird appears to be enjoying the view of the water below. In the background, there are several umbrellas set up along the beach, providing shade for beachgoers.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 10477.1, "ram_available_mb": 112029.2, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10476.4, "ram_available_mb": 112029.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.86, 37.86, 37.86, 37.86], "power_watts_avg": 37.86, "power_watts_peak": 37.86, "energy_joules_est": 12.41, "sample_count": 4, "duration_seconds": 0.328}, "timestamp": "2026-01-25T20:07:04.190608"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 118.329, "latencies_ms": [118.329], "images_per_second": 8.451, "prompt_tokens": 756, "response_tokens_est": 14, "n_tiles": 1, "output_text": "\n The pelican is perched on a rock overlooking the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10476.3, "ram_available_mb": 112030.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10472.6, "ram_available_mb": 112033.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.28, 34.28], "power_watts_avg": 34.28, "power_watts_peak": 34.28, "energy_joules_est": 4.07, "sample_count": 2, "duration_seconds": 0.119}, "timestamp": "2026-01-25T20:07:04.396876"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 328.591, "latencies_ms": [328.591], "images_per_second": 3.043, "prompt_tokens": 744, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nA man in a blue and white checkered shirt sits on a gray couch, holding a toothbrush with his right hand while looking at it with a smile on his face.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10472.6, "ram_available_mb": 112033.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [34.28, 34.28, 33.82, 33.82], "power_watts_avg": 34.05, "power_watts_peak": 34.28, "energy_joules_est": 11.2, "sample_count": 4, "duration_seconds": 0.329}, "timestamp": "2026-01-25T20:07:04.811060"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 72.514, "latencies_ms": [72.514], "images_per_second": 13.79, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n Chair: 1", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [33.82], "power_watts_avg": 33.82, "power_watts_peak": 33.82, "energy_joules_est": 2.47, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T20:07:04.917130"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 131.37, "latencies_ms": [131.37], "images_per_second": 7.612, "prompt_tokens": 763, "response_tokens_est": 17, "n_tiles": 1, "output_text": "\nA man holding a toothbrush in his hand while sitting on a chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [33.82, 33.82], "power_watts_avg": 33.82, "power_watts_peak": 33.82, "energy_joules_est": 4.46, "sample_count": 2, "duration_seconds": 0.132}, "timestamp": "2026-01-25T20:07:05.123950"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 209.537, "latencies_ms": [209.537], "images_per_second": 4.772, "prompt_tokens": 757, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA man in a blue plaid shirt sits on a gray couch, holding a toothbrush. He has a brown paper bag next to him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10474.8, "ram_available_mb": 112031.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 10473.6, "ram_available_mb": 112032.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [32.24, 32.24, 32.24], "power_watts_avg": 32.24, "power_watts_peak": 32.24, "energy_joules_est": 6.77, "sample_count": 3, "duration_seconds": 0.21}, "timestamp": "2026-01-25T20:07:05.430951"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 126.44, "latencies_ms": [126.44], "images_per_second": 7.909, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n The man is sitting in a chair with his hand on the bag.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.8, "ram_available_mb": 112032.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10475.2, "ram_available_mb": 112031.1, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [32.24, 32.24], "power_watts_avg": 32.24, "power_watts_peak": 32.24, "energy_joules_est": 4.09, "sample_count": 2, "duration_seconds": 0.127}, "timestamp": "2026-01-25T20:07:05.637632"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 295.057, "latencies_ms": [295.057], "images_per_second": 3.389, "prompt_tokens": 744, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\nA man in an orange shirt and black shorts is playing tennis, holding a racket and preparing to hit a yellow ball on a green court.", "error": null, "sys_before": {"cpu_percent": 4.8, "ram_used_mb": 10475.2, "ram_available_mb": 112031.1, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 10477.5, "ram_available_mb": 112028.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 44.0}, "power_stats": {"power_watts_samples": [29.18, 29.18, 29.18], "power_watts_avg": 29.18, "power_watts_peak": 29.18, "energy_joules_est": 8.62, "sample_count": 3, "duration_seconds": 0.295}, "timestamp": "2026-01-25T20:07:05.950625"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 107.879, "latencies_ms": [107.879], "images_per_second": 9.27, "prompt_tokens": 759, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. Man in orange shirt and black shorts playing tennis", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 10477.5, "ram_available_mb": 112028.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 10479.4, "ram_available_mb": 112026.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [29.18, 29.18], "power_watts_avg": 29.18, "power_watts_peak": 29.18, "energy_joules_est": 3.16, "sample_count": 2, "duration_seconds": 0.108}, "timestamp": "2026-01-25T20:07:06.156331"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 123.836, "latencies_ms": [123.836], "images_per_second": 8.075, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\nThe man is holding a tennis racket and preparing to hit a ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.4, "ram_available_mb": 112026.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10483.6, "ram_available_mb": 112022.7, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [33.55, 33.55], "power_watts_avg": 33.55, "power_watts_peak": 33.55, "energy_joules_est": 4.16, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T20:07:06.361819"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 186.195, "latencies_ms": [186.195], "images_per_second": 5.371, "prompt_tokens": 757, "response_tokens_est": 29, "n_tiles": 1, "output_text": "\nA man in an orange shirt and black shorts is playing tennis on a green court, holding a racket and preparing to hit a ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.6, "ram_available_mb": 112022.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10483.8, "ram_available_mb": 112022.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [33.55, 33.55], "power_watts_avg": 33.55, "power_watts_peak": 33.55, "energy_joules_est": 6.26, "sample_count": 2, "duration_seconds": 0.187}, "timestamp": "2026-01-25T20:07:06.567495"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 38.917, "latencies_ms": [38.917], "images_per_second": 25.695, "prompt_tokens": 756, "response_tokens_est": 1, "n_tiles": 1, "output_text": "", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10483.8, "ram_available_mb": 112022.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10483.3, "ram_available_mb": 112023.0, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [33.55], "power_watts_avg": 33.55, "power_watts_peak": 33.55, "energy_joules_est": 1.32, "sample_count": 1, "duration_seconds": 0.039}, "timestamp": "2026-01-25T20:07:06.672374"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 254.722, "latencies_ms": [254.722], "images_per_second": 3.926, "prompt_tokens": 744, "response_tokens_est": 23, "n_tiles": 1, "output_text": "urns of water are on a table in the kitchen, and there is a sink with dishes inside it.", "error": null, "sys_before": {"cpu_percent": 3.6, "ram_used_mb": 10483.3, "ram_available_mb": 112023.0, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10477.5, "ram_available_mb": 112028.8, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 46.0}, "power_stats": {"power_watts_samples": [32.91, 32.91, 32.91], "power_watts_avg": 32.91, "power_watts_peak": 32.91, "energy_joules_est": 8.39, "sample_count": 3, "duration_seconds": 0.255}, "timestamp": "2026-01-25T20:07:06.988290"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 477.215, "latencies_ms": [477.215], "images_per_second": 2.095, "prompt_tokens": 759, "response_tokens_est": 85, "n_tiles": 1, "output_text": "\n 1. Toaster oven  2. Coffee maker 3. Coffee pot 4. Coffee maker 5. Coffee maker 6. Coffee maker 7. Coffee maker 8. Coffee maker 9. Coffee maker 10. Coffee maker 11. Coffee maker 12. Coffee maker 13. Coffee maker 14. Coffee maker 15. Coffee maker 16. Coffee maker 17. Coffee maker 18. Coffee maker 19. Coffee maker 20. Toaster oven", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10477.5, "ram_available_mb": 112028.8, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [32.91, 32.09, 32.09, 32.09, 32.09], "power_watts_avg": 32.26, "power_watts_peak": 32.91, "energy_joules_est": 15.42, "sample_count": 5, "duration_seconds": 0.478}, "timestamp": "2026-01-25T20:07:07.494982"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 72.484, "latencies_ms": [72.484], "images_per_second": 13.796, "prompt_tokens": 763, "response_tokens_est": 4, "n_tiles": 1, "output_text": "urn on counter", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.8, "ram_available_mb": 112025.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10482.4, "ram_available_mb": 112023.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 29.0}, "power_stats": {"power_watts_samples": [32.09], "power_watts_avg": 32.09, "power_watts_peak": 32.09, "energy_joules_est": 2.34, "sample_count": 1, "duration_seconds": 0.073}, "timestamp": "2026-01-25T20:07:07.600726"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 420.628, "latencies_ms": [420.628], "images_per_second": 2.377, "prompt_tokens": 757, "response_tokens_est": 71, "n_tiles": 1, "output_text": "\nThe image shows a small, well-lit kitchen with yellow walls. The kitchen features white cabinets that contrast with the warm tones of the room. A sink can be seen in one corner, along with various appliances such as an oven and a refrigerator. There are also several cups and bowls scattered throughout the space, indicating daily use by the occupants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.6, "ram_available_mb": 112023.7, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10482.4, "ram_available_mb": 112023.9, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.18, 37.18, 37.18, 37.18, 37.18], "power_watts_avg": 37.18, "power_watts_peak": 37.18, "energy_joules_est": 15.65, "sample_count": 5, "duration_seconds": 0.421}, "timestamp": "2026-01-25T20:07:08.107256"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 123.743, "latencies_ms": [123.743], "images_per_second": 8.081, "prompt_tokens": 756, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n The kitchen is painted yellow and has a black rug on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10482.4, "ram_available_mb": 112023.9, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10479.8, "ram_available_mb": 112026.5, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.05, 37.05], "power_watts_avg": 37.05, "power_watts_peak": 37.05, "energy_joules_est": 4.6, "sample_count": 2, "duration_seconds": 0.124}, "timestamp": "2026-01-25T20:07:08.314235"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 246.393, "latencies_ms": [246.393], "images_per_second": 4.059, "prompt_tokens": 744, "response_tokens_est": 19, "n_tiles": 1, "output_text": "iced tea and a sandwich on a white plate, with a computer monitor in the background.", "error": null, "sys_before": {"cpu_percent": 4.3, "ram_used_mb": 10479.8, "ram_available_mb": 112026.5, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10479.0, "ram_available_mb": 112027.3, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [37.05, 37.05, 37.05], "power_watts_avg": 37.05, "power_watts_peak": 37.05, "energy_joules_est": 9.14, "sample_count": 3, "duration_seconds": 0.247}, "timestamp": "2026-01-25T20:07:08.627853"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 69.572, "latencies_ms": [69.572], "images_per_second": 14.374, "prompt_tokens": 759, "response_tokens_est": 5, "n_tiles": 1, "output_text": "\n Sandwich: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10479.0, "ram_available_mb": 112027.3, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 10480.7, "ram_available_mb": 112025.6, "ram_percent": 8.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.59], "power_watts_avg": 35.59, "power_watts_peak": 35.59, "energy_joules_est": 2.5, "sample_count": 1, "duration_seconds": 0.07}, "timestamp": "2026-01-25T20:07:08.734549"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 142.206, "latencies_ms": [142.206], "images_per_second": 7.032, "prompt_tokens": 763, "response_tokens_est": 16, "n_tiles": 1, "output_text": "\n A sandwich on a white plate next to an apple and a computer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10480.7, "ram_available_mb": 112025.6, "ram_percent": 8.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.59, 35.59], "power_watts_avg": 35.59, "power_watts_peak": 35.59, "energy_joules_est": 5.08, "sample_count": 2, "duration_seconds": 0.143}, "timestamp": "2026-01-25T20:07:08.939242"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 246.316, "latencies_ms": [246.316], "images_per_second": 4.06, "prompt_tokens": 757, "response_tokens_est": 38, "n_tiles": 1, "output_text": "\nA white plate holds a half-eaten sandwich with lettuce, tomato, ham, cheese, and pickles. The plate rests on a desk next to an open laptop computer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10468.6, "ram_available_mb": 112037.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 18.0}, "power_stats": {"power_watts_samples": [35.59, 35.59, 31.3], "power_watts_avg": 34.16, "power_watts_peak": 35.59, "energy_joules_est": 8.42, "sample_count": 3, "duration_seconds": 0.247}, "timestamp": "2026-01-25T20:07:09.245461"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 124.706, "latencies_ms": [124.706], "images_per_second": 8.019, "prompt_tokens": 756, "response_tokens_est": 15, "n_tiles": 1, "output_text": "\n A sandwich on a white plate with lettuce and other food items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 10468.8, "ram_available_mb": 112037.5, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 18.0}, "power_stats": {"power_watts_samples": [31.3, 31.3], "power_watts_avg": 31.3, "power_watts_peak": 31.3, "energy_joules_est": 3.92, "sample_count": 2, "duration_seconds": 0.125}, "timestamp": "2026-01-25T20:07:09.451503"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 235.876, "latencies_ms": [235.876], "images_per_second": 4.24, "prompt_tokens": 744, "response_tokens_est": 19, "n_tiles": 1, "output_text": "\nA computer desk with two monitors, a keyboard, and mouse on top of it.", "error": null, "sys_before": {"cpu_percent": 5.0, "ram_used_mb": 10468.8, "ram_available_mb": 112037.5, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 10472.0, "ram_available_mb": 112034.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 18.0}, "power_stats": {"power_watts_samples": [31.3, 31.3, 30.71], "power_watts_avg": 31.1, "power_watts_peak": 31.3, "energy_joules_est": 7.35, "sample_count": 3, "duration_seconds": 0.236}, "timestamp": "2026-01-25T20:07:09.762983"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 79.126, "latencies_ms": [79.126], "images_per_second": 12.638, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1 keyboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.0, "ram_available_mb": 112034.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 18.0}, "power_stats": {"power_watts_samples": [30.71], "power_watts_avg": 30.71, "power_watts_peak": 30.71, "energy_joules_est": 2.45, "sample_count": 1, "duration_seconds": 0.08}, "timestamp": "2026-01-25T20:07:09.869249"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 112.053, "latencies_ms": [112.053], "images_per_second": 8.924, "prompt_tokens": 763, "response_tokens_est": 13, "n_tiles": 1, "output_text": "\n 1. A computer monitor with a website on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 18.0}, "power_stats": {"power_watts_samples": [30.71, 30.71], "power_watts_avg": 30.71, "power_watts_peak": 30.71, "energy_joules_est": 3.45, "sample_count": 2, "duration_seconds": 0.112}, "timestamp": "2026-01-25T20:07:10.075475"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 698.729, "latencies_ms": [698.729], "images_per_second": 1.431, "prompt_tokens": 757, "response_tokens_est": 128, "n_tiles": 1, "output_text": "\nThe image shows a desk with two computer monitors, one on each side. The screens display different web pages, indicating that they are being used for work or study purposes. A keyboard and mouse can be seen in front of the monitors, suggesting that someone is actively using them to navigate through the content displayed on the screens.\n\nIn addition to the computers, there are two cell phones placed near each other on the desk. One phone is located closer to the left monitor, while the other one is positioned towards the right side of the desk. A chair can be seen in front of the monitors, indicating that someone has been sitting at", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10472.1, "ram_available_mb": 112034.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 64.0}, "power_stats": {"power_watts_samples": [30.71, 31.26, 31.26, 31.26, 31.26, 31.26, 35.24], "power_watts_avg": 31.75, "power_watts_peak": 35.24, "energy_joules_est": 22.2, "sample_count": 7, "duration_seconds": 0.699}, "timestamp": "2026-01-25T20:07:10.782071"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 543.079, "latencies_ms": [543.079], "images_per_second": 1.841, "prompt_tokens": 756, "response_tokens_est": 96, "n_tiles": 1, "output_text": "\n The image shows a desk with two computer monitors and a keyboard. One of the monitors is turned on while the other one is off. There are also two cell phones placed on the desk - one near the left side and another towards the right side. A mouse can be seen in front of the second monitor, and there's a laptop positioned at the bottom center of the scene. The lighting appears to be coming from above, creating shadows that add depth to the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [35.24, 35.24, 35.24, 35.24, 41.08, 41.08], "power_watts_avg": 37.19, "power_watts_peak": 41.08, "energy_joules_est": 20.21, "sample_count": 6, "duration_seconds": 0.543}, "timestamp": "2026-01-25T20:07:11.388945"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 310.087, "latencies_ms": [310.087], "images_per_second": 3.225, "prompt_tokens": 744, "response_tokens_est": 31, "n_tiles": 1, "output_text": "\nA group of people are gathered in a bathroom, with some standing and others sitting on the floor. A white toilet is visible in the background.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 10469.1, "ram_available_mb": 112037.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 10470.4, "ram_available_mb": 112035.9, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 92.0}, "power_stats": {"power_watts_samples": [41.08, 41.08, 41.24, 41.24], "power_watts_avg": 41.16, "power_watts_peak": 41.24, "energy_joules_est": 12.79, "sample_count": 4, "duration_seconds": 0.311}, "timestamp": "2026-01-25T20:07:11.798392"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 214.49, "latencies_ms": [214.49], "images_per_second": 4.662, "prompt_tokens": 759, "response_tokens_est": 30, "n_tiles": 1, "output_text": "\n 1. Urinal 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.4, "ram_available_mb": 112035.9, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [41.24, 41.24, 41.24], "power_watts_avg": 41.24, "power_watts_peak": 41.24, "energy_joules_est": 8.86, "sample_count": 3, "duration_seconds": 0.215}, "timestamp": "2026-01-25T20:07:12.105041"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 109.104, "latencies_ms": [109.104], "images_per_second": 9.166, "prompt_tokens": 763, "response_tokens_est": 12, "n_tiles": 1, "output_text": "\n 1. Urinal - top right corner of photo", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10471.3, "ram_available_mb": 112035.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.25, 36.25], "power_watts_avg": 36.25, "power_watts_peak": 36.25, "energy_joules_est": 3.97, "sample_count": 2, "duration_seconds": 0.11}, "timestamp": "2026-01-25T20:07:12.310044"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 230.51, "latencies_ms": [230.51], "images_per_second": 4.338, "prompt_tokens": 757, "response_tokens_est": 37, "n_tiles": 1, "output_text": "\nA group of people are gathered in a bathroom, with one person holding their head up high. The floor is covered in tiles, and there's a toilet visible above them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [36.25, 36.25, 36.25], "power_watts_avg": 36.25, "power_watts_peak": 36.25, "energy_joules_est": 8.37, "sample_count": 3, "duration_seconds": 0.231}, "timestamp": "2026-01-25T20:07:12.615092"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 77.542, "latencies_ms": [77.542], "images_per_second": 12.896, "prompt_tokens": 756, "response_tokens_est": 7, "n_tiles": 1, "output_text": "urns on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10473.1, "ram_available_mb": 112033.2, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.07], "power_watts_avg": 30.07, "power_watts_peak": 30.07, "energy_joules_est": 2.35, "sample_count": 1, "duration_seconds": 0.078}, "timestamp": "2026-01-25T20:07:12.720780"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 364.989, "latencies_ms": [364.989], "images_per_second": 2.74, "prompt_tokens": 744, "response_tokens_est": 44, "n_tiles": 1, "output_text": "\nA small gray bird is perched on a tree branch, facing to the right with its head turned slightly towards the viewer. The background features green leaves and blurred trees, creating an impression of depth in the scene.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 10469.6, "ram_available_mb": 112036.7, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [30.07, 30.07, 30.07, 30.07], "power_watts_avg": 30.07, "power_watts_peak": 30.07, "energy_joules_est": 10.99, "sample_count": 4, "duration_seconds": 0.366}, "timestamp": "2026-01-25T20:07:13.131654"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 74.939, "latencies_ms": [74.939], "images_per_second": 13.344, "prompt_tokens": 759, "response_tokens_est": 6, "n_tiles": 1, "output_text": "\n 1. Tree branch", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.3, "ram_available_mb": 112036.0, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.53], "power_watts_avg": 34.53, "power_watts_peak": 34.53, "energy_joules_est": 2.61, "sample_count": 1, "duration_seconds": 0.076}, "timestamp": "2026-01-25T20:07:13.236317"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 102.613, "latencies_ms": [102.613], "images_per_second": 9.745, "prompt_tokens": 763, "response_tokens_est": 11, "n_tiles": 1, "output_text": "\nThe bird is perched on a tree branch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.0, "ram_available_mb": 112037.3, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.53, 34.53], "power_watts_avg": 34.53, "power_watts_peak": 34.53, "energy_joules_est": 3.55, "sample_count": 2, "duration_seconds": 0.103}, "timestamp": "2026-01-25T20:07:13.442246"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 250.492, "latencies_ms": [250.492], "images_per_second": 3.992, "prompt_tokens": 757, "response_tokens_est": 42, "n_tiles": 1, "output_text": "\nA small gray bird with a yellow beak perches on a tree branch, facing to the right. The background appears blurry, suggesting that the focus of attention is on the bird in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10469.2, "ram_available_mb": 112037.1, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [34.53, 34.53, 33.58], "power_watts_avg": 34.21, "power_watts_peak": 34.53, "energy_joules_est": 8.58, "sample_count": 3, "duration_seconds": 0.251}, "timestamp": "2026-01-25T20:07:13.748279"}
{"runtime": "ollama", "precision_mode": "quantized", "quantization": "q4_0", "model_id": "ollama:moondream", "image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/dgxwaggle/Desktop/SageEdge/Benchmarking/data/coco/val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 85.808, "latencies_ms": [85.808], "images_per_second": 11.654, "prompt_tokens": 756, "response_tokens_est": 9, "n_tiles": 1, "output_text": "\n The bird is gray and brown.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 10470.9, "ram_available_mb": 112035.4, "ram_percent": 8.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA GB10", "gpu_mem_alloc_mb": 0.0, "gpu_mem_reserved_mb": 0.0, "gpu_max_mem_alloc_mb": 0.0, "gpu_max_mem_reserved_mb": 0.0, "gpu_utilization_percent": 91.0}, "power_stats": {"power_watts_samples": [33.58], "power_watts_avg": 33.58, "power_watts_peak": 33.58, "energy_joules_est": 2.9, "sample_count": 1, "duration_seconds": 0.086}, "timestamp": "2026-01-25T20:07:13.853233"}
